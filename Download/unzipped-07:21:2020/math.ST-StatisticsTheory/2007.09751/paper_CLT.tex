% Template for the submission to:
%   The Annals of Probability           [aop]
%   The Annals of Applied Probability   [aap]
%   The Annals of Statistics            [aos] 
%   The Annals of Applied Statistics    [aoas]
%   Stochastic Systems                  [ssy]
%
%Author: In this template, the places where you need to add information
%        (or delete line) are indicated by {???}.  Mostly the information
%        required is obvious, but some explanations are given in lines starting
%Author:
%All other lines should be ignored.  After editing, there should be
%no instances of ??? after this line.

% use option [preprint] to remove info line at bottom
% journal options: aop,aap,aos,aoas,ssy
% natbib option: authoryear
\documentclass[11pt]{article}
\usepackage{fullpage}
\RequirePackage[OT1]{fontenc}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\RequirePackage{amsthm,amsmath}
\RequirePackage{natbib}

\usepackage{amssymb,bbm,tikz}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\usepackage{cases}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{microtype}
\usepackage{enumitem}
%\usepackage{authblk}
\usepackage{url}
\usepackage{amsfonts, mathabx}

%\usepackage[colorlinks,citecolor=blue,urlcolor=blue,linkcolor=blue,linktocpage=true]{hyperref}
%\usepackage{sectsty}
%\allsectionsfont{\bfseries\sffamily}


\usepackage{appendix}

% settings
%\pubyear{2005}
%\volume{0}
%\issue{0}
%\firstpage{1}
%\lastpage{8}
% \arxiv{arXiv:0000.0000}

% \startlocaldefs
%\usepackage[colorinlistoftodos]{todonotes}
%\newcommand{\aleg}[1]{\todo[inline,color=blue!30]{\textbf{Ale:} #1}}

%\pdfstringdefDisableCommands{\def\Cref#1{#1}}
%\usepackage{cleveref}
%\crefname{appsec}{Appendix}{Appendices}
%\crefname{appsec}{Supplement}{Supplement}
%\crefformat{equation}{(#2#1#3)}
%\crefrangeformat{equation}{(#3#1#4) to~(#5#2#6)}
%\crefname{equation}{}{}
%\Crefname{equation}{}{}


\renewcommand{\qedsymbol}{\hfill\rule{2mm}{2mm}}

\let\hat\widehat
\let\tilde\widetilde

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}{Definition}
\newtheorem*{definition*}{Definition}
\newtheorem*{remark*}{Remark}
%\crefname{definition}{\textbf{definition}}{definitions}
%\Crefname{definition}{Definition}{Definitions}
%\crefname{assumption}{\textbf{assumption}}{assumptions}
%\Crefname{assumption}{Assumption}{Assumptions}





\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\renewcommand{\P}{\mbox{$\mathbb{P}$}}
\newcommand{\E}{\mbox{$\mathbb{E}$}}
\newcommand{\wS}{\widehat{S}}


\makeatletter
\def\namedlabel#1#2{\begingroup
    #2%
    \def\@currentlabel{#2}%
    \phantomsection\label{#1}\endgroup
}
\makeatother


%%%% new version of enumerate with less spacing
%\newenvironment{enum}{
%\begin{enumerate}
%  \setlength{\itemsep}{1pt}
%  \setlength{\parskip}{0pt}
%  \setlength{\parsep}{0pt}
%}{\end{enumerate}}


%\parskip 10pt
%\parindent 0pt

\usepackage[textsize=scriptsize]{todonotes}
% \newcommand{\arun}[1]{\todo[color=blue,backgroundcolor=blue!25,backgroundcolor=white]{Arun: #1}}
\title{\bf Berry-Esseen Bounds for Projection Parameters and Partial Correlations With Increasing Dimension}
%\runtitle{Berry-Esseen for Projection Parameters}
\author{
    Arun Kumar Kuchibhotla\thanks{Email: {\tt
            karun3kumar@gmail.com}.}
    \and
    Alessandro Rinaldo\thanks{Email: {\tt arinaldo@cmu.edu}}
    \and
    Larry Wasserman\thanks{Email: {\tt larry@stat.cmu.edu}.}
	%
}
%\runauthor{Kuchibholta, Rinaldo and Wasserman}
\begin{document}
\maketitle

{\centering
\vspace*{-0.5cm}
\textit{Carnegie Mellon University}\\
\par\bigskip
July 15, 2020
\par
}

% \begin{document}

% \begin{frontmatter}

% "Title of the paper"

% indicate corresponding author with \corref{}
% \author{\fnms{John} \snm{Smith}\corref{}\ead[label=e1]{smith@foo.com}\thanksref{t1}}
% \thankstext{t1}{Thanks to somebody} 
% \address{line 1\\ line 2\\ printead{e1}}
% \affiliation{Some University}
% \begin{aug}
% \author{\fnms{Arun Kumar} \snm{Kuchibhotla}\ead[label=e1]{karun3kumar@gmail.com}},
% \author{\fnms{Alessandro} \snm{Rinaldo}\ead[label=e2]{arinaldo@cmu.edu}}
% \and
% \author{\fnms{Larry} \snm{Wasserman}\ead[label=e3]{larry@stat.cmu.edu}}


% \affiliation{Carnegie Mellon University}

% \address{A. K. Kuchibhotla\\
% A. Rinaldo\\
%   L. Wasserman\\
% Department of Statistics and Data Science\\
% Carnegie Mellon University\\
% Pittsburgh, Pennsylvania 15213\\
% \printead{e1}\\
% \printead*{e2}\\
% \printead*{e3}%\\
% %\printead*{e4}
% }
% \end{aug}


\begin{abstract}
The linear regression model can be used even when
the true regression function is not linear.
The resulting estimated linear function is the best linear
approximation to the regression function and
the vector $\beta$ of the coefficients of this linear approximation
are the projection parameter.
We provide finite sample bounds
on the Normal approximation to the law of
the least squares estimator of the projection parameters
normalized by the sandwich-based standard error. Our results hold in the increasing dimension setting and under minimal assumptions on the distribution of the response variable.
Furthermore, we construct confidence sets 
for $\beta$ in the form of hyper-rectangles and establish rates on their coverage accuracy.
We provide analogous results for partial correlations among the entries of sub-Gaussian vectors. 
\end{abstract}

%\begin{keyword}[class=MSC]
%\kwd[Primary ]{}
%\kwd{}
%\kwd[; secondary ]{}
%\end{keyword}

%\begin{keyword}
%\kwd{}
%\kwd{}
%\end{keyword}

% \end{frontmatter}


% AOS,AOAS: If there are supplements please fill:
%\begin{supplement}[id=suppA]
%  \sname{Supplement A}
%  \stitle{Title}
%  \slink[doi]{10.1214/00-AOASXXXXSUPP}
%  \sdatatype{.pdf}" 
%  \sdescription{Some text}
%\end{supplement}



%\textcolor{red}{
%	\begin{itemize}
%		\item Add more text and context, in particular references to the more recent literature on CLT for regression parameters (mostly about debiased methods) and more general references to high-dim CLTs.
%		\item Add text about the use of the results on partial correlations for graphical models
%		\item Comment on size of the intervals and compare to existing results, with the problem of optimality left for future work
%		\item some application of the deterministic inequalities to dependent data? 
%	\end{itemize}}


\section{Introduction}
Linear regression is a ubiquitous technique in applied statistics. In much of the classic and recent literature in regression, the theoretical study of ordinary least squares (OLS) estimation has focused primarily on the well-specified case, where the observations are obtained from a model postulating a linear regression function. Although widely studied in the statistical and econometric literature, the properties of the OLS estimator for inference in non-standard settings has gained attraction only relatively recently~\citep{Buja14,Buja16,Uniform:Kuch18}.

In this paper, we study the finite-sample theoretical properties of the OLS estimator, such as estimation error and approximation error to a normal distribution, under minimal assumptions on the data generating distribution and in high-dimensional settings in which the dimension $d$ of the covariates may grow with the sample size $n$ in such a way that $d = o(n)$. In particular, we focus on misspecified regression models in which the regression function is not linear.
%\textcolor{red}{ARUN: Should we name this setting ``high-dimensional'' now? or should we change ``high-dimensional'' to ``moderate dimensional''? I prefer the second because high-dimensional now mostly refers to $d \gg n$.}
%We are concerned with inference for 
%the linear regression  parameters in high-dimensional misspecified regression models.
Specifically, we adopt the standard regression setting,  in which we observe an i.i.d sample $(X_1,Y_1),\ldots, (X_n,Y_n)$ from an unknown distribution $P$ on $\mathbb{R}^d \times \mathbb{R}$, where $X_i$ is the $d$-dimensional vector of covariates and $Y_i$ the response variable for the $i$th sample point. 
 We are interested in providing inferential guarantees for the best linear approximation to the  regression function $x \in \mathbb{R}^d \mapsto \mathbb{E}[Y|X=x]$, which may take any form.
When the underlying  distribution $P$ admits a second moment, it is well known \citep[see, e.g.][]{Buja14} that, even in misspecified models and regardless of true underlying relationship between the covariate and the response variables, the best (in $L_2$ sense) linear approximation to the regression function is well-defined. It is equal to the linear function  $x \mapsto x^\top \beta$, where $\beta \in \mathbb{R}^d$ is any solution to the optimization problem
\[
\beta = \argmin_{b \in \mathbb{R}^d}\, \mathbb{E}[(Y-b^T X)^2],
\]
with $(X,Y) \sim P$.
When the Gram matrix $\Sigma  = \mathbb{E}[X X^\top]$ of the covariate vector is invertible, the solution is unique and is given by the vector of {\it projection parameters}
\[
\beta = \Sigma^{-1}\Gamma, \quad \text{where} \quad  \Gamma = \mathbb{E}[YX]\in\mathbb{R}^d.
\]
Making inferential statements on $\beta$ in these {\it assumption-lean} settings \citep{Buja14}, i.e. with random covariates and without a true underlying linear model, is an exceedingly common task in applied regression. However, as elucidated in \cite{Buja14}, in this case it is necessary to apply appropriate modifications to standard theory and methods in order to obtain valid asymptotic conclusions, even in fixed-dimensional settings. In particular, it is essential to deploy the sandwich estimator \citep{White1980,Buja14} of the variance of the ordinary least squares estimators. 

In this paper we follow the {\it assumption-lean} framework put forward by \cite{Buja14}, \cite{kuchibhotla2018valid} and \cite{boot} and derive novel non-asymptotic inferential guarantees for the projection parameters that hold under minimal assumptions on the data generating distribution and in the high-dimensional regime of $d$ increasing in (but of smaller order than) $n$.
The main goals of this paper are to present (1) precise high-dimensional Berry-Esseen bounds 
for the Normal approximation to the law of
the OLS estimator
$\hat\beta$ of $\beta$ (normalized by the sandwich standard error) under weak assumptions,
(2) finite sample bounds on the
accuracy of the coverage of a simple and practicable class of confidence intervals for the entries of
 $\beta$
and
(3) similar results for the 
partial correlations of the entries of sub-Gaussian vectors.
The confidence sets we consider are
hyper-rectangles, which
immediately imply
simultaneous
confidence intervals
for all the components of $\beta$.

To the best of our knowledge, our results provide the sharpest known rates for the projection parameters under arguably the weakest possible settings considered in the  literature. 


\subsubsection*{Related Work}
Berry-Esseen bounds for M-estimators such as ordinary least squares is a seasoned topic in statistics. \cite{pfanzagl1973accuracy} and~\cite{paulauskas1996rates}, among others, have considered Berry-Esseen bounds for multivariate estimators albeit without explicit focus on the dependence on the dimension. 
Statistical inference for linear regression models based on central limit theorems in increasing dimensions is also a well-established topic in the statistical literature. 
In a series of paper, \cite{Portnoy84,Portnoy85,Portnoy86,Portnoy88} established 
various types of asymptotic normal approximations in increasing dimensions in a variety of settings. When applied to our problem, those results imply a scaling for the dimension of order $d = o(\sqrt{n})$, assuming a correctly specified model and arguably strong assumptions.
\cite{bickel1983bootstrapping}
showed  consistency of the bootstrap
when $d=o(\sqrt{n})$ assuming again a linear regression model, i.i.d. errors and deterministic covariates \cite[see also][]{mammen1989}. Under more general settings, but still postulating a correctly specified linear model, \cite{mammen1993} proposed the wild (aka multiplier) bootstrap strategy \citep[see also][]{liu1988} for linear contrasts and proved its consistency. \cite{He2000} \citep[see also][]{welsh1989} established component-wise asymptotic normality of regression parameters and of more general estimators in parametric models in increasing dimensions. 


Recently, in a groundbreaking series of papers \cite{Cher13,chernozhukov2017detailed, chernozhukov2019improved} have obtained high-dimensional Berry-Esseen rates over hyper-rectangles and certain types of sparsely-convex sets exhibiting only a poly-logarithmic dependence on the dimension (see also~\citet[Section 2]{MR1115160}). These results, which also hold for the ordinary and multiplier bootstrap,   have been further extended by  \cite{2018arXiv180606153K}, \cite{hang.hzhang.bootstrap.17} (only for the bootstrap)  and then by \cite{koike2019high}. They have seen applications in numerous statistical problems and especially in high-dimensional regression settings: see, e.g., \cite{zhang2014confidence}, \cite{wasserman2014berry}, \cite{10.1093/biomet/asu056},  \cite{doi:10.1146/annurev-economics-012315-015826}, \cite{zhang2017simultaneous}, \cite{test}, \cite{boot}  and \cite{hang.hzhang.bootstrap.17}.
The recent statistical literature has produced a variety of methods for constructing confidence sets for the individual regression parameters (or fixed contrast thereof) in high-dimensional settings, some based on the bootstrap. See, e.g., 
\cite{javanmard2014confidence}, \cite{javanmard2018}, \cite{ning2017},  \cite{zhu2018,doi:10.1080/01621459.2017.1356319}, \cite{cai2017confidence}, \cite{ren2015}, \cite{rajen.peter.2018} and \cite{peter.sarah.2015}. 
What set the present paper apart from much of the existing  literature on the topic is the lack of the linearity and of the sparsity assumptions and, more generally, reliance on very weak conditions on the underlying data distribution, consistent with the {\it assumption-lean} approach. 

\cite{boot} tackled the same misspecified settings considered in this article and formulated general Berry-Esseen bounds for non-linear statistics 
in increasing dimensions.
When applied to the projection parameters $\beta$,
the resulting rates are sub-optimal, as they require
$d = o(n^{1/5})$.
For partial correlations,
\cite{wasserman2014berry}
obtain Berry-Esseen bounds in the increasing dimension case.
The current paper sharpens these bounds considerably and requires much weaker assumptions.

Under the {\it assumption-lean} settings, \cite{kuchibhotla2018valid} proposed the UPoSI methodology for constructing simultaneous confidence sets for the projection parameters of all possible submodels, which in turn is equivalent to post-selection inference control. For the special case of the saturated model, UPoSI implies a confidence set for $\beta$ with coverage guarantees under weaker scaling for the dimension than the one required by the present paper, though at the cost of larger volumes. We comment on the differences between our results and those of  \cite{kuchibhotla2018valid} below in Section \ref{section::conclusion}.

%Cite: \cite{kuelbs2010}
%xxxx Arun's All of Regression paper xxxx


%\vspace{1cm}

\subsubsection*{Summary of our Contributions}
The main
contributions of the paper can be summarized as follows:
\begin{itemize}
\item Theorem \ref{thm:Berry-Esseen-OLS} 
provides a Berry-Esseen bound
on the difference between
the law of $\hat\beta - \beta$ ---
normalized by the standard errors from the sandwich estimator---
and an appropriate Gaussian distribution.
The result is deterministic in the sense that
no distributional assumptions are imposed
on the data generating process. In particular, it holds true for data that are not i.i.d..

\item
Theorem \ref{thm::berry-esseen} 
is our main result.
Assuming independence and additional mild conditions,
we bound the error terms in
Theorem \ref{thm:Berry-Esseen-OLS} 
to derive explicit Berry-Esseen rates where the dimension $d$, as well as other parameters of the underlying distribution (including the condition number of the Gram matrix and the number of finite moments of the response variable) are accounted for.
To that effect, we apply recent high-dimensional central limit results of \cite{koike2019high}, and~\cite{Chern17} for hyper-rectangles that exhibit only a logarithmic dependence on the dimension $d$, albeit at the cost of a worse sample complexity of order  $n^{-1/6}$ instead of the parametric rate $n^{-1/2}$.
Considering  the case where only the dimension $d$ is allowed to change, and ignoring log terms for convenience, the coverage rates we derive are vanishing provided that
$$
d = o\left(\min\left\{n^{1/2},\, n^{(q-3)/(q-1)}\right\}\right)
$$
where $q$ is the number of
finite moments of
$Y-X^T\beta$, assumed to satisfy $q \geq 4 \log(n)/(\log(n) - 2)$. 
If $q \ge 5$, this requirement reduces to $d = o(\sqrt{n})$, a scaling that has become  known informally as the ``Portnoy rate,'' based on a body of work by  \cite{Portnoy84,Portnoy85,Portnoy86,Portnoy88} and others, though with different settings, techniques  and assumptions. 
To the best of our knowledge,
this is the sharpest result
% Further
in the mis-specified case. Our finite sample bounds immediately yield practicable simultaneous confidence intervals for the projection parameters, constructed either through a simple Bonferroni or {\v{S}}id{\'a}k correction, or using the bootstrap (see Theorem~\ref{thm:multiplier-bootstrap-consistency}), a more laborious but sharper method. Furthermore, in all these cases, the length of the (simultaneous) confidence intervals for the entries of the $\beta$ is of order $1/\sqrt{n}$, independently of the dimension.
It is noteworthy that the final scaling requirement of $d = o(\sqrt{n})$ is imposed to ensure consistency in the operator norm of the sandwich estimator (see Theorem~\ref{thm:main-rates-thm-OLS-independence}). We conjecture that such scaling cannot be weakened while retaining the parametric rate of $n^{-1/2}$ for accuracy.  
 Finally, we mention that by using instead more general high-dimensional Berry-Esseen bounds for convex sets as in \cite{bentkus2003dependence,raivc2019multivariate}, we can achieve coverage rates with a root-n dependence on $n$ but with a much worse scaling in $d$, namely $d = o(n^{2/7})$.

\item 
Leveraging these results and the mathematical relationship between partial correlations and projection parameters, in Theorem \ref{thm:Berry-Esseen-bound-partial-corr}
we derive a Berry-Esseen bound
for the $d\times d$ matrix of partial correlations
corresponding to a sub-Gaussian random vector $X\in\mathbb{R}^d$, which in turns yield simultaneous confidence intervals for the partial correlation parameters.  


\end{itemize}








\subsection*{Problem Formulation and Notation}
Let $(X_1,Y_1), \ldots, (X_n,Y_n)$ be a sample of $n$ observations in $\mathbb{R}^{d} \times \mathbb{R}$, not necessarily independent nor identically distributed. If an intercept term is included in the regression fit, as it is customary, the first coordinate of each covariate vector $X_i$ is set to $1$. 
We seek to draw inference on the {\it projection parameter}
\[
\beta = \beta_n := \argmin_{\theta\in\mathbb{R}^d}\,\frac{1}{n}\sum_{i=1}^n \mathbb{E}[(Y_i - X_i^{\top}\theta)^2].
\]
If the matrix
\[
\Sigma_n : = n^{-1}\sum_{i=1}^n \mathbb{E}[X_iX_i^{\top}]
\]
is positive definite, the projection parameter  is well-defined and equal to $\Sigma_n^{-1} \Gamma_n$, where $\Gamma_n : = n^{-1}\mathbb{E} \left[ \sum_{i=1}^n X_i Y_i\right]$.
When the sample points satisfy the linear model
\[
Y_i = X_i^\top \beta^* + \xi_i, \quad i=1,\ldots,n
\] 
where $\mathbb{E}[\xi_i | X_i ] = 0$ for all $i$, then the projection parameter corresponds to the vector of linear coefficients, i.e. $\beta = \beta^*$. In this paper, we will \emph{not} posit any relationship between the vectors of covariates $X_i$ and the responses. In this case, the projection parameter lacks a direct interpretation. In the i.i.d. setting, if the response variable has finite second moment, then the projection parameter collects the coefficient of the $L_2$ projection of $Y_1$ into the linear space spanned by the coordinates of $X_1$, i.e. $X_1^\top \beta$. See~\cite{Buja14,Buja16} for a discussion on interpretation of $\beta$ in a mis-specified case.

The projection parameter is traditionally estimated using the ordinary least squares (OLS) estimator 
\[
\widehat{\beta} = \widehat{\beta}_n := \argmin_{\theta\in\mathbb{R}^d}\,\frac{1}{n}\sum_{i=1}^n (Y_i - X_i^{\top}\theta)^2,
\]
which corresponds to the plug-in estimator of $\beta$.
Letting 
\[
\widehat{\Sigma}_n := \frac{1}{n}\sum_{i=1}^n X_iX_i^{\top}\quad\mbox{and}\quad \widehat{\Gamma}_n := \frac{1}{n}\sum_{i=1}^n X_iY_i,
\]
and provided that $\widehat{\Sigma}_n$ is positive definite, the ordinary least squares estimator is well-defined and can be expressed as  
\[
\widehat{\beta}_n := \widehat{\Sigma}_n^{-1} \widehat{\Gamma}_n.
\]
\paragraph{Notation} For any $x\in\mathbb{R}^d$ and a positive-definite matrix $A\in\mathbb{R}^{d\times d}$, $\|x\|_A = \sqrt{x^{\top}A x}$ represents the scaled Euclidean norm. If $A$ is a squared matrix, $\mbox{diag}(A)$ is the diagonal matrix with diagonal elements matching those of $A$ and if, in addition, $A$ a positive definite (thus, a covariance matrix), we set $\mbox{Corr}(A) = (\mathrm{diag}(A))^{-1/2}A(\mathrm{diag}(A))^{-1/2}$ to be the corresponding correlation matrix.  We denote with $S^{d-1} := \{\theta\in\mathbb{R}^d:\,\theta^{\top}\theta = 1\}$ the unit sphere in $\mathbb{R}^d$.


\paragraph{Outline}
Section \ref{section::determiniswtic}
provides the deterministic CLT for 
the OLS estimator normalized by an estimated
standard deviation.
Section \ref{section::explicit}
treats the case of indpendent observations
and provides explicit rate constraints on
the growth of dimension $d$ with respect to
the sample size $n$.
Section~\ref{sec::confidence-sets-OLS} provides
explicit confidence sets for the projection 
parameter $\beta_n$; we describe three methods
based on Bonferroni, {\v{S}}id{\'a}k inequality
and wild/multiplier bootstrap.
Section \ref{section::partial}
derives similar results for partial correlations.
Concluding remarks and future directions are in
Section \ref{section::conclusion}.

\section{Central Limit Theorems using a Deterministic Inequality}
\label{section::determiniswtic}
In this section
we establish a Berry Esseen bound
for the joint law
of the entries of $\hat\beta-\beta$ divided elementwise by the estimated
standard errors.
The result is deterministic, as it does not hinge upon any distributional  assumptions on the sample.
The strategy is to first obtain a deterministic finite sample bound
for the magnitude of the difference between $\hat\beta-\beta$
and a sample average of natural quantities akin to evaluations of an influence function
(Theorem \ref{thm:Basic-deter-ineq}).
Then the effect of the randomness
due to the use of the standard errors
is bounded
by comparing to the average of the values of the influence function
divided by its true standard deviation
(Corollary \ref{cor:Max-Statistic-Correct-Scaling}).
This leads to the main result,
Theorem \ref{thm:Berry-Esseen-OLS}.
In the subsequent section we will describe minimal distributional assumptions
that will allows us to explicitly bound
the error terms in
Theorem \ref{thm:Berry-Esseen-OLS} and derive rates of consistency for the normal approximation.
The proofs of these results are in Appendix \ref{appendix:deterministic}. 


To introduce our deterministic bound, we first define the matrix
\[
V_n := \mbox{Var}\left(\frac{1}{n}\sum_{i=1}^n X_i(Y_i - X_i^{\top}\beta)\right),
\]
which corresponds to the ``meat'' of the sandwich variance of the OLS estimator in a mis-specified linear models; see~\cite{Buja14}.
Notice that in the above expression the variance cannot be pushed inside the summation since the observations are not assumed independent. Throughout this section, for any non-singular matrix $A$, we let $\kappa(A) = \|A^{-1}\|_{\mathrm{op}}\|A\|_{\mathrm{op}}$ be its condition number.
%Also, note that $V_n$ is generally of order $n^{-1}$ (if the dimension $d$ is fixed).
%We begin by proving a deterministic inequality bounding the scaled Euclidean norm of $\widehat{\beta} - \beta$ by a multiple of the scaled Euclidean norm of a sample average, which can be thought of as an influence function of sort. Our subsequent results rely on this bound in a fundamental way. 


%Suppose $(X_i, Y_i)\in\mathbb{R}^{d+1}$ are observations (not
%necessarily independent) and $\widehat{\beta}\in\mathbb{R}^d$ is the
%OLS estimator. Define the target as
%\[
%\beta := \argmin_{\theta\in\mathbb{R}^d}\,\frac{1}{n}\sum_{i=1}^n \mathbb{E}[(Y_i - X_i^{\top}\theta)^2],
%\]%
%and also set $\Sigma = n^{-1}\sum_{i=1}^n \mathbb{E}[X_iX_i^{\top}]$, \textcolor{red}{assumed positive definite. Also, %should be write instead $\Sigma_n$?}. Since the observations are not assumed to be identically distributed, $\beta$ and %$\Sigma$ are moving (with $n$) population quantities. 
%


%Below and throughout the paper, 
%\[
%\widehat{\Sigma} = \frac{1}{n}\sum_{i=1}^n X_iX_i^{\top}.
%\]
%\textcolor{red}{Again I would write $\widehat{\Sigma}_n$.}

\begin{theorem}\label{thm:Basic-deter-ineq}
Assume $\Sigma_n$ and $V_n$ to be invertible and set 
\begin{equation}\label{eq:Dn}
\mathcal{D}_n^{\Sigma} := \|\Sigma_n^{-1/2}\widehat{\Sigma}_n\Sigma_n^{-1/2} - I_d\|_{\mathrm{op}}.
\end{equation}
 If $\mathcal{D}_n^{\Sigma} < 1$, then 
\[
%\left\|\widehat{\beta} - \beta - \frac{1}{n}\sum_{i=1}^n \psi_{\beta}(X_i, Y_i)\right\|_{\Sigma V^{-1}_n\Sigma} \le \frac{\mathcal{D}_n^{\Sigma}\|n^{-1}\sum_{i=1}^n \psi_{\beta}(X_i, Y_i)\|_{\Sigma V^{-1}_n\Sigma}}{(1 - \mathcal{D}_n^{\Sigma})_+},
\left\|\widehat{\beta} - \beta - \frac{1}{n}\sum_{i=1}^n \psi_i \right\|_{\Sigma_n V^{-1}_n\Sigma_n} ~\le~ \kappa(\Sigma_n^{-1/2}V_n^{1/2})\frac{\mathcal{D}_n^{\Sigma}}{(1 - \mathcal{D}_n^{\Sigma})}\left\|\frac{1}{n}\sum_{i=1}^n \psi_i \right\|_{\Sigma_n V^{-1}_n\Sigma_n},
\]
where 
\[
\psi_i : = \Sigma^{-1}_nX_i(Y_i - X_i^{\top}\beta) \in \mathbb{R}^d, \quad i=1,\ldots,n.
\]
%, $\mathcal{D}_n^{\Sigma} := \|\Sigma_n^{-1/2}\widehat{\Sigma}_n\Sigma_n^{-1/2} - I_d\|$ and for
%Then, for any $n > d$, we have
%\[
%\left\|\widehat{\beta} - \beta - \frac{1}{n}\sum_{i=1}^n \psi_{\beta}(X_i, Y_i)\right\|_{\Sigma V^{-1}_n\Sigma} \le \frac{\mathcal{D}_n^{\Sigma}\|n^{-1}\sum_{i=1}^n \psi_{\beta}(X_i, Y_i)\|_{\Sigma V^{-1}_n\Sigma}}{(1 - \mathcal{D}_n^{\Sigma})_+},
%\left\|\widehat{\beta} - \beta - \frac{1}{n}\sum_{i=1}^n \psi_{\beta}(X_i, Y_i)\right\|_{\Sigma V^{-1}_n\Sigma} \le \frac{  \mathcal{D}_n^{\Sigma} \|n^{-1}\sum_{i=1}^n \psi_{\beta}(X_i, Y_i)\|_{\Sigma V^{-1}_n\Sigma}}{(1 - \mathcal{D}_n^{\Sigma})_+},
%\]
%where $\psi_i := \psi(X_i, Y_i) = \Sigma^{-1}X_i(Y_i - X_i^{\top}\beta)$ and $\mathcal{D}_n^{\Sigma} := \|\Sigma^{-1/2}\widehat{\Sigma}\Sigma^{-1/2} - I_d\|$.
\end{theorem}

%The above result is, of course,  non-trivial and useful for our purposes only when $\mathcal{D}_n^{\Sigma}$ remains bounded away from $1$. \textcolor{red}{I think in the numerator on the right hand side there is an extra $\mathcal{D}_n^{\Sigma}$. Also, we may need some extra terms in the above bound, namely $\|V ^{-1/2}_n \Sigma^{1/2} \|_{\mathrm{op}} \| \Sigma^{-1/2} V ^{1/2}_n\|_{\mathrm{op}}$}


The previous  result immediately implies the following normalized point-wise bound, which also holds deterministically.

\begin{corollary}\label{cor:Max-Statistic-Correct-Scaling}
Under the same conditions of Theorem \ref{thm:Basic-deter-ineq}, we have that 
\[
%\max_{1\le j\le d}\left|\frac{\widehat{\beta}_j - \beta_j - n^{-1}\sum_{i=1}^n \psi_{ij}}{\sqrt{(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}}}\right| \le \frac{\mathcal{D}_n^{\Sigma}}{(1-\mathcal{D}_n^{\Sigma})_+}\left\|\frac{1}{n}\sum_{i=1}^n \psi_i\right\|_{\Sigma V^{-1}_n\Sigma},
\max_{1\le j\le d}\left|\frac{\widehat{\beta}_j - \beta_j - n^{-1}\sum_{i=1}^n \psi_{ij}}{\sqrt{(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}}}\right| \le \kappa(\Sigma_n^{-1/2}V_n^{1/2})\frac{\mathcal{D}_n^{\Sigma}}{(1 - \mathcal{D}_n^{\Sigma})}\left\|\frac{1}{n}\sum_{i=1}^n \psi_i\right\|_{\Sigma_n V^{-1}_n\Sigma_n},
\]
where $\psi_{ij}$ represents the $j$-th coordinate of $\psi_i\in\mathbb{R}^d$ and $(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}$ is the $j$-th diagonal element of $\Sigma_n^{-1}V_n\Sigma_n^{-1}$.
\end{corollary}

Note that $\Sigma_n^{-1}V_n\Sigma_n^{-1}$ is the variance of $n^{-1}\sum_{i=1}^n \psi_i$ and hence the statistics defined in Corollary~\ref{cor:Max-Statistic-Correct-Scaling} are normalized by their standard deviation. Corollary~\ref{cor:Max-Statistic-Correct-Scaling} is a deterministic inequality and can be used to derive bounds on the Gaussian approximation for the maximum of ``$z$-statistics''. The basic identity is as follows: if $U$, $W$ and $R > 0$ are real-valued random variables such that $|U - W| \le R$ almost surely, then for \emph{any} $\varepsilon > 0$ and \emph{any} function $\Psi(\cdot)$,
\begin{equation}\label{eq:basic-identity-BE}
\begin{split}
\sup_{t\in\mathbb{R}}|\mathbb{P}(U \le t) - \Psi(t)| ~&\le~ \sup_{t\in\mathbb{R}}|\mathbb{P}(W \le t) - \Psi(t)|\\ 
&\qquad+ \mathbb{P}(R > \varepsilon) + \sup_{t\in\mathbb{R}}\,[\Psi(t + \varepsilon) - \Psi(t-\varepsilon)].
\end{split}
\end{equation}
See Corollary 10 of~\cite{paulauskas1996rates} for details.
% \textcolor{blue}{Reference}
% \textcolor{red}{
% 	\begin{proof}[Proof of~\eqref{eq:basic-identity-BE}](To be Removed or Moved to Appendix)
% 	\begin{align*}
% 	\mathbb{P}(U \le t) &= \mathbb{P}(U \le t, R \le \varepsilon) + \mathbb{P}(U \le t, R > \varepsilon)\\
% 	&\le \mathbb{P}(V \le t + \varepsilon) + \mathbb{P}(R > \varepsilon),
% 	\end{align*}
% 	This implies that
% 	\[
% 	\mathbb{P}(U \le t) - \Psi(t) \le \mathbb{P}(V \le t + \varepsilon) - \Psi(t + \varepsilon) + \mathbb{P}(R > \varepsilon) + \Psi(t + \varepsilon) - \Psi(t).
% 	\]
% 	Reversing the roles of $U$ and $V$ proves the result.
% 	\end{proof}
% }
Each term on the right hand side has a natural interpretation. The first term shows how well the distribution of $W$ is approximated by $\Psi$. The second term shows the magnitude of difference between $U$ and $W$. The third term measures the distortion in $\Psi$ because of difference between $U$ and $V$. The third term corresponds to anti-concentration.  
A direct application of the inequality~\eqref{eq:basic-identity-BE} with the bound in Corollary~\ref{cor:Max-Statistic-Correct-Scaling} yields a Gaussian approximation for $U = \max_{1\le j\le d}|\widehat{\beta}_j - \beta_j|/(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}^{1/2}$. This can be seen by 
 setting $\Psi(t) = \mathbb{P}(\max_{1\le j\le d}|G_j| \le t)$ for $t\ge0$,  where $(G_1,\ldots,G_d)$ is a centered Gaussian vector with covariance given by \eqref{eq:cov.G} below and $V = \max_{1\le j\le d}|{ n^{-1}\sum_{i=1}^n \psi_{ij}}|/{(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}^{1/2}}$.
 This result is, of course, only of theoretical interest;  for practical inferential purposes, we need a stronger distributional approximation result with the true standard errors replaced by their estimators. Towards that end, let $\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}}$ be an estimator of $\Sigma_n^{-1}V_n\Sigma_n^{-1}$ and consider the event
\begin{equation}\label{eq:key.event}
\begin{split}
\mathcal{E}_{\eta_n} &:= \left\{\mathcal{D}_n^{\Sigma} \le
\frac{1}{2}\right\} \bigcap
\left\{\mathcal{D}_n^{\Sigma}\left\|\frac{1}{n}\sum_{i=1}^n
\psi_i\right\|_{\Sigma_n V^{-1}_n\Sigma_n} \le
\eta_n\right\}
% \\
% &\qquad
\bigcap\left\{\max_{1\le j\le   d}
\left|\sqrt{\frac{(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}}{(\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}})_{jj}}}- 1\right| \le \eta_n\right\},
\end{split}
\end{equation}
where $\eta_n>0$ is a positive number, possibly depending on $n$.
The first two  events in Equation~\eqref{eq:key.event} allow us bound the right hand side of the inequality in Corollary~\ref{cor:Max-Statistic-Correct-Scaling}, while the third event enables us to replace $(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}$ by $(\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}})_{jj}$. 

Next, define
\[
\Delta_n := \sup_{t\ge0}\left|\mathbb{P}\left(\max_{1\le j\le d}\left|\frac{n^{-1}\sum_{i=1}^n\psi_{ij}}{\sqrt{(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}}}\right| \le t\right) - \mathbb{P}\left(\max_{1\le j\le d}|G_j| \le t\right)\right|,
\]
where $G = (G_1,\ldots,G_d)^{\top}$ is a mean zero Gaussian vector such that
\begin{equation}\label{eq:cov.G}
\mbox{Var}(G) = \mbox{Corr}\left( \frac{1}{n} \sum_{i=1}^n \psi_i \right) = \mbox{Corr}\left( \Sigma_n^{-1}V_n\Sigma_n^{-1}  \right),
\end{equation}
 and further set
\[
\Phi_{AC} := \sup_{t\ge0,\varepsilon>0}\,\frac{1}{\varepsilon}\mathbb{P}\left(t \le \max_{1\le j\le d}|G_j| \le t+\varepsilon\right),
\]
as the anti-concentration constant.
Then, Corollary~\ref{cor:Max-Statistic-Correct-Scaling} and the inequality~\eqref{eq:basic-identity-BE} with $\Psi(t) = \mathbb{P}(\max_{1\le j\le d}|G_j| \le t)$ yields
the following general Berry-Esseen bound for the normalized entries of the OLS estimator. This is the main result of this section. 



\begin{theorem}\label{thm:Berry-Esseen-OLS}
For any $\eta_n > 0$,
\begin{equation}\label{eq:deterministic-BE-bound}
\begin{split}
&\sup_{t\ge0}\left|\mathbb{P}\left(\max_{1\le j\le d}\left|\frac{\widehat{\beta}_j - \beta_j}{\sqrt{(\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}})_{jj}}}\right| \le t\right) - \mathbb{P}\left(\max_{1\le j\le d}|G_j| \le t\right)\right|
\\ 
&\quad\qquad
\le 2\Delta_n + \mathbb{P}(\mathcal{E}_{\eta_n}^c) + C_n(\eta_n)\eta_n\Phi_{AC} + \frac{d}{n},
\end{split}
\end{equation}
where $\mathcal{E}_{\eta_n}$ is the event given in \eqref{eq:key.event} and $C_n(\eta_n) := 2 \kappa_n + 2 \kappa_n \eta_n + \sqrt{2\log(2  n)}$, with $\kappa_n = \kappa(\Sigma_n^{-1/2}V_n^{1/2}) $.
\end{theorem}
%\textcolor{red}{ALE: maybe we can redefine $C_n(\eta_n)$ to be $2 + 2\eta_n + \sqrt{2\log(2 d n)}$, so that the probability bound $d/n$ becomes $1/n$.}

The above result is deterministic and holds with minimal assumptions on the data generating distributions. In particular, it does not require independence or identically distributed observations. Thus a Berry--Esseen type result for $\widehat{\beta}$ can now be proved under various assumptions of dependence among the observations, such as $m$-dependence and time series. Theorem~\ref{thm:Berry-Esseen-OLS} does not even require the sample size $n$ to be fixed number; in case $n$ is a random variable (for example, a stopping time) and $(X_i, Y_i)$ are identically distributed, then the $d/n$ term in~\eqref{eq:deterministic-BE-bound} is replaced by $d/\mathbb{E}[n]$ and $C_n(\eta_n)$ is replaced by $2\kappa_n + 2\kappa_n\eta_n + \sqrt{2\log(2\mathbb{E}[n])}$. In principle, this allows for $n$ to be a stopping time with respect to the filtration generated by $(X_i, Y_i), i\ge1$ or just a random number independent of $(X_i, Y_i), i\ge1$.%\textcolor{red}{If $n$ is random, we may need it to be indpendent of the $(X_i,Y_i)$'s}\arun{I believe that is not needed, it goes into controlling $\Delta_n$.}

As mentioned before, we took $\Psi(t) = \mathbb{P}(\max_{1\le j\le d}|G_j| \le t)$ in identity~\eqref{eq:basic-identity-BE}, which is arguably a natural choice because $n^{-1}\sum_{i=1}^n \psi_{ij}/(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}^{1/2}$ converges in distribution to $G_j$ for each $1\le j\le d$. There are other choices for $\Psi(\cdot)$ that lead to a faster rate of convergence for $\Delta_n$. Such choices include Edgeworth expansions and moment-matching distributions. Edgeworth expansions can be found in~\citet[Theorem 20.1]{bhattacharya2010normal}, but the dependence on the dimension here is not explicit. With moment-matching distributions one replaces the Gaussian vector $G$ by a different one which matches more than the first two moments of $n^{-1}\sum_{i=1}^n \psi_{ij}/(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}^{1/2}$; these can be found in~\cite{boutsikas2015penultimate} and~\cite{zhilova2016non}. We leave these refined Berry-Esseen bounds for future work.


The four terms appearing in the Berry-Esseen bound~\eqref{eq:deterministic-BE-bound} of Theorem~\ref{thm:Berry-Esseen-OLS} capture different types of approximations, both of deterministic and stochastic nature. Specifically, the quantity  $C_n(\eta_n)\eta_n$ is a bound on the linearization  error, appropriately measured in the $\| \cdot \|_{ \Sigma_n V^{-1}_n\Sigma_n }$, stemming from replacing $\widehat{\beta} - \beta$ with its  linear approximation $n^{-1}\sum_{i=1}^n \psi_i$, and  $\Delta_n$  is its corresponding Berry-Esseen bound. The term $\mathbb{P}(\mathcal{E}_{\eta_n}^c)$ collects multiple types of estimation errors: for $\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}}$, $\Sigma_n^{-1/2}\widehat{\Sigma}_n\Sigma_n^{-1/2}$ and for the norm of $n^{-1} \sum_{i=1}^n \psi_i$, which, in the i.i.d. settings studied in the next section, happens to be of the same order of magnitude and are therefore grouped together. The presence of the anti-concentration constant $\Phi_{AC}$ is standard in high-dimensional Berry--Esseen type result \citep[see, e.g.][]{chernozhukov2017detailed}, and allows to separate the effect of the estimation errors from the choice of the value of the threshold $t$ to produce an approximate $1-\alpha$ nominal coverage. 

Under a given dependence assumption on the random vectors $(X_i, Y_i), 1\le i\le n$, the right hand side of~\eqref{eq:deterministic-BE-bound} is bounded as follows. For any $\eta_n > 0$, the quantity $\mathbb{P}(\mathcal{E}_{\eta_n}^c)$ is controlled using concentration inequalities for mean zero random vectors and random matrices. For independent observations, such  inequalities are well-known and can be found, for example, in~\cite{LED91},~\cite{einmahl2008characterization},~\cite{Ver12,Vershynin18} and~\cite{tropp2016expected}. For data obeying certain types of dependence, analogous concentration inequalities can be found in~\cite{Liu13} and~\citet[Section 5 and Appendix B]{Uniform:Kuch18}. Then, choosing $\eta_n$ suitably so that $\mathbb{P}(\mathcal{E}_{\eta_n}^c)$ tends to zero as $n$ and $d$ increase yields that $C_n(\eta_n)\eta_n\Phi_{AC} = o(1)$. Finally, the quantity $\Delta_n$ is controlled using Berry-Esseen bounds for averages of mean zero random vectors with explicit dependence on the dimension. This can be accomplished in more than one way. For independent random vectors, optimal Gaussian approximation bounds holding uniformly over all convex sets as given in ~\cite{bentkus2003dependence} and~\cite{raivc2019multivariate} would imply the requirement that $d = o(n^{2/7})$. When the dimension $d$ is fixed, this rate is parametric in the sample size. However, in high-dimensional settings in which $d$ is permitted to grow with $n$, such scaling is much too pessimistic for our purposes~\citep{MR1115160}. Indeed, in our analysis we only require convergence to standard Gaussian distribution over all symmetric rectangles (and not over the much larger class of convex sets). In this case, we apply the recent high-dimensional Berry--Esseen results by~\cite{Chern17} and~\cite{koike2019notes} \citep[and, for dependent observations, by][]{ZhangWu17} where the dependence on the dimension is only poly-logarithmic. Interestingly, these bounds are sub-optimal in the fixed-dimensional case, where the implied rate would scale as $n^{-1/6}$. Recently, improvements to $n^{-1/4}$ for independent sub-Gaussian random vectors are provided in~\cite{chernozhukov2019improved}. 


%An interesting direct corollary is in the case of fixed design (this won't be covered if the result is only proved for identically distributed data). In case of fixed design $\widehat{\Sigma} = \Sigma$ and hence $\mathcal{D}_n^{\Sigma} = 0$. This implies $\mathbb{P}(\mathcal{E}_{\eta_n}^c) = 0$ for any $\eta_n > 0$. Hence taking limit as $\eta_n\downarrow0$ the result proves an upper bound which is just $\Delta_n$; there is no requirement on $d/n$ except that $d < n$. This conclusion for fixed design has nothing to do with the dependence structure of the data. 
% From Bentkus' result, $\Delta_n = O(d^{7/4}/n^{1/2})$ and from Chernozhukov's result, $\Delta_n = O(\log^6d/n)$. When using Chernozhukov's result, the requirement on scaling of $d$ with $n$ comes only from controlling the probability of event $\mathcal{E}_{\eta_n}^c$. Under exponential tail assumptions, the only requirement would be used to prove convergence to zero (essentially); this is what your draft does. Under finite number of moments, this would change drastically and I used (in my other draft) central limit theorem to convery rates for $\mathbb{P}(\mathcal{E}_{\eta_n}^c)$.  



\section{Explicit Rates in case of Independent Observations}
\label{section::explicit}


Theorem~\ref{thm:Berry-Esseen-OLS} in the previous section provides a bound on the difference between the distribution of OLS estimator to that of the Gaussian distribution without assuming any specific dependence structure on the observations $(X_i, Y_i), 1 \le i\le n$. In order to derive concrete rates from this result, it remains to construct an estimator $\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}}$ and to bound $\mathbb{P}(\mathcal{E}_{\eta_n}^c)$ for a suitable chosen $\eta_n > 0$. 

Below, we carry out this program assuming independent and identically distributed observations and in a high-dimensional framework in which the parameters of the data generating distribution, including its dimension, are allowed to vary with the sample size. In this case, letting $(X, Y)$ be identically distributed as the observations $(X_i, Y_i), 1\le i\le n$, we have that 
\[
\Sigma_n = \Sigma := \frac{1}{n}\sum_{i=1}^n \mathbb{E}[X_iX_i^{\top}] = \mathbb{E}[XX^{\top}] 
\]
and
\begin{align*}
V_n &= \mbox{Var}\left(\frac{1}{n}\sum_{i=1}^n X_i(Y_i - X_i^{\top}\beta)\right)\\ 
~&=~ \frac{1}{n^2}\sum_{i=1}^n \mbox{Var}(X_i(Y_i - X_i^{\top}\beta)) ~\overset{(a)}{=}~ \frac{1}{n}\mathbb{E}[XX^{\top}(Y - X^{\top}\beta)].
\end{align*}
The first equality follows because $(X_i, Y_i), 1\le i\le n$ are independent and $\beta$ satisfies $\sum_{i=1}^n \mathbb{E}[X_i(Y_i - X_i^{\top}\beta)] = 0$. It is interesting to note that equality (a) holds only because $\mathbb{E}[X_i(Y_i - X_i^{\top}\beta)] = 0$ for all $i$, which does not follow if the observations are non-identically distributed.
Furthermore,  the matrix $\Sigma$ can be estimated by $\widehat{\Sigma}_n$ and the matrix $V$ by the estimator
\[
\widehat{V}_n := \frac{1}{n}\times\frac{1}{n}\sum_{i=1}^n X_iX_i^{\top}(Y_i - X_i^{\top}\hat{\beta})^2. 
\]
The final plug-in estimator of the
asymptotic variance $\Sigma_n^{-1}V_n\Sigma_n^{-1}$ is the classical
{\em sandwich estimator}~\citep{White1980,Buja14}:
\[
\widehat{\Sigma}_n^{-1}\widehat{V}_n\widehat{\Sigma}_n^{-1}.
\]
%Because $\Sigma$ can be estimated by $\widehat{\Sigma}$, we can consider estimators $\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}}%$ of the form $\widehat{\Sigma}_n^{-1}\widehat{V}_n\widehat{\Sigma}_n^{-1}$ for some invertible matrix $\widehat{V}_n$. 
%
%In this section, we bound $\mathbb{P}(\mathcal{E}_{\eta_n}^c)$ for a specific $\eta_n$ (converging to zero with sample %size $n$) when the observations are independent {\color{red}and identically distributed} satisfying certain moment %restrictions. For a wider applicability, we allow the response to be heavy tailed in that it is not required to have %exponential tails but require the covariates to be sub-Gaussian (defined later).
%
%Let $(X, Y)$ be identically distributed as the observations $(X_i, Y_i), 1\le i\le n$. Then
%\[
%\widehat{\Sigma} := \frac{1}{n}\sum_{i=1}^n X_iX_i^{\top},\quad
%\]%
%and
%\[
%V_n := \mbox{Var}\left(\frac{1}{n}\sum_{i=1}^n X_i(Y_i - X_i^{\top}\beta)\right) ~=~ \frac{1}{n^2}\sum_{i=1}^n \mbox{Var%}(X_i(Y_i - X_i^{\top}\beta)) ~\overset{(a)}{=}~ \frac{1}{n}\mathbb{E}[XX^{\top}(Y - X^{\top}\beta)].
%\]%
%The first equality follows because $(X_i, Y_i), 1\le i\le n$ are independent and $\beta$ satisfies $\sum_{i=1}^n \mathbb%{E}[X_i(Y_i - X_i^{\top}\beta)] = 0$. It is interesting to note that equality (a) holds only because $\mathbb{E}[X_i(Y_i% - X_i^{\top}\beta)] = 0$ for all $i$, which does not follow if the observations are non-identically distributed. Based %on the equalities for $V_n$, we define the estimator
%\[
%\widehat{V}_n := \frac{1}{n}\times\frac{1}{n}\sum_{i=1}^n X_iX_i^{\top}(Y_i - X_i^{\top}\hat{\beta})^2. 
%\]%
%The final estimator
%$\widehat{\Sigma}_n^{-1}\widehat{V}_n\widehat{\Sigma}_n^{-1}$ of the
%asymptotic variance $\Sigma_n^{-1}V_n\Sigma_n^{-1}$ is the classical
%{\em sandwich estimator}. 
For notational convenience, set
\[
V := \mathbb{E}[XX^{\top}(Y - X^{\top}\beta)^2]\quad\mbox{and}\quad \widehat{V} := \frac{1}{n}\sum_{i=1}^n X_iX_i^{\top}(Y_i - X_i^{\top}\widehat{\beta})^2,
\]
so that $V_n = n^{-1}V$ and $\widehat{V}_n = n^{-1}\widehat{V}$.


We will impose the following, mild, assumptions on the data generating distribution. In particular, we only require moment conditions  on the distribution of the response, which can therefore be heavy-tailed. This is a significant weakening of the assumptions commonly used in the literature, where the response variable is often assumed to have moments of all order, independently of $d$ and $n$. In contrast we only assume the response variable to have $q \geq 2 $ moments, whose values may depend on the dimension $d$.  As for the covariates,  we assume  their distribution to be sub-Gaussian, though this could be  relaxed to weaker types of light-tail behavior.


We will first bound the probability of event $\mathcal{E}_{\eta_n}^c$ for a specific $\eta_n$ under the following assumptions. 
\begin{description}
	\item[\namedlabel{eq:DGP}{(\textbf{DGP})}] The observations $(X_i, Y_i)\in\mathbb{R}^d\times\mathbb{R}, 1\le i\le n$ are independent and identically distributed (i.i.d.).
	\item[\namedlabel{eq:moments-errors}{(\textbf{E})($q$)}] There exists some $q \ge 2$ and a constant $K_q\in(0, \infty)$ such that
	\[
	\Big(\mathbb{E}[|Y_i - X_i^{\top}\beta|^q]\Big)^{1/q} ~\le~ K_q < \infty,\quad\mbox{for all}\quad 1\le i\le n. 
	\]
	\item[\namedlabel{eq:covariate-subGaussian}{(\textbf{X-SG})}] There exists a constant $K_x\in(0, \infty)$ such that
	\[
	\mathbb{E}\left[\exp\left(\frac{|u^{\top}\Sigma^{-1/2}X_i|^2}{2K_x^2}\right)\right] \le 2,\quad\mbox{for all}\quad 1\le i\le n\mbox{ and }u\in S^{d-1}.
	\]
	\item[\namedlabel{eq:bounded-asymptotic-variance}{(\textbf{$\Sigma$-$V$})}] There exist constants $0 < \underline{\lambda} \le \overline{\lambda} < \infty$ such that
	\[
	\underline{\lambda} ~\le~ \lambda_{\texttt{min}}(\Sigma^{1/2} V^{-1}\Sigma^{1/2}) ~\le~ \lambda_{\texttt{max}}(\Sigma^{1/2}V^{-1}\Sigma^{1/2}) ~\le~ \overline{\lambda}.
	\]
\end{description}
We now provide some comments on these assumptions. Our main result, Theorem~\ref{thm:main-rates-thm-OLS-independence} below, remains true even if the condition~\ref{eq:DGP} 
is weakened by requiring the the observations to the independent and not necessarily identically distributed.
 %Condition~\ref{eq:DGP} requires the observations to be i.i.d. and the identical distributions part is not required for the proof of Theorem~\ref{thm:main-rates-thm-OLS-independence} below. 
 However, in this case, the parameter $\beta$ will depend on the data generating distributions in complicated ways and only satisfies the optimality condition
\[
\frac{1}{n}\sum_{i=1}^n \mathbb{E}[X_i(Y_i - X_i^{\top}\beta)] = 0,
\]
with no control on the expectation of individual summands. This leads to an impossibility in estimating the variance of $n^{-1}\sum_{i=1}^n X_i(Y_i - X_i^{\top}\beta)$, without further assumptions; see~\cite{Liu95} and~\citet[Proposition 3.5]{Bac16} for details. Condition~\ref{eq:moments-errors} requires the existence of $q$-th order moment of the ``errors'' $Y_i - X_i^{\top}\beta$ and may be weakened by assuming the response variables $Y_i$'s' to only have a finite $q$-th order moment. Indeed, observe that
\[
\Big(\mathbb{E}[|Y - X^{\top}\beta|^q]\Big)^{1/q} \le \left(\mathbb{E}[|Y|^q]\right)^{1/q} + \left(\mathbb{E}[ |(\Sigma^{-1/2}X)^{\top}\Sigma^{1/2}\beta |^q]\right)^{1/q}.
\]
Now, because $0 \leq \mathbb{E}[(Y - X^{\top}\beta)^2] = \mathbb{E}[Y^2] - \beta^\top \Sigma \beta$, we have $\|\Sigma^{1/2}\beta\| \le (\mathbb{E}[Y^2])^{1/2}$ and hence
\[
\Big(\mathbb{E}[|Y - X^{\top}\beta|^q]\Big)^{\frac{1}{q}} \le \left(\mathbb{E}[|Y|^q]\right)^{\frac{1}{q}} + (\mathbb{E}[Y^2])^{1/2}\sup_{a\in S^{d-1}}\,\left(\mathbb{E}[\|(\Sigma^{-1/2}X)^{\top}a\|^q]\right)^{1/q}.
\]
Therefore, assuming that $(\mathbb{E}[|Y|^q])^{1/q} \le \overline{K}_x < \infty$ for some $\overline{K}_x$ along with condition~\ref{eq:covariate-subGaussian} implies condition~\ref{eq:moments-errors}. For the sake of readability, we do not make the dependence on the parameter $K_q$ in Condition~\ref{eq:moments-errors} explicit in our bounds, though it could be tracked through the constants in our proofs, allowing in principle for a dependence of $K_q$ on $d$.
Condition~\ref{eq:covariate-subGaussian} is a rewording of $K_x$-sub-Gaussianity of the response variables $X_1, \ldots, X_n$ and necessarily requires $K_x \ge 1$. The condition number assumption~\ref{eq:bounded-asymptotic-variance} requires $\Sigma$ and $V$ to be of the ``same order'' and appears to be unavoidable.  Noting that
\[
V = \mathbb{E}[XX^{\top}(Y - X^{\top}\beta)^2] = \mathbb{E}\left[XX^{\top}\mathbb{E}[(Y - X^{\top}\beta)^2|X]\right],
\]
we see that condition~\ref{eq:bounded-asymptotic-variance} is satisfied if
\[
\overline{\lambda}^{-1} ~\le~ \inf_{x}\,\mathbb{E}[(Y - X^{\top}\beta)^2|X = x] ~\le~ \sup_{x}\mathbb{E}[(Y - X^{\top}\beta)^2|X = x] ~\le~ \underline{\lambda}^{-1},
\]
where $\inf_x$ and $\sup_x$ should be taken as the essential infimum and supremum with respect to the distribution of $X$. 
% \textcolor{blue}{This follows because
% \begin{align*}
% \lambda_{\max}(\Sigma^{-1/2}V\Sigma^{-1/2}) &= \sup_{\theta\in\mathbb{R}^d}\frac{\theta^{\top}\Sigma^{-1/2}V\Sigma^{-1/2}\theta}{\theta^{\top}\theta}\\ 
% &= \sup_{\theta\in\mathbb{R}^d}\,\frac{\mathbb{E}[(X^{\top}\Sigma^{-1/2}\theta)^2(Y - X^{\top}\beta)^2]}{\theta^{\top}\theta}\\
% &\le \sup_{x}\mathbb{E}[(Y - X^{\top}\beta)^2|X = x]\sup_{\theta\in\mathbb{R}^d}\frac{\theta^{\top}\mathbb{E}[\theta^{\top}\Sigma^{-1/2}XX^{\top}\Sigma^{-1/2}\theta]}{\theta^{\top}\theta}\\
% &= \sup_{x}\mathbb{E}[(Y - X^{\top}\beta)^2|X = x].
% \end{align*}
% Did I miss something?}
% \textcolor{red}{ATTN: I think it should be that the 
% \[
% \overline{\lambda}^{-1} \frac{\lambda_{\max}(\Sigma)}{\lambda_{\min}(\Sigma)} \le \inf_x
% \]
% and 
% \[
% \sup_x \le \underline{\lambda}^{-1} \frac{\lambda_{\min}(\Sigma)}{\lambda_{\max}(\Sigma)} 
% \]
% }
In particular, Condition~\ref{eq:bounded-asymptotic-variance} does not rule out the possibility of vanishing eigenvalues (in $n$ and/or $d$). Again, for readability we have not made the dependence on $\underline{\lambda}$ and $\overline{\lambda}$ explicit in our rates. Observe that $\kappa_n = \kappa(\Sigma_n^{-1/2}V_n^{1/2}) = \kappa(\Sigma^{-1/2}V^{1/2}) \le \sqrt{\widebar{\lambda}/\underline{\lambda}}$.

Finally, it is noteworthy that the constants $K_x$, $\overline{\lambda} K_q^2$, and $\overline{\lambda}/\underline{\lambda}$ are all unitless. Hence the dependence of our rates on these constants is unaffected by any re-scaling of the $X$'s or of the $Y$'s. %\arun{Need to comment on relation between $\kappa_n$ and $\widebar{\lambda}/\underline{\lambda}$.}
% \begin{center}
% \vspace{0.1in}
% [{\color{red}A Theorem combining Lemmas below comes here.}]
% \vspace{0.1in}
% \end{center}

In our first result we will derive a high-probability estimation error bounds which in turn will yield a probability bound for the event $\mathcal{E}^c_{\eta_n}$, as well as a choice for the threshold $\eta_n$. Because we impose only polynomial moment assumptions on the response, these bounds are non-trivial. The proof of the theorem, and of all the results form this section, can be found  in Appendix~\ref{appendix:main.ols}.  

\begin{theorem}\label{thm:main-rates-thm-OLS-independence}
Assume conditions~\ref{eq:DGP},~\ref{eq:covariate-subGaussian},~\ref{eq:bounded-asymptotic-variance}. If condition~\ref{eq:moments-errors} holds for some $q \ge 4\log n/(\log n - 2)$, then there exists a positive constant $C = C(q, K_x, \overline{\lambda} K_q^2, \overline{\lambda}/\underline{\lambda}) \ge 1$ depending only on its arguments such that with probability at least $1 - 3d/n - 3\sqrt{d/n}$, we have
\begin{equation}\label{eq:main-quantity-bounds}
\begin{split}
\mathcal{D}_n^{\Sigma} ~&\le~ C\sqrt{(d + \log(n/d))/n},\\
\mathcal{D}_n^{\Sigma}\left\|\frac{1}{n}\sum_{i=1}^n \psi_i\right\|_{\Sigma V_n^{-1}\Sigma} ~&\le~ C\frac{d + \log(n/d)}{\sqrt{n}}%\\ &\qquad
+ C\frac{d^{1-1/(2q)}\sqrt{\log(n/d)} + d^{1/2 - 1/(2q)}\log(n/d)}{n^{1-3/(2q)}},\\
\sup_{\theta\in\mathbb{R}^d}\left|\frac{\theta^{\top}\widehat{\Sigma}^{-1}_n\widehat{V}\widehat{\Sigma}_n^{-1}\theta}{\theta^{\top}\Sigma^{-1}V\Sigma^{-1}\theta} - 1\right| ~&\le~ C\frac{(1 + d/\sqrt{n})}{(1-4q^{-1})}\sqrt{\frac{d\log(ed) + \log n}{n}}
% \\ &\qquad
+ C\frac{d^{1-1/q}\log(ed)\log n}{n^{1-3/q}},
\end{split}
\end{equation} 
whenever all the quantities on the right hand side are smaller than $1/2$ and we recall that $\mathcal{D}_n^{\Sigma}$ is defined in \eqref{eq:Dn}. %\textcolor{red}{ATTN: in the last bound, it should be $\widehat{V}$ and not $\widehat{V}_n$, and $V$ and not $V_n$.} \arun{Shouldn't matter, because $\widehat{V}_n/V_n = \widehat{V}/V$. But we can remove $_n$. Am I missing something?}
\end{theorem}
Because $\widehat{V}_n = n^{-1}\widehat{V}$ and $V_n = n^{-1}V$, the last inequality of Theorem~\ref{thm:main-rates-thm-OLS-independence} also holds with $(\widehat{V}, V)$ replaced by $(\widehat{V}_n, V_n)$.

Note that the bounds in Theorem~\ref{thm:main-rates-thm-OLS-independence} hold only when these bounds are smaller than $1/2$ which, in particular, require that $d \le \sqrt{n}$ (because $C \ge 1$). 

The first inequality of Theorem~\ref{thm:main-rates-thm-OLS-independence} is a standard concentration inequality~\citep[see, e.g.][Theorem 1]{koltchinskii2017a} for the average of the outer product of sub-Gaussian random vectors. 
The second inequality also follows from concentration inequalities for average of zero-mean random vectors but because each terms $Y_i - X_i^{\top}\beta$ involved in the definition of $\psi_i$ only has finite polynomial moments, we require a careful use of the Fuk-Nagaev inequality of~\cite{einmahl2008characterization}. Finally, the third inequality is more complicated because $\widehat{\Sigma}^{-1}_n\widehat{V}_n\widehat{\Sigma}_n^{-1}$ is a non-linear function of averages of random matrices and $\widehat{\beta}$.   

Theorem~\ref{thm:main-rates-thm-OLS-independence} is possibly the first result providing a high-dimensional rate of convergence (in operator norm)  of the sandwich variance estimator under only polynomial $(q\ge4\log n/(\log n - 2))$ moments on the response. 
In particular, if the response variable is also sub-Gaussian (and thus has moments of all orders) and assuming a uniformly bounded condition number throughout,  the sandwich variance estimator is consistent at the usual high-dimensional rate of $\sqrt{d/n}$ (ignoring for simplicity $\log$ terms), holding, e.g., for covariance matrix estimation, provided that $d = O(\sqrt{n})$. 


%\textcolor{red}{A quick remark:  Lemma~\ref{lem:std-err-consistency} does not give a high prob bound for
%\[
%\max_{1\le j\le d} \left| \sqrt{\frac{(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}}{(\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}})_{jj}}} - 1% \right|,
%\]
%which is needed to specify $\eta_n$ (see proof of Theorem~\ref{thm:Berry-Esseen-OLS}), but for 
%\[
%\max_{1\le j\le d} \left| \frac{ (\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}})_{jj}}{  (\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}} - 1 \%right|,
%\]
%which targets the inverse of the maximal ratio of true over estimated variances. Of course, one can extract the  need %result from this, but I just wanted to make sure this has been accounted for in the rates displayed next.
%}

Substituting Theorem~\ref{thm:main-rates-thm-OLS-independence} in Theorem~\ref{thm:Berry-Esseen-OLS}, we are now ready to state a Berry-Esseen bound for the OLS estimator. See Appendix~\ref{appendix:main.ols} for the proof. 

\begin{theorem}[Berry--Esseen bound under Independence]\label{thm::berry-esseen}
Assume conditions~\ref{eq:DGP},~\ref{eq:covariate-subGaussian},~\ref{eq:bounded-asymptotic-variance}. If condition~\ref{eq:moments-errors} holds for some $q \ge 4\log n/(\log n - 2)$, then there exists a constant $C = C(q, K_x, \overline{\lambda} K_q^2, \overline{\lambda}/\underline{\lambda})$ depending only on its arguments such that
\begin{align*}
&\sup_{t\ge0}\left|\mathbb{P}\left(\max_{1\le j\le d}\left|\frac{n^{1/2}(\widehat{\beta}_j - \beta_j)}{\sqrt{(\widehat{\Sigma}_n^{-1}\widehat{V}\widehat{\Sigma}_n^{-1})_{jj}}}\right| \le t\right) - \mathbb{P}\left(\max_{1\le j\le d}|G_j| \le t\right)\right|\\ 
&\le C\frac{(\log d)^{7/6}}{n^{1/6}} + C\log(n)\left[\frac{d+\log n}{\sqrt{n}} + \frac{d\sqrt{\log n} + \sqrt{d}\log n}{d^{1/(2q)}n^{1-3/(2q)}} + \frac{d\log d\log n}{d^{1/q}n^{1 - 3/q}}\right],
\end{align*}
\end{theorem}
Because $\widehat{V}_n = n^{-1}\widehat{V}$,
\[
\frac{n^{1/2}(\widehat{\beta}_j - \beta_j)}{\sqrt{(\widehat{\Sigma}_n^{-1}\widehat{V}\widehat{\Sigma}_n^{-1})_{jj}}} = \frac{\widehat{\beta}_j - \beta_j}{\sqrt{(\widehat{\Sigma}_n^{-1}\widehat{V}_n\widehat{\Sigma}_n^{-1})_{jj}}},
\]
for which Theorem~\ref{thm:Berry-Esseen-OLS} applies.
\begin{remark}\label{rem:scaling-in-d}
Theorem~\ref{thm::berry-esseen} is proved by combining Theorems~\ref{thm:main-rates-thm-OLS-independence} and~\ref{thm:Berry-Esseen-OLS}. Note that Theorem~\ref{thm:main-rates-thm-OLS-independence} is true only under certain constraints on $d$ and $n$ (in particular, $d \le \sqrt{n}$), but Theorem~\ref{thm::berry-esseen} does not require such constraints. The reason is that if the constraints do not hold, then the bound given in Theorem~\ref{thm::berry-esseen} becomes larger than 1 and hence the claim holds trivially.
\end{remark}

Ignoring the $\log d, \log n$ terms, the bound converges to zero if 
\begin{equation}\label{eq:requirement-d-BE-OLS}
d = o\left(\min\left\{n^{1/2},\, n^{(q-3)/(q-1)},\, n^{(2q-3)/(2q-1)}\right\}\right),
\end{equation}
(we remark that $n^{(2q-3)/(2q-1)} \ge n^{(q-3)/(q-1)}$ for all $q\ge1$, so the third term in the above expression is in fact superfluous). Provided that $n, q \ge 5$, requirement~\eqref{eq:requirement-d-BE-OLS} reduces to the scaling $d = o(\sqrt{n})$, which matches the one obtained, in different settings and based on more stringent assumptions and techniques, by~\cite{Portnoy84,Portnoy85,Portnoy86,portnoy1987central,Portnoy88},~\cite{He2000} and~\cite{spokoiny2012parametric}. We point out, however, the important difference that in these and related papers, the authors prove central limit theorems under a well-specified model (e.g., assuming a linear regression function) and for  estimators normalized by their true but unknown variance. In contrast, we prove a finite-sample Berry-Esseen bound with an estimated variance. 

Importantly,  Theorem~\ref{thm::berry-esseen} further yields that the length of the individual confidence intervals for the entries of $\beta$ are of order $1/\sqrt{n}$, which amounts to a parametric accuracy rate, independent of the dimension. Indeed, for any fixed $t \in \mathbb{R}$, the length of the interval for the $j$th coordinate is ${2tn^{-1/2}}(\widehat{\Sigma}_n^{-1}\widehat{V}\widehat{\Sigma}_n^{-1})_{jj}^{1/2}$, which is
\begin{align*}
%\frac{2}{\sqrt{n}} t \sqrt{(\widehat{\Sigma}_n^{-1}\widehat{V}_n\widehat{\Sigma}_n^{-1})_{jj}} & = \\%2t    \sqrt{\frac{(\widehat{\Sigma}_n^{-1}\widehat{V}\widehat{\Sigma}_n^{-1})_{jj}}{n}}\\
\frac{2t}{\sqrt{n}} \sqrt{({\Sigma}^{-1}V{\Sigma}^{-1})_{jj}}
+ \frac{2t}{\sqrt{n}} \left( \sqrt{(\widehat{\Sigma}_n^{-1}\widehat{V}\widehat{\Sigma}_n^{-1})_{jj}} - \sqrt{({\Sigma}^{-1}V{\Sigma}^{-1})_{jj}}  \right) = O \left( \sqrt{\frac{({\Sigma}^{-1}V{\Sigma}^{-1})_{jj}}{n}} \right),
\end{align*}
where the bound stems from Theorem~\ref{thm:main-rates-thm-OLS-independence}, which  implies,  when $d = o(\sqrt{n})$ and since $\underline{\lambda}$, $\overline{\lambda}$, $q$ and $K_x$ are of constant order, that the difference inside the parenthesis in the above equation is vanishing in $n$.

In both Theorems~\ref{thm:main-rates-thm-OLS-independence} and~\ref{thm::berry-esseen}, we require $q \ge 4\log n/(\log n - 2)$; specifically, this assumption is used in the proof of Lemma \ref{lem:std-err-consistency} in Appendix~\ref{appendix:auxiliary.ols} to demonstrate a consistency rate of the sandwich estimator. For $n$ large, this amounts to requiring $q$ to be strictly larger than $4$ and this conditions seems unavoidable for the following reason. For the rates of the sandwich variance estimator in Theorem~\ref{thm:main-rates-thm-OLS-independence}, it is crucial to attain a dependence of $n^{-1/2}$ on the sample size for the difference between $n^{-1}\sum_{i=1}^n X_iX_i^{\top}(Y_i - X_i^{\top}\beta)^2$ and its expectation. Even if the $X_i$'s are univariate and bounded, the rate of $n^{-1/2}$ still requires two moments on the summands, which amounts to 4 moments on $Y_i - X_i^{\top}\beta$ or, equivalently, to $q \ge 4$. Because the $X_i$'s are not bounded but only sub-Gaussian a requirement of $q \ge 4\log n/(\log n - 2)$ seems unavoidable.




\begin{remark}[The role of of High-dimensional Central Limit Theorems]
The first term on the right hand side of the bound given in Theorem~\ref{thm::berry-esseen} is obtained using the high-dimensional Berry-Esseen bound of~\cite{koike2019notes} in order to bound $\Delta_n$ in inequality~\eqref{eq:deterministic-BE-bound}. 
We note that, more recently,~\citet[Theorem 2.1]{chernozhukov2019improved} proved a rate of $(B_n^2\log^5(dn)/n)^{1/4}$ for certain deterministic quantities  $B_n$. 
In particular, the exponent $1/6$ is improved to $1/4$. We could not use Theorem~2.1 of~\cite{chernozhukov2019improved} because it requires sub-Gaussian summands, a condition that does not hold under our assumption~\ref{eq:moments-errors}. However, we strongly believe that the first term in Theorem~\ref{thm::berry-esseen} can be replaced by $(\log^7(dn)/n)^{1/4}$; this requires proving an analogue of Theorem 2.1 of~\cite{chernozhukov2019improved} under weaker moment assumptions which we leave for future research. Finally, note that this improvement does not impact the requirement on the growth of the dimension $d$ with respect to the sample size $n$ in Theorem~\ref{thm::berry-esseen}.

%In Theorem~\ref{thm::berry-esseen}, we made use of the high-dimensional CLT of \cite{koike2019notes} to bound $\Delta_n$ in inequality~\eqref{eq:deterministic-BE-bound}. 
Rather than deploying the high-dimensional Berry-Esseen bound of \cite{koike2019notes}, the multivariate CLT of \cite{raivc2019multivariate} yields a bound of $O(d^{7/4}/n^{1/2})$ for Gaussian approximation. Relatedly, following the proof of Theorem~\ref{thm:Berry-Esseen-OLS}, one can replace the limiting Gaussian by a different distribution to obtain a faster rate for $\Delta_n$; see, e.g., \cite{zhilova2016non}. This strategy might provide a better rate of convergence for the distributional approximation. However, again, this alternative strategy would not impact the requirement on the growth of the dimension $d$ with respect to the sample size $n$, which mainly stems from the bounds in Theorem~\ref{thm:main-rates-thm-OLS-independence}.  
\end{remark}


\section{Confidence Sets for the Regression Parameters}\label{sec::confidence-sets-OLS}
In the previous section, we have proved a Gaussian approximation for the normalized least squares estimator of the projection parameter. To obtain confidence intervals for the coordinates of the projection parameter we further need to know the quantiles of $\max_{1\le j\le d}\,|G_j|$. In the current section, we provide three solutions to this issue. The first two solutions are conservative and are based on Bonferroni and {\v{S}}id{\'a}k inequalities. The final way is asymptotically exact and uses multiplier bootstrap.  
\subsection{Bonferroni and {\v{S}}id{\'a}k Method}\label{subsec:bonferroni.sidak}

We have proved that
\[
\sup_{t\ge 0}\left|\mathbb{P}\left(\max_{1\le j\le d}\left|\frac{n^{1/2}(\widehat{\beta}_j - \beta_j)}{\sqrt{(\widehat{\Sigma}_n^{-1}\widehat{V}\widehat{\Sigma}_n^{-1})_{jj}}}\right| \le t\right) - \mathbb{P}\left(\max_{1\le j\le d}|G_j| \le t\right)\right| \le r_n,
\]
for some rate $r_n$ and mean zero Gaussian random vector $G\in\mathbb{R}^d$ with unit variance on each coordinate. Taking $t = z_{\alpha/(2d)}$, where $z_{\gamma}$ represents the $(1-\gamma)$-th quantile of the standard Gaussian distribution, by symmetry and the union bound we get that
%\begin{align*}
%\mathbb{P}\left(\max_{1\le j\le d}|G_j| > z_{\alpha/(2d)}\right) &\le \sum_{j=1}^d \mathbb{P}(|G_j| > z_{\alpha/(2d)})\\ 
%&\le 2\sum_{j=1}^d \mathbb{P}\left(G_j > z_{\alpha/(2d)}\right) \le 2d\frac{\alpha}{sd} = \alpha.
%\end{align*}
%This implies that
\begin{align*}
\mathbb{P}\left(\max_{1\le j\le d}\left|\frac{n^{1/2}(\widehat{\beta}_j - \beta_j)}{\sqrt{(\widehat{\Sigma}_n^{-1}\widehat{V}\widehat{\Sigma}_n^{-1})_{jj}}}\right| \le z_{\alpha/(2d)}\right) &\ge \mathbb{P}\left(\max_{1\le j\le d}|G_j| \le z_{\alpha/(2d)}\right) - r_n\\
&\ge (1 - \alpha) - r_n.
\end{align*}
Alternatively, we can sharpen the Bonferroni confidence regions by using instead  {\v{S}}id{\'a}k's inequality~\citep[Corollary 1]{vsidak1967rectangular}, which implies that, for all $t > 0$,
\[
\mathbb{P}\left(\max_{1\le j\le d}|G_j| \le t\right) \ge \prod_{j=1}^d \mathbb{P}(|G_j| \le t).
\]
Thus,
%\[
%\mathbb{P}\left(\max_{1\le j\le d}|G_j| \le z_{(1 - (1-\alpha)^{1/d})/2}\right) \ge \prod_{j=1}^d (1 - \alpha)^{1/d} = (1 - \alpha).
%\]
%Therefore, we conclude
\begin{align*}
\mathbb{P}\left(\max_{1\le j\le d}\left|\frac{n^{1/2}(\widehat{\beta}_j - \beta_j)}{\sqrt{(\widehat{\Sigma}_n^{-1}\widehat{V}\widehat{\Sigma}_n^{-1})_{jj}}}\right| \le z_{(1 - (1-\alpha)^{1/d})/2}\right) &\ge (1 - \alpha) - r_n.
\end{align*}
For any $d \ge 1$, $\alpha\in[0, 1]$, we have $1 - (1 - \alpha)^{1/d} \ge \alpha/d$
% \[
% (1 - \alpha)^{1/d} \le 1 - \frac{\alpha}{d}\quad\Rightarrow\quad \frac{1 - (1 - \alpha)^{1/d}}{2} \ge \frac{\alpha}{2d},
% \]
and hence, $z_{(1- (1-\alpha)^{1/d})/2} \le z_{\alpha/(2d)}$. Thus, the confidence sets based on  {\v{S}}id{\'a}k's method are always smaller than the ones based on Bonferroni's adjustment and should be preferred. 
The preference of {\v{S}}id{\'a}k's method over Bonferroni's and its possible use have been discussed in~\cite{westfall1993resampling} and~\cite{drton2004model}.

\subsection{Bootstrap}
The confidence sets described in the previous section can be conservative because they do not take into account the correlation structure of $G = (G_1, \ldots, G_d)^{\top}$.  
% An application of Theorem~\ref{thm:Berry-Esseen-OLS} for confidence regions requires the computation of appropriate quantiles of $\max_{1\le j\le d}|G_j|$. 
Recall that $(G_1, \ldots, G_d)^{\top}$ has a normal distribution on $\mathbb{R}^d$ with mean zero and unknown covariance matrix given by $\mbox{corr}(\Sigma_n^{-1}V_n\Sigma_n^{-1})$. Hence one way to find the quantiles of $\max_{1\le j\le d}|G_j|$ is to generate Guassian random vectors from the distribution $$N_d(0, \mbox{corr}(\widehat{\Sigma}_n^{-1}\widehat{V}_n\widehat{\Sigma}_n^{-1}))$$ and use the sample quantiles of the maximum norm of these random vectors. This procedure is equivalent to the multiplier bootstrap which is given the following pseudocode:
\begin{enumerate}
	\item Define the estimated ``score'' vectors 
	\[
	\widehat{\psi}_i := \widehat{\Sigma}^{-1}X_i(Y_i - X_i^{\top}\widehat{\beta})~\in~\mathbb{R}^d, \quad 1 \leq i \leq n.
	\]
	From the definition of $\widehat{\beta}$, we have $\sum_{i=1}^n \widehat{\psi}_i = 0.$
	\item Fix the number of bootstrap samples $B \ge 1$. For each $1 \leq b \leq B$, generate random vectors $e_i^{(b)}\overset{iid}{\sim} N(0, 1)$, $1\le i\le n$ and compute the bootstrap statistics
	\[
	T_b ~:=~ \max_{1\le j\le d}\frac{|n^{-1}\sum_{i=1}^n e_i^{(b)}\widehat{\psi}_{i,j}|}{\sqrt{(\widehat{\Sigma}_n^{-1}\widehat{V}_n\widehat{\Sigma}_n^{-1}})_{jj}}.
	\]
\end{enumerate}
Conditionally on the data $\mathcal{D}_n := \{(X_i, Y_i):\,1\le i\le n\}$, the vector
\[
\left(\frac{n^{-1}\sum_{i=1}^n e_i^{(n)}\widehat{\psi}_{i,j}}{\sqrt{(\widehat{\Sigma}_n^{-1}\widehat{V}_n\widehat{\Sigma}_n^{-1})_{jj}}}~:\, 1\le j\le d\right)^{\top}
\]
has a normal distribution with mean zero and variance given by $\mbox{corr}(\widehat{\Sigma}_n^{-1}\widehat{V}_n\widehat{\Sigma}_n^{-1})$. This follows from the fact that $$\widehat{\Sigma}_n^{-1}\widehat{V}_n\widehat{\Sigma}_n^{-1} ~=~ \frac{1}{n^2}\sum_{i=1}^n \widehat{\psi}_i\widehat{\psi}_i^{\top}.$$
The following result proves that the empirical distribution of $T_b, 1\le b\le B$ approximates the distribution of $\max_{1\le j\le d}|G_j|$ and hence provides an approximation to the distribution of $\max_{1\le j\le d}|n^{1/2}\widehat{\beta}_j - \beta_j|/(\widehat{\Sigma}_n^{-1}\widehat{V}\widehat{\Sigma}_n)_{jj}^{1/2}$. The proof is given in Appendix~\ref{appendix:main.ols}.
\begin{theorem}[Consistency of Multiplier Bootstrap]\label{thm:multiplier-bootstrap-consistency}
Under the assumptions of Theorem~\ref{thm::berry-esseen}, for every $B \ge 1$, we have
\begin{align*}
&\sup_{t\ge0}\,\left|\frac{1}{B}\sum_{b=1}^B \mathbbm{1}\{T_b \le t\} - \mathbb{P}\left(\max_{1\le j\le d}|G_j| \le t\right)\right|\\ 
&\le \sqrt{\frac{\log(2n)}{2B}} + C\log^3(dn)\left[\left(\frac{d\log d + \log n}{n}\right)^{\frac{1}{6}}
% \\
% &\qquad
+ \left(\frac{d^{1-1/q}\log(d)\log(n)}{n^{1-3/q}}\right)^{\frac{1}{3}}\right],
\end{align*}
with probability at least $1 - (d+3)/n - \sqrt{d/n}$. Here $C = C(q, K_x, \overline{\lambda}K_q^2, \overline{\lambda}/\underline{\lambda})$ is a constant depending only on its arguments. 
\end{theorem}

A consideration similar to the one made in Remark~\ref{rem:scaling-in-d} holds true for Theorem~\ref{thm:multiplier-bootstrap-consistency}: if  $d>n$, then the result is obvious because we are claiming some event holds true with probability at least 0.

Comparing the above bootstrap bound with the Berry-Esseeen bound from Theorem~\ref{thm::berry-esseen}, we see that deploying the bootstrap procedure does not add additional requirements on the allowable scaling between $d$ and $n$.
We further remark that the usual consistency results for the bootstrap imply closeness of $\mathbb{P}(T_b \le t\big|\mathcal{D}_n)$ to $\mathbb{P}(\max_{1\le j\le d}|G_j| \le t)$, uniformly in $t$. In contrast Theorem~\ref{thm:multiplier-bootstrap-consistency} proves (uniform) closeness of the empirical bootstrap distribution $t\mapsto B^{-1}\sum_{b=1}^B \mathbbm{1}\{T_b \le t\}$ to $\mathbb{P}(\max_{1\le j\le d}|G_j| \le t)$, with a rate depending on the number $B$ of the bootstrap repetitions.

%However, the practical use of bootstrap is hindered by the finite computation ability and hence pratitioners only compute quantile estimators based on the empirical bootstrap distribution which is exactly $t\mapsto B^{-1}\sum_{b=1}^B \mathbbm{1}\{T_b \le t\}$. For these reasons, Theorem~\ref{thm:multiplier-bootstrap-consistency} proves (uniform) closeness of the empirical bootstrap distribution to $\mathbb{P}(\max_{1\le j\le d}|G_j| \le t)$. 

\section{Berry--Esseen bounds for Partial Correlations}
\label{section::partial}


It is well known that the vector of projection parameters is related to the
partial correlations between $Y$ and each component of $X$ given
the other components.
This suggests that our results should be generalizable to
partial correlations.
In this section we confirm that this is the case.
This is of interest since partial correlations
play an important role in graphical models: see, e.g., \cite{Lau96} and~\cite{drton2004model}. The results in this section sharpen complementary results in \cite{wasserman2014berry}. 


Suppose $X_1, \ldots, X_n\in\mathbb{R}^d$ are identically distributed random vectors (but not required to be independent). Let $\Sigma$ denote the $d\times d$ covariance matrix of $X_i$ and let $\Omega = \Sigma^{-1}.$ Then the $d \times d$ matrix of partial correlations is the symmetric matrix given by $\Theta = (\theta_{jk})_{j,k=1,\ldots,d}$ where
\[
\theta_{jk} := -\frac{e_j^{\top}\Sigma^{-1}e_k}{\sqrt{(e_j^{\top}\Sigma^{-1}e_j)(e_k^{\top}\Sigma^{-1}e_k)}},
\]
and $e_1, \ldots, e_d$ denote the canonical basis of $\mathbb{R}^d$. A natural estimator of $\theta_{jk}$ is given by $\widehat{\theta}_{jk}$ defined as
\[
\widehat{\theta}_{jk} := -\frac{e_j^{\top}\widehat{\Sigma}^{-1}e_k}{\sqrt{(e_j^{\top}\widehat{\Sigma}^{-1}e_j)(e_k^{\top}\widehat{\Sigma}^{-1}e_k)}}\quad\mbox{where}\quad \widehat{\Sigma}_n := \frac{1}{n}\sum_{i=1}^n (X_i - \overline{X}_n)(X_i - \overline{X}_n)^{\top},
\]
with $\overline{X}_n$ representing the (sample) average of $X_1, \ldots, X_n$. Notice that in this section, $\Sigma$ and $\widehat{\Sigma}_n$ denote the true and the sample covariance matrices, respectively,  rather than the corresponding Gram matrices as in the previous sections.

Berry--Esseen bound for the partial correlation coefficients can be derived from arguments similar, albeit more involved,  to those used to prove the results in previous sections. We begin by establishing a basic linear representation result for partial correlations. To that effect, we define the intermediate covariance ``estimator'' as
\[
\widetilde{\Sigma} := \frac{1}{n}\sum_{i=1}^n (X_i - \mu_X)(X_i - \mu_X)^{\top},\quad\mbox{where}\quad \mu_X := \mathbb{E}[X_i].
\]
In fact, this is not an estimator because of the unknown vector $\mu_X$ in the definition. For notational convenience, and with a slight abuse of notation, set
\begin{equation}\label{eq:D-sigma-notation}
\mathcal{D}_n^{\Sigma} ~:=~ \|\Sigma^{-1/2}\widehat{\Sigma}_n\Sigma^{-1/2} - I_d\|_{\mathrm{op}}. 
\end{equation}
It is important to realize that this quantity is different from the corresponding one defined in previous sections because $\widehat{\Sigma}_n$ is the sample covariance matrix. 
The following result mirrors Theorem~\ref{thm:Basic-deter-ineq} as it provides a linear approximation to the difference between the the true and estimated partial correlation coefficients in terms of influence-like functions. 
% Theorem~\ref{thm:multiplier-bootstrap-consistency} \textcolor{red}{I think Theorem~\ref{thm:multiplier-bootstrap-consistency} is the wrong reference. Maybe Lemma~\ref{lem:Basic-deter-ineq}?} 
 %Recall $\mathcal{D}_n^{\Sigma}$ from~\eqref{eq:D-sigma-notation}.
\begin{theorem}\label{thm:linear-expansion-partial-corr}
Under the assumption that $\mathcal{D}_n^{\Sigma} \le 1/2$, there exists a universal constant $C\in(0, \infty)$ such that
\[
\max_{1\le j \le k\le d}\left|\widehat{\theta}_{jk} - \theta_{jk} + \frac{1}{n}\sum_{i=1}^n \psi_{jk}(X_i)\right| \le C(\mathcal{D}_n^{\Sigma})^2 + \|\overline{X}_n - \mu_X\|_{\Sigma^{-1}}^2,
\]
 where %$\psi_{jk}(X_i)$ are mean zero random variables defined as
\begin{equation}
\begin{split}
\frac{1}{n}\sum_{i=1}^n \psi_{jk}(X_i) ~&:=~ \frac{e_j^{\top}\Sigma^{-1}(\widetilde{\Sigma} - \Sigma)\Sigma^{-1}e_k}{\sqrt{(e_j^{\top}\Sigma^{-1}e_j)(e_k^{\top}\Sigma^{-1}e_k)}}
% \\ 
% &\qquad
- \frac{\theta_{jk}}{2}\left[\frac{e_j^{\top}\Sigma^{-1}(\widetilde{\Sigma} - \Sigma)\Sigma^{-1}e_j}{e_j^{\top}\Sigma^{-1}e_j} + \frac{e_k^{\top}\Sigma^{-1}(\widetilde{\Sigma} - \Sigma)\Sigma^{-1}e_k}{e_k^{\top}\Sigma^{-1}e_k}\right].
\end{split}
\end{equation}
\end{theorem}

\begin{remark}
The functions $\psi_{jk} \colon \mathbb{R}^d \rightarrow \mathbb{R}$, $1 \leq j \leq k \leq d$, are linear functions, since the right hand side of the last expression is an average, because $\widetilde{\Sigma}$ is itself an average.
Furthermore, each  $\psi_{jk}(X_i)$ has zero expectation.
\end{remark}




Using Theorem~\ref{thm:linear-expansion-partial-corr}, we derive a high-dimensional central limit approximation  for the properly normalized partial correlation coefficients~\eqref{eq:proper-normalized-partial-correlation}. Towards that end, we note that, for each $j \leq k$, the function  $\psi_{jk}(\cdot)$ from Theorem~\ref{thm:linear-expansion-partial-corr} can be written as
\begin{equation}\label{eq:def-psi-jk}
\begin{split}
\psi_{jk}(x) &= -\bigg\{a_j(x)a_k(x) - \mathbb{E}[a_j(X_1)a_k(X_1)]\bigg\}
% \\ 
% &\qquad
- \frac{\theta_{jk}}{2}\bigg\{a_j^2(x) + a_k^2(x) - \mathbb{E}[a_j^2(X_1) + a_k^2(X_1)]\bigg\},
\end{split}
\end{equation}
where
\begin{equation}\label{eq:ajx-function}
x \in \mathbb{R}^d \mapsto a_j(x) ~:=~ \frac{(x - \mu_X)^{\top}\Sigma^{-1}e_j}{(e_j^{\top}\Sigma^{-1}e_j)^{1/2}}.
\end{equation}
Now, for a fixed $x \in \mathbb{R}^d$, define the plug-in estimator of $a_j(x)$ as
\begin{equation}\label{eq:ajhatx-function}
\widehat{a}_j(x) ~:=~ \frac{(x - \widebar{X}_n)^{\top}\widehat{\Sigma}^{-1}e_j}{(e_j^{\top}\widehat{\Sigma}^{-1}e_j)^{1/2}}.
\end{equation}
In turn, this estimator leads to an estimator $\widehat{\psi}_{jk}(x)$ of $\psi_{jk}(x)$ by replacing $a_j(x)$ and $a_k(x)$ by $\widehat{a}_j(x)$ and $\widehat{a}_k(x)$, respectively.  Formally, for any $x \in \mathbb{R}$, we let
\begin{equation}\label{eq:def-widehat-psi-jk}
\begin{split}
\widehat{\psi}_{jk}(x) ~&:=~ -\bigg\{\widehat{a}_j(x)\widehat{a}_k(x) - \mathbb{E}_n[\widehat{a}_j(X)\widehat{a}_k(X)]\bigg\}
% \\ 
% &\qquad
- \frac{\widehat{\theta}_{jk}}{2}\bigg\{\widehat{a}_j^2(x) + \widehat{a}_k^2(x) - \mathbb{E}_n[\widehat{a}_j^2(X) + \widehat{a}_k^2(X)]\bigg\},
\end{split}
\end{equation}
where, for any arbitrary, possibly random, function $f \colon \mathbb{R}^d \rightarrow \mathbb{R}$,  we set $\mathbb{E}_n[f(X)] := n^{-1}\sum_{i=1}^n f(X_i)$. Because the asymptotic variance of $\sqrt{n}(\widehat{\theta}_{jk} - \theta_{jk})$ is $\mathbb{E}[\psi_{jk}^2(X_1)]$ and its plug-in is estimator is $\sum_{i=1}^n \widehat{\psi}_{jk}^2(X_i)/n$, a proper normalization of the partial correlation coefficient is given by 
\[
\frac{\sqrt{n}(\widehat{\theta}_{jk} - \theta_{jk})}{\widehat{\zeta}_{jk}}, \quad \text{where} \quad 
\widehat{\zeta}_{jk}^2 ~:=~ {\frac{1}{n}\sum_{i=1}^n \widehat{\psi}_{jk}^2(X_i)}.
\] 


We are now ready to state the main result of this section, a high-dimensional  Berry-Esseen bound for the partial correlations of sub-Gaussian vectors. In deriving this bound, we have taken extra care in exhibiting an explicit dependence on the minimal variance of the $\psi_{jk}(X_i)$'s.
\begin{theorem}\label{thm:Berry-Esseen-bound-partial-corr}
Suppose $X_1, \ldots, X_n$ are independent and identically distributed random vectors such that
\begin{equation}\label{eq:centered-subGaussian-x}
\mathbb{E}\left[\exp\left(\frac{|u^{\top}\Sigma^{-1/2}(X_i - \mu_X)|^2}{2K_x^2}\right)\right] \le 2\quad\mbox{for all}\quad u\in S^{d-1},
\end{equation}
for some constant $K_x\in(0, \infty).$ Assume that $d \leq \sqrt{n}$. Then, there exists a constant $C = C(K_x) \in(0,\infty)$ depending only on $K_x$  such that %\textcolor{red}{should we have $j < k $ instead?}\arun{yes}
\begin{align*}
&\sup_{t\ge0}\left|\mathbb{P}\left(\max_{1\le j < k\le d}\left|\frac{\sqrt{n}(\widehat{\theta}_{jk} - \theta_{jk})}{\widehat{\zeta}_{jk}}\right| \le t\right) - \mathbb{P}\left(\max_{1\le j < k\le d}|G_{jk}| \le t\right)\right|\\ 
&\qquad\le C \left( \frac{(\log(d\vee n))^{5/6}}{n^{1/6}} + \eta_n \right),%C\frac{\sqrt{\log(dn)}(d + \log n) + \log(dn)\sqrt{d + \log n}}{\sqrt{n}},
\end{align*}
for a mean zero Gaussian vector $(G_{jk})_{1\le j < k\le d}$ with the covariance matrix satisfying $\mathrm{cov}(G_{jk}, G_{j'k'}) = \mathrm{corr}(\psi_{jk}(X_1), \psi_{j'k'}(X_1))$ and where
\[
\eta_n := \frac{K_x^4}{\zeta_{\min}}\frac{d + \log n}{\sqrt{n}} ~+~ \left(\frac{K_x^8}{\zeta_{\min}^2} + \frac{K_x^6}{\zeta^3_{\min}}\right)\sqrt{\frac{(d + \log n)\log(dn)}{n}},
\]
with
\[
\zeta_{\min} := \min_{1 \leq j < k \leq d} \left(\mathbb{E}[\psi^2_{jk}(X_i)]\right)^{1/2}.
\]

\end{theorem}




\begin{remark}
Unlike in Theorem~\ref{thm:linear-expansion-partial-corr}, in Theorem~\ref{thm:Berry-Esseen-bound-partial-corr}, we only consider the maximum over $1\le j < k \le d$ because, by construction, $\widehat{\theta}_{jj} = \theta_{jj} = 1$ for all $1\le j\le d$. 

Ignoring log terms, the upper bound on the distributional approximation holds true when $d = o(\sqrt{n})$. Should the  assumption of sub-Gaussianity be relaxed to moment bounds only, the requirement on $d$ will depend on the number of moments of the $X_i$'s.
\end{remark}


\begin{remark}
Although Theorem~\ref{thm:Berry-Esseen-bound-partial-corr} is proved for independent random vectors, it is easy to obtain a result similar to Theorem~\ref{thm:Berry-Esseen-OLS} for arbitrary random vectors. %Theorem~\ref{thm:linear-expansion-partial-corr} does not account for heteroscedasticity of $\widehat{\theta}_{jk}$ across $1\le j < k\le d$. Under the assumption that $\min_{1\le j < k\le d}\mbox{Var}(\psi_{jk}(X_1))$ is bounded away from zero, Theorem~\ref{thm:linear-expansion-partial-corr} implies
% \begin{equation}\label{eq:proper-normalized-partial-correlation}
% \max_{1\le j < k\le d}\frac{|\widehat{\theta}_{jk} - \theta_{jk} + n^{-1}\sum_{i=1}^n \psi_{jk}(X_i)|}{\sqrt{\mbox{Var}(\psi_{jk}(X_1))}} ~\le~ \frac{C(\mathcal{D}_n^{\Sigma})^2 + \|\overline{X}_n - \mu_X\|_{\Sigma^{-1}}^2}{\min_{1\le j < k\le d}\sqrt{\mbox{Var}(\psi_{jk}(X_1))}}.
% \end{equation}
% Following the proof of Theorem~\ref{thm:Berry-Esseen-bound-partial-corr} yields a result for the properly normalized partial correlation estimators.
\end{remark}

For inference, one needs to estimate the quantiles of $\max_{1\le j < k\le d}|G_{jk}|$ in order to produce simultaneous confidence intervals for the partial correlation coefficients. An easy solution is to simply apply Bonferroni's or {\v{S}}id{\'a}k's correction for multiple parameters -- in this case ${d \choose 2} $--  as described above in Section~\ref{subsec:bonferroni.sidak}.

Alternatively, (asymptotically) sharper results may be obtained with the multiplier bootstrap, described as follows:
\begin{enumerate}
\item Define the estimated ``score'' vectors $\widehat{\psi}_{jk}(X_i)$ as above. From the definition, it follows that $\sum_{i=1}^n \widehat{\psi}_{jk}(X_i) = 0$ for all $1\le j < k\le d$.
\item Fix the number of bootstrap samples $B \ge 1$. Generate random vectors $e_i^{(b)}\overset{iid}{\sim}N(0, 1)$, $1\le i\le n, 1\le b\le B$ and compute the bootstrap statistics
\[
T_b ~:=~ \max_{1\le j < k\le d}\frac{|n^{-1/2}\sum_{i=1}^n e_i^{b}\widehat{\psi}_{jk}(X_i)|}{\sqrt{n^{-1}\sum_{i=1}^n \widehat{\psi}_{jk}^2(X_i)}}.
\]
\end{enumerate}
The following result proves that the empirical distribution of $T_b, 1\le b\le B$ approximates the distribution of $\max_{1\le j < k\le d}|G_{jk}|$.
\begin{theorem}[Consistency of Multiplier Bootstrap]\label{eq:multplier-bootstrap-consistency-partial-corr}
Under the assumptions of Theorem~\ref{thm:Berry-Esseen-bound-partial-corr}, for every $B\ge1$, we have with probability at least $1 - C/n$,
\begin{align*}
&\sup_{t\ge0}\left|\frac{1}{B}\sum_{b=1}^B \mathbbm{1}\{T_b \le t\} - \mathbb{P}\left(\max_{1\le j < k\le d}|G_{jk}| \le t\right)\right|\\ 
&\quad\le \sqrt{\frac{\log(2n)}{2B}} + CK_x^{2}(\log d)^{{2}/{3}}\left(\frac{d + \log n}{n}\right)^{{1}/{6}},
\end{align*}
whenever the right hand side is less than 1.
\end{theorem}
In Theorem~\ref{eq:multplier-bootstrap-consistency-partial-corr}, we make all the assumptions used in Theorem~\ref{thm:Berry-Esseen-bound-partial-corr}, in particular, we also assume $d \le \sqrt{n}$.
%\textcolor{blue}{The condition $\tilde{\Delta}_0 \le 1/2$ is not required for the result in the sense that we show that $\tilde{\Delta}_0 \le 1/2$ with probability at least $1 - C/n$. See the last two displays in the proof of Theorem 14.} \textcolor{red}{I see. I think we need to be a bit more explicit about the scaling in $d$. I have flagged few places in the proofs. Search for ``ATTN:''.}





\section{Conclusion}
\label{section::conclusion}

We have provided explicit Berry-Esseen bounds
for mis-specified linear models. The bounds are
derived based on deterministic inequalities that do not
require any specific independence or dependence assumptions
on the observations. 
Explicit requirements on the growth of dimension $d$ 
in terms of the sample size $n$ are given for an asymptotic
normal approximation when the observations are independent
and identically distributed. 
% The requirement is $d=o(\sqrt{n})$, 
% when the response has finite 4 moments.
The Berry--Esseen bounds as well as the bootstrap consistency guarantees here allow for construction of valid confidence sets
for the projection parameter provided that $d = o(\sqrt{n})$. 
Using the methods in
\cite{kuchibhotla2018valid}, confidence sets for the projection parameters can be constructed even for larger $d$ .
However, these sets are not rectangles and the projected
confidence intervals for individual projection parameters 
are much wider than those obtained here. The confidence 
intervals we obtained here have width of order $O(1/\sqrt{n})$,
whenever $d = o(\sqrt{n})$.

All the results are derived without any structural or sparsity assumptions on the projection parameter $\beta$ (and hence an \emph{assumption-lean} setting), unlike much of the recent literature on high-dimensional linear regression. Because we consider the ordinary least squares as our estimator, imposing any sparsity assumption on $\beta$ will not impact the final results; in particular, the estimator is still asympotically normal when centered at its target. If one uses different estimators designed to produce sparse estimates, then these estimators cannot be uniformly $\sqrt{n}$-consistent; see ~\cite{potscher2009confidence}. Further, if one applies debiasing~\citep{javanmard2014confidence,vandegeer2014asymptotically,zhang2014confidence} on the sparse estimator, then such an estimator has asymptotically the same behavior as the OLS estimator because the OLS estimator is semiparametrically efficient for the projection parameter $\beta$~\cite[Example 5]{Levit76}. 

In the following, we describe two interesting future directions.
\begin{enumerate}
\item Our confidence intervals have width of order $n^{-1/2}$ whenever $d = o(\sqrt{n})$. We believe this requirement on the dimension is optimal in order to obtain $n^{-1/2}$ width intervals. This conjecture is obtained from the results of~\cite{cai2017confidence}; the authors prove that the minimax width of a confidence intervals for individual coordinates in a $k$-sparse linear regression is $n^{-1/2} + k(\log d)/n$ whenever $k = O(\min\{d^{\gamma}, n/\log d\})$ (for $\gamma < 1/2$). We believe the correct formulation of the minimax width is $n^{-1/2} + k\log(ed/k)/n$ for all $1\le k\le d$, in which case taking $k = d$ yields the minimax width $n^{-1/2} + k/n$. This rate becomes $n^{-1/2}$ only when $d = O(\sqrt{n})$. This raises several interesting questions: ``What is the analogue of our results when $d \gg \sqrt{n}$? What kind of asymptotic distribution can one expect? What is a confidence set that works simultaneously for all $d \le n$? Does such a set still center at $\widehat{\beta}$?'' 
\item It would be interesting to develop similar bounds for
other mis-specified parametric models such as a generalized linear
models (GLMs). The deterministic inequalities of~\cite{2018arXiv180905172K}
imply results similar to Theorem~\ref{thm:Basic-deter-ineq} and hence
a parallel set of results for GLMs could be obtained. Of course, it involves non-trivial calculations to derive sandwich consistency which is
relatively easy for linear models because the objective function is quadratic.  
\end{enumerate}

\newpage

\begin{appendices}

\begin{center}
{\Large {\bf Appendix}}
\end{center}

\section{Proofs of the Results from Section~\ref{section::determiniswtic}}
\label{appendix:deterministic}


\begin{proof}[Proof of Theorem \ref{thm:Basic-deter-ineq}]
By the optimality of $\widehat{\beta}$, we have the normal equations $\widehat{\Sigma}_n\widehat{\beta} = \widehat{\Gamma}_n.$
Subtracting $\widehat{\Sigma}_n\beta\in\mathbb{R}^d$ from both sides, we get
$\widehat{\Sigma}_n(\widehat{\beta} - \beta) = \widehat{\Gamma}_n - \widehat{\Sigma}_n\beta,$
which is equivalent to
\[\textstyle
(\Sigma^{-1/2}_n\widehat{\Sigma}_n\Sigma_n^{-1/2})\Sigma^{1/2}_n(\widehat{\beta} - \beta) ~=~ \Sigma^{-1/2}_n(\widehat{\Gamma}_n - \widehat{\Sigma}_n\beta),
\]
since $\Sigma_n$ is invertible.
Adding and subtracting $\Sigma^{-1/2}_n (\widehat{\beta}  - \beta)$ on both sides further yields the identity
% $\Sigma^{1/2}(\widehat{\beta} - \beta) - \Sigma^{-1/2}(\widehat{\Gamma} - \widehat{\Sigma}\beta) = \left(I_d - \Sigma^{-1/2}\Sigma\Sigma^{-1/2}\right)\Sigma^{1/2}(\widehat{\beta} - \beta).$
% Taking Euclidean norm on both sides yields
% \begin{align}\label{eq:Almost-Final-Bound}
% \begin{split}
% \left\|\Sigma^{1/2}\big[\widehat{\beta} - \beta - \Sigma^{-1}(\widehat{\Gamma} - \widehat{\Sigma}\beta)\big]\right\| ~&=~ \|(I_d - \Sigma^{-1/2}\widehat{\Sigma}\Sigma^{-1/2})\Sigma^{1/2}(\widehat{\beta} - \beta)\|\\
% ~&\le~ \|I_d - \Sigma^{-1/2}\widehat{\Sigma}\Sigma^{-1/2}\|_{\mathrm{op}}\|\Sigma^{1/2}(\widehat{\beta} - \beta)\|\\
% ~&=~ \mathcal{D}_n^{\Sigma}\|\widehat{\beta} - \beta\|_{\Sigma},
% \end{split}
% \end{align}
% where the inequality follows from the definition of the operator norm, $\|\cdot\|_{\mathrm{op}}$. 
\[
\Sigma ^{1/2}_n\left[\widehat{\beta}  - \beta  - \Sigma ^{-1}_n(\widehat{\Gamma}_n  - \widehat{\Sigma}_n \beta )\right] ~=~ (I_d - \Sigma_n ^{-1/2}\widehat{\Sigma}_n \Sigma_n ^{-1/2})\Sigma_n ^{1/2}(\widehat{\beta}  - \beta ).
\]
Taking the Euclidean norm, we see that
\begin{equation}\label{eq:influence-error-bound}
\begin{split}
\|\widehat{\beta} - \beta - \Sigma_n^{-1}(\widehat{\Gamma}_n - \widehat{\Sigma}_n\beta)\|_{\Sigma_n} ~&=~ \|(I_d - \Sigma_n ^{-1/2}\widehat{\Sigma}_n \Sigma_n ^{-1/2})\Sigma_n ^{1/2}(\widehat{\beta}  - \beta )\|\\
~&\le~ \|I_d - \Sigma_n ^{-1/2}\widehat{\Sigma}_n \Sigma_n ^{-1/2}\|_{\mathrm{op}}\|\widehat{\beta}  - \beta\|_{\Sigma_n}\\
~&=~ \mathcal{D}_n^{\Sigma}\|\widehat{\beta} - \beta\|_{\Sigma_n}.
\end{split}
\end{equation}
The triangle inequality and the previous bound imply that
\begin{align*}
\|\widehat{\beta} - \beta\|_{\Sigma_n} ~&\le~ \|\Sigma_n^{-1}(\widehat{\Gamma}_n - \widehat{\Sigma}_n\beta)\|_{\Sigma_n} + \|\widehat{\beta} - \beta - \Sigma_n^{-1}(\widehat{\Gamma}_n - \widehat{\Sigma}_n\beta)\|_{\Sigma_n}\\
~&\le~ \|\Sigma_n^{-1}(\widehat{\Gamma}_n - \widehat{\Sigma}_n\beta)\|_{\Sigma_n} + \mathcal{D}_n^{\Sigma}\|\widehat{\beta} - \beta\|_{\Sigma_n},
\end{align*}
and hence (using the assumption that $\mathcal{D}_n^{\Sigma} < 1$) that
\begin{equation}\label{eq:bound-on-estimation-error}
\|\widehat{\beta} - \beta\|_{\Sigma_n} ~\le~ \frac{1}{(1 - \mathcal{D}_n^{\Sigma})}\|\Sigma_n^{-1}(\widehat{\Gamma}_n - \widehat{\Sigma}_n\beta)\|_{\Sigma_n}.
\end{equation}
Combining~\eqref{eq:bound-on-estimation-error} and~\eqref{eq:influence-error-bound} we conclude that
\begin{equation}\label{eq:almost-final-inequality}
\|\widehat{\beta} - \beta - \Sigma_n^{-1}(\widehat{\Gamma}_n - \widehat{\Sigma}_n\beta)\|_{\Sigma_n} ~\le~ \frac{\mathcal{D}_n^{\Sigma}}{(1 - \mathcal{D}_n^{\Sigma})}\|\Sigma_n^{-1}(\widehat{\Gamma}_n - \widehat{\Sigma}_n\beta)\|_{\Sigma_n}.
\end{equation}
To obtain a bound in the $\| \cdot \|_{\Sigma_n V_n^{-1}\Sigma_n}$ norm instead of the $\| \cdot \|_{\Sigma_n}$ norm, note that, for any $\theta\in\mathbb{R}^d$,
\[
\|\theta\|_{\Sigma_nV_n^{-1}\Sigma_n} = \|V_n^{-1/2}\Sigma_n\theta\| ~\le~ \|V_n^{-1/2}\Sigma_n^{1/2}\|_{\mathrm{op}}\|\theta\|_{\Sigma_n}, 
\]
and
\[
\|\theta\|_{\Sigma_n} = \|\Sigma_n^{1/2}\theta\| = \|\Sigma_n^{-1/2}V_n^{1/2}V_n^{-1/2}\Sigma_n\theta\| ~\le~ \|\Sigma_n^{-1/2}V_n^{1/2}\|_{\mathrm{op}}\|\theta\|_{\Sigma_n V_n^{-1}\Sigma_n}.
\]
After substituting these inequalities in~\eqref{eq:almost-final-inequality} we arrive at the bound
\[
\|\widehat{\beta} - \beta - \Sigma_n^{-1}(\widehat{\Gamma}_n - \widehat{\Sigma}_n\beta)\|_{\Sigma_n V_n^{-1}\Sigma_n} ~\le~ \frac{\kappa(\Sigma_n^{-1/2}V_n^{1/2})\mathcal{D}_n^{\Sigma}}{(1 - \mathcal{D}_n^{\Sigma})}\|\Sigma_n^{-1}(\widehat{\Gamma}_n - \widehat{\Sigma}_n\beta)\|_{\Sigma_n V_n^{-1}\Sigma_n}.
\]
The claim now follows because, by definition,  $n^{-1}\sum_{i=1}^n \psi_i = \Sigma_n^{-1}(\widehat{\Gamma}_n - \widehat{\Sigma}_n\beta)$.
%Multiplying on the left both sides by $V ^{-1/2}_n$ and applying the Euclidean norm, we get
%\begin{align*}
%&\|\widehat{\beta}  - \beta  - \Sigma ^{-1}(\widehat{\Gamma}  - \widehat{\Sigma} \beta )\|_{\Sigma V ^{-1}_n\Sigma }\\ 
%&\qquad\le \|V ^{-1/2}_n(I_{d} - \Sigma ^{-1/2}\widehat{\Sigma} \Sigma ^{-1/2})V ^{1/2}_nV ^{-1/2}_n\Sigma ^{1/2}(\widehat{\beta}  - \beta )\|\\
%&\qquad\le \|V ^{-1/2}_n(I_{d} - \Sigma ^{-1/2}\widehat{\Sigma} \Sigma ^{-1/2})V ^{1/2}_n\|_{\mathrm{op}}\|\widehat{\beta}  - \beta \|_{\Sigma V ^{-1}_n\Sigma }\\
%&\qquad= \mathcal{D} ^{\Sigma}\|\widehat{\beta}  - \beta \|_{\Sigma V ^{-1}_n\Sigma }.
%\end%{align*}
%The last equality above follows from the fact that $\|AB\|_{\mathrm{op}} = \|BA\|_{\mathrm{op}}$. This implies
% Multiplying  on the left by $V ^{-1/2}_n \Sigma^{1/2}$ and applying the Euclidean norm, we get that 
% \begin{align*}
% &\|\widehat{\beta}  - \beta  - \Sigma ^{-1}(\widehat{\Gamma}  - \widehat{\Sigma} \beta )\|_{\Sigma V ^{-1}_n\Sigma }\\ 
% & \qquad =  \|V ^{-1/2}_n \Sigma^{1/2}(I_{d} - \Sigma ^{-1/2}\widehat{\Sigma} \Sigma ^{-1/2}) \Sigma^{-1/2} V ^{1/2}_nV ^{-1/2}_n\Sigma(\widehat{\beta}  - \beta )\|\\
% &\qquad\le \|V ^{-1/2}_n \Sigma^{1/2} (I_{d} - \Sigma ^{-1/2}\widehat{\Sigma} \Sigma ^{-1/2})\Sigma^{-1/2} V ^{1/2}_n\|_{\mathrm{op}}\|\widehat{\beta}  - \beta \|_{\Sigma V ^{-1}_n\Sigma }\\
% &\qquad\le \textcolor{red}{\|V ^{-1/2}_n \Sigma^{1/2} \|_{\mathrm{op}} \| \Sigma^{-1/2} V ^{1/2}_n\|_{\mathrm{op}}} \mathcal{D} ^{\Sigma}\|\widehat{\beta}  - \beta \|_{\Sigma V ^{-1}_n\Sigma },%\\
% %& \qquad =   \mathcal{D} ^{\Sigma}\|\widehat{\beta}  - \beta \|_{\Sigma V ^{-1}_n\Sigma },
% \end{align*}
% where in inequalities we have applied the matrix inequality $\| A B \|_{\mathrm{op}} \leq \|A\|_{\mathrm{op}} \|B\|_{\mathrm{op}}$ repeatedly. The previous inequality along with the triangle inequality implies that
% \begin{align*}
% & \|\widehat{\beta}  - \beta \|_{\Sigma V ^{-1}_n\Sigma }\\
% & \qquad  \leq \|\widehat{\beta}  - \beta  - \Sigma ^{-1}(\widehat{\Gamma}  - \widehat{\Sigma} \beta )\|_{\Sigma V ^{-1}_n\Sigma } + \| \Sigma ^{-1}(\widehat{\Gamma}  - \widehat{\Sigma} \beta ) \|_{\Sigma V ^{-1}_n\Sigma }\\
% & \qquad  \leq \textcolor{red}{\|V ^{-1/2}_n \Sigma^{1/2} \|_{\mathrm{op}} \| \Sigma^{-1/2} V ^{1/2}_n\|_{\mathrm{op}}} \mathcal{D}_n^{\Sigma} \|\widehat{\beta}  - \beta \|_{\Sigma V ^{-1}_n\Sigma } + \| \Sigma ^{-1}(\widehat{\Gamma}  - \widehat{\Sigma} \beta ) \|_{\Sigma V ^{-1}_n\Sigma }.
% \end{align*}
% Rearranging, we obtain that
% \begin{equation}\label{eq:Reformulation-corollary-uniform}
% %\|\widehat{\beta}  - \beta  - \Sigma ^{-1}(\widehat{\Gamma}  - \widehat{\Sigma} \beta )\|_{\Sigma V ^{-1}_n\Sigma } \le \frac{\mathcal{D}_n^{\Sigma} }{(1 - \mathcal{D} ^{\Sigma})_+}\|\Sigma ^{-1}(\widehat{\Gamma}  - \widehat{\Sigma} \beta )\|_{\Sigma V ^{-1}_n\Sigma }.
% \\|\widehat{\beta}  - \beta  - \Sigma ^{-1}(\widehat{\Gamma}  - \widehat{\Sigma} \beta )\|_{\Sigma V ^{-1}_n\Sigma } \le \frac{1 }{(1 - \textcolor{red}{\|V ^{-1/2}_n \Sigma^{1/2} \|_{\mathrm{op}} \| \Sigma^{-1/2} V ^{1/2}_n\|_{\mathrm{op}}}\mathcal{D} ^{\Sigma})}\|\Sigma ^{-1}(\widehat{\Gamma}  - \widehat{\Sigma} \beta )\|_{\Sigma V ^{-1}_n\Sigma },
% \end{equation}
% from which the claim follows, since $n^{-1}\sum_{i=1}^n \psi_{\beta}(X_i, Y_i) = \Sigma ^{-1}(\widehat{\Gamma}  - \widehat{\Sigma} \beta )$ by definition.
\end{proof}

\begin{proof}[Proof of the Corollary \ref{cor:Max-Statistic-Correct-Scaling}]
Observe that, for any $x\in\mathbb{R}^{d}$ and any invertible matrix $A$,
\begin{align}\label{eq:Scaled-Euclidean-Maximum-Comparison}
\begin{split}
\|x\|_A = \|A^{1/2}x\| &= \max_{\theta\in\mathbb{R}^{d}}\frac{\theta^{\top}x}{\sqrt{\theta^{\top}A^{-1}\theta}}\\ &\geq \max_{\theta \in \{ \pm e_1,\ldots,\pm e_d \}} \frac{\theta^{\top}x}{\sqrt{\theta^{\top}A^{-1}\theta}} = \max_{1\le j\le d}\,\frac{|x_j|}{\sqrt{(A^{-1})_{jj}}}.%\max_{\substack{\theta=\pm e_j,\\1\le j\le d}} 
\end{split}
\end{align}
The result follows from Theorem~\ref{thm:Basic-deter-ineq}.
\end{proof}


\begin{proof}[Proof of Theorem \ref{thm:Berry-Esseen-OLS}]
On the event $\mathcal{E}_{\eta_n}$, we have that
\[
%\frac{1}{1 - \mathcal{D}_n^{\Sigma}} \le 2,\quad \mathcal{D}_n^{\Sigma}\left\|\frac{1}{n}\sum_{i=1}^n \psi_i\right\|_{\Sigma V^{-1}_n\Sigma} \le \eta_n,\quad\mbox{and}\quad \max_{1\le j\le d}\sqrt{\frac{(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}}{(\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}})_{jj}}} \le 1 + \eta_n.
\frac{1}{1 - \mathcal{D}_n^{\Sigma}} \le 2,\quad \mathcal{D}_n^{\Sigma}\left\|\frac{1}{n}\sum_{i=1}^n \psi_i\right\|_{\Sigma_n V^{-1}_n\Sigma_n} \le \eta_n,\quad\mbox{and}\quad \max_{1\le j\le d}\sqrt{\frac{(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}}{(\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}})_{jj}}} \le 1 + \eta_n.
\]
Hence Corollary~\ref{cor:Max-Statistic-Correct-Scaling} yields
\[
\max_{1\le j\le d}\left|\frac{\widehat{\beta}_j - \beta_j - n^{-1}\sum_{i=1}^n \psi_{ij}}{\sqrt{(\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}})_{jj}}}\right| \le 2\kappa_n\eta_n{(1 + \eta_n)}.
\]
Notice that the denominator involves an estimator of the ``asymptotic'' standard deviation. Therefore, on the event $\mathcal{E}_{\eta_n}$,
\begin{align*}
&\max_{1\le j\le d}\left|\frac{\widehat{\beta}_j - \beta_j}{\sqrt{(\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}})_{jj}}} - \frac{n^{-1}\sum_{i=1}^n \psi_{ij}}{\sqrt{(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}}}\right|\\ 
&\qquad\le 2\kappa_n\eta_n(1 + \eta_n) ~+~ \max_{1\le j\le d}\left|\frac{n^{-1}\sum_{i=1}^n \psi_{ij}}{\sqrt{(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}}}\right|\times\max_{1\le j\le d}\left|\sqrt{\frac{(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}}{(\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}})_{jj}}} - 1\right|\\
&\qquad\le 2\kappa_n\eta_n(1 + \eta_n) ~+~ \eta_n\max_{1\le j\le d}\left|\frac{n^{-1}\sum_{i=1}^n \psi_{ij}}{\sqrt{(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}}}\right|
\end{align*}
By standard Gaussian concentration and a union bound,
\[
\mathbb{P}\left(\max_{1\le j\le d}|G_j| \ge \sqrt{2\log(2 n  )}\right) \le \frac{d}{n},
\]
and the definition of $\Delta_n$ implies that
\begin{align*}
\mathbb{P}\left(\max_{1\le j\le d}\left|\frac{n^{-1}\sum_{i=1}^n \psi_{ij}}{\sqrt{(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}}}\right| \ge \sqrt{2\log(2n)}\right) &\le \mathbb{P}\left(\max_{1\le j\le d}|G_j| \ge \sqrt{2\log(2n)}\right) + \Delta_n\\
&\le \frac{d}{n} + \Delta_n.
\end{align*}
Combining  these inequalities we obtain  that that there exists an event of probability at least $1 - \Delta_n - d/n - \mathbb{P}(\mathcal{E}_{\eta_n}^c)$ such that, on that event, it holds that 
\begin{align*}
\max_{1\le j\le d}\left|\frac{\widehat{\beta}_j - \beta_j}{\sqrt{(\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}})_{jj}}} - \frac{n^{-1}\sum_{i=1}^n \psi_{ij}}{\sqrt{(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}}}\right| & \leq  \eta_n\times \left[ 2\kappa_n + 2\kappa_n \eta_n + \sqrt{2\log(2n)} \right]\\
& = \eta_n C_n(\eta_n),
\end{align*}
by the definition of $C(\eta_n)$.
 Hence for any $t \ge 0$ and $\eta_n > 0$, we have that
\begin{align*}
\mathbb{P}\left(\max_{1\le j\le d}\left|\frac{\widehat{\beta}_j - \beta_j}{\sqrt{(\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}})_{jj}}}\right| \le t\right) &\le \mathbb{P}\left(\max_{1\le j\le d}\left|\frac{n^{-1}\sum_{i=1}^n \psi_{ij}}{\sqrt{(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}}}\right| \le t + C_n(\eta_n)\eta_n\right)\\ &\qquad+ \mathbb{P}(\mathcal{E}_{\eta_n}^c) + d/n + \Delta_n\\
\mathbb{P}\left(\max_{1\le j\le d}\left|\frac{\widehat{\beta}_j - \beta_j}{\sqrt{(\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}})_{jj}}}\right| \le t\right) &\ge \mathbb{P}\left(\max_{1\le j\le d}\left|\frac{n^{-1}\sum_{i=1}^n \psi_{ij}}{\sqrt{(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jj}}}\right| \le t - C_n(\eta_n)\eta_n\right)\\ &\qquad- \mathbb{P}(\mathcal{E}_{\eta_n}^c) - d/n - \Delta_n.
\end{align*}
Next, from the definition of $\Delta_n$ and $\Phi_{AC}$, we finally obtain that
\begin{align*}
\mathbb{P}\left(\max_{1\le j\le d}\left|\frac{\widehat{\beta}_j - \beta_j}{\sqrt{(\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}})_{jj}}}\right| \le t\right) &\le \mathbb{P}\left(\max_{1\le j\le d}|G_j| \le t + C_n(\eta_n)\eta_n\right) + 2\Delta_n + \mathbb{P}(\mathcal{E}_{\eta_n}^c) + \frac{d}{n}\\
&\le \mathbb{P}\left(\max_{1\le j\le d}|G_j| \le t\right) + 2\Delta_n + \mathbb{P}(\mathcal{E}_{\eta_n}^c) + C_n(\eta_n)\eta_n \Phi_{AC} + \frac{d}{n}\\
\mathbb{P}\left(\max_{1\le j\le d}\left|\frac{\widehat{\beta}_j - \beta_j}{\sqrt{(\widehat{\Sigma_n^{-1}V_n\Sigma_n^{-1}})_{jj}}}\right| \le t\right) &\ge \mathbb{P}\left(\max_{1\le j\le d}|G_j| \le t - C_n(\eta_n)\eta_n\right) - 2\Delta_n - \mathbb{P}(\mathcal{E}_{\eta_n}^c) - \frac{d}{n}\\
&\ge \mathbb{P}\left(\max_{1\le j\le d}|G_j| \le t\right) - 2\Delta_n - \mathbb{P}(\mathcal{E}_{\eta_n}^c) - C_n(\eta_n)\eta_n \Phi_{AC} - \frac{d}{n},
\end{align*}
as claimed.
\end{proof}



\section{Proof of the Main Results from Sections~\ref{section::explicit} and~\ref{sec::confidence-sets-OLS} (Projection Parameters)}
\label{appendix:main.ols}

\begin{proof}[Proof of Theorem \ref{thm:main-rates-thm-OLS-independence}]
The first inequality follows from Lemma~\ref{lem:concentration-of-covariance} with $\delta = \sqrt{d/n}$. The second inequality follows from the first and Lemma~\ref{lem:concentration-influence-function} with $\delta = \sqrt{d/n}, \eta_n = q/(2\log(C_qn^{3/2}/d^{1/2}))$. The third inequality follows from Lemma~\ref{lem:std-err-consistency}.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm::berry-esseen}]
Define
\[
\eta_n = C\left[\frac{d + \log n}{\sqrt{n}} + \frac{d\sqrt{\log n} + d^{1/2}\log n}{d^{1/(2q)}n^{1 - 3/(2q)}} + \sqrt{\frac{d\log(ed) + \log n}{n}}+ \frac{d\log(d) + \log n}{d^{1/q}n^{1-3/q}}\right],
\]
where the constant $C = C(q, K_x, \widebar{\lambda}K_q^2, \widebar{\lambda}/\underline{\lambda})$ is the same as in Theorem~\ref{thm:main-rates-thm-OLS-independence}. 
We complete the proof by considering two cases (1) $\eta_n \le 1/2$, or (2) $\eta_n > 1/2$. 
\paragraph{Case (1): $\eta_n \le 1/2$} In this case, Theorem~\ref{thm:main-rates-thm-OLS-independence} implies that
\[
\mathbb{P}\left(\mathcal{E}_{\eta_n}^{c}\right) \le \frac{3d}{n} + \sqrt{\frac{d}{n}},
\]
and hence Theorem~\ref{thm:Berry-Esseen-OLS} yields
\begin{align*}
&\sup_{t\ge 0}\left|\mathbb{P}\left(\max_{1\le j\le d}\left|\frac{\widehat{\beta}_j - \beta_j}{\sqrt{(\widehat{\Sigma}_n^{-1}\widehat{V}_n\widehat{\Sigma}_n^{-1})_{jj}}}\right|\right) - \mathbb{P}\left(\max_{1\le j\le d}|G_j| \le t\right)\right|\\
&\quad\le 2\Delta_n + \frac{3d}{n} + \sqrt{\frac{d}{n}} + \frac{d}{n} + C_n(\eta_n)\eta_n\Phi_{AC}.
\end{align*}
Because $\eta_n \le 1/2$ and $\kappa_n \ge1$, 
$$
C_n(\eta_n) \le 3\kappa_n + \sqrt{2\log(2n)} \le 5\kappa_n\sqrt{\log(en)}.
$$ 
Further, note that
\[
\eta_n \le C\left[\frac{d + \log n}{\sqrt{n}} + \frac{d\sqrt{\log n} + \sqrt{d}\log n}{d^{1/(2q)}n^{1-3/(2q)}} + \frac{d\log d + \log n}{d^{1/q}n^{1-3/q}}\right],
\]
for some other constant $C = C(q, K_x, \widebar{\lambda}K_q^2, \widebar{\lambda}/\underline{\lambda})$. We bound $\Phi_{AC}$ as $\Phi_{AC} \le C\sqrt{\log(ed)}$ for some universal constant $C$; see~\cite{Chern15,chernozhukov2017detailed} for details. (The constant $C$ is universal here because the marginal variances of $G_j$'s are all equal to 1.) 

These inequalities imply that
\begin{align*}
&\sup_{t\ge 0}\left|\mathbb{P}\left(\max_{1\le j\le d}\left|\frac{\widehat{\beta}_j - \beta_j}{\sqrt{(\widehat{\Sigma}_n^{-1}\widehat{V}_n\widehat{\Sigma}_n^{-1})_{jj}}}\right|\right) - \mathbb{P}\left(\max_{1\le j\le d}|G_j| \le t\right)\right|\\
&\le 2\Delta_n + \frac{4d}{n} + \sqrt{\frac{d}{n}}\\ 
&\quad+ 5C\kappa_n\sqrt{\log(en)\log(ed)}\left[\frac{d + \log n}{\sqrt{n}} + \frac{d\sqrt{\log n} + \sqrt{d}\log n}{d^{1/(2q)}n^{1-3/(2q)}} + \frac{d\log d + \log n}{d^{1/q}n^{1-3/q}}\right].
\end{align*}
Observe now that we can assume $d \le n$ because otherwise the bound trivially holds true. This allows us to absorb $4d/n + \sqrt{d/n}$ into the last term above.
\paragraph{Case (2): $\eta_n > 1/2$} In this case, we use
\begin{align*}
&\sup_{t\ge 0}\left|\mathbb{P}\left(\max_{1\le j\le d}\left|\frac{\widehat{\beta}_j - \beta_j}{\sqrt{(\widehat{\Sigma}_n^{-1}\widehat{V}_n\widehat{\Sigma}_n^{-1})_{jj}}}\right|\right) - \mathbb{P}\left(\max_{1\le j\le d}|G_j| \le t\right)\right|\\ 
&\le 1 \le 2\eta_n \le 5C\kappa_n\eta_n\sqrt{\log(en)\log(ed)}\\ 
&\le \Delta_n + 5C\kappa_n\eta_n\sqrt{\log(en)\log(ed)}.
\end{align*}

Now in both cases, it suffices to bound $\Delta_n$. For this purpose, we use Theorem 2.1(b) of~\cite{koike2019notes} by setting, in the notation of that paper, $q = 4$. Then, letting $r = 4q/(q-4),$ 
\begin{align*}\textbf{}
&\left(\mathbb{E}\left[\max_{1\le j\le d}\left|\frac{\psi_{ij}}{\sqrt{(\Sigma^{-1}V\Sigma^{-1})_{jj}}}\right|^4\right]\right)^{1/4}\\ 
&\qquad= \left(\mathbb{E}\left[\max_{1\le j\le d}\left|\frac{e_j^{\top}\Sigma^{-1}X_i(Y_i - X_i^{\top}\beta)}{\sqrt{e_j^{\top}\Sigma^{-1}V\Sigma^{-1}e_j}}\right|^4\right]\right)^{1/4}\\
&\qquad\le \left(\mathbb{E}\left[\max_{1\le j\le d}\left|\frac{e_j^{\top}\Sigma^{-1}X_i}{\sqrt{e_j^{\top}\Sigma^{-1}V\Sigma^{-1}e_j}}\right|^{r}\right]\right)^{1/r}\left(\mathbb{E}[|Y_i - X_i^{\top}\beta|^q]\right)^{1/q}\\
&\qquad\le CK_qK_x\sqrt{\log d}\max_{1\le j\le d}\frac{\|\Sigma^{-1/2}e_j\|_{I_d}}{\sqrt{e_j^{\top}\Sigma^{-1}V\Sigma^{-1}e_j}}\\
&\qquad\le \frac{C}{\sqrt{\underline{\lambda}}}K_qK_x\sqrt{\log d} \le C\sqrt{\log d},
\end{align*}
for a constant $C = C(K_q, K_x, \underline{\lambda})$. Above, the first inequality is H\"{o}lder inequality, the second uses Conditions~\ref{eq:moments-errors} along with the sub-Gaussianity assumption~\ref{eq:covariate-subGaussian}. Specifically, we have used the facts that the maximum of $d$ sub-Gaussian variables is also sub-Gaussian with Orlicz norm proportional to $\sqrt{\log(d)}$ and that the expected value of the the $L_r$ norm of a sub-Gaussian random variable is bounded by its Orlicz norm times a term dependent on $r$ only.  Finally, the third inequality  follows from Condition~\ref{eq:bounded-asymptotic-variance}. The final value of the constant $C$ depends on $q$, $\underline{\lambda}$, $K_x$ and $K_q$. This verifies the assumption of~\citet[Theorem 2.1(b)]{koike2019notes} for $q = 4$ with $D_n = C\sqrt{\log d}$. Note that using the same steps as above but without the maximum over $1\le j\le d$ yields $B_n = C$ in Theorem 2.1 of~\cite{koike2019notes}. Finally because $\Theta_X = \Phi_{AC} \le C\sqrt{\log d}$, Theorem 2.1(b) of~\cite{koike2019notes} proves
% Hence
\[
\Delta_n \le C(\log d)^{1/3}\left[\frac{(\log d)^{1/2}}{n^{1/6}} + \left(\frac{(\log d)(\log d)^{2 - 1/2}}{n^{1 - 1/2}}\right)^{1/3}\right] \le C\frac{(\log d)^{7/6}}{n^{1/6}}.
\]
This concludes the proof in this case.
\end{proof}


\begin{proof}[Proof of Theorem \ref{thm:multiplier-bootstrap-consistency}]
Firstly, because $T_b, 1\le b\le B$, are independent and identically distributed random variables in $\mathbb{R}$ conditional on $\mathcal{D}_n$, Corollary 1 of~\cite{massart1990tight} concludes
\begin{equation}\label{eq:Average-to-conditional-probability}
\sup_{t\ge 0}\left|\frac{1}{B}\sum_{b=1}^B\mathbbm{1}\{T_b \le t\} - \mathbb{P}(T_b \le t\big|\mathcal{D}_n)\right| \le \sqrt{\frac{\log(2n)}{2B}},
\end{equation}
with probability at least $1 - 1/n$.

Secondly, because $T_b$ is the maximum absolute value of a Gaussian vector with unit variances conditional on $\mathcal{D}_n$, Lemma 3.1 of~\cite{Cher13} yields
\begin{equation}\label{eq:Gaussian-comparison-bound}
\sup_{t\ge 0}\,\left|\mathbb{P}(T_b \le t\big|\mathcal{D}_n) - \mathbb{P}\left(\max_{1\le j\le d}|G_j| \le t\right)\right| \le C\Delta_0^{1/3}(1\vee \log(d/\Delta_0))^{2/3},
\end{equation}
where
\[
\Delta_0 ~:=~ \max_{1\le j < k\le d}\left|\mbox{corr}(\widehat{\Sigma}_n^{-1}\widehat{V}_n\widehat{\Sigma}_n^{-1})_{jk} - \mbox{corr}(\Sigma_n^{-1}V_n\Sigma_n^{-1})_{jk}\right|.
\]
For notational convenience, let
$A := \widehat{\Sigma}_n^{-1}\widehat{V}_n\widehat{\Sigma}_n^{-1},$ and $B := \Sigma_n^{-1}V_n\Sigma_n^{-1}.$
Also, set $b_j = B^{1/2}e_j, 1\le j\le d$ with $B^{1/2}$. %representing a symmetric square root of $B$. \textcolor{red}{for PSD matrices, the square root is unique, I think}
Next, we claim that 
\[
\Delta_0 \le \|B^{-1/2}AB^{-1/2} - I_d\|_{\mathrm{op}}\left(2 + \|B^{-1/2}AB^{-1/2}\|_{\mathrm{op}}\right).
\]
Indeed,
\begin{align*}
\Delta_0 
&= \max_{1\le j < k\le d}\left|\frac{e_j^{\top}Ae_k}{\sqrt{(e_j^{\top}Ae_j)(e_k^{\top}Ae_k)}} - \frac{e_j^{\top}Be_k}{\sqrt{(e_j^{\top}Be_j)(e_k^{\top}Be_k)}}\right|\\
&= \max_{1\le j\le k \le d}\left|\frac{b_j^{\top}B^{-1/2}AB^{-1/2}b_k}{\sqrt{(b_j^{\top}B^{-1/2}AB^{-1/2}b_j)(b_k^{\top}B^{-1/2}AB^{-1/2}b_k)}} - \frac{b_j^{\top}b_k}{\|b_j\|_{I_d}\|b_k\|_{I_d}}\right|\\
&\le \max_{1\le j < k\le d}\left|\frac{b_j^{\top}B^{-1/2}AB^{-1/2}b_k}{\sqrt{(b_j^{\top}B^{-1/2}AB^{-1/2}b_j)(b_k^{\top}B^{-1/2}AB^{-1/2}b_k)}} - \frac{b_j^{\top}B^{-1/2}AB^{-1/2}b_k}{\sqrt{(b_j^{\top}b_j)(b_k^{\top}b_k)}}\right|\\
&\qquad+ \max_{1\le j < k\le d}\left|\frac{b_j^{\top}B^{-1/2}AB^{-1/2}b_k}{\sqrt{(b_j^{\top}b_j)(b_k^{\top}b_k)}} - \frac{b_j^{\top}b_k}{\|b_j\|_{I_d}\|b_k\|_{I_d}}\right|\\
%\end{align*}
%Using Cauchy-Schwarz inequality and the inequality $|1 - \sqrt{x}| \le |1 - x|$, valid for all $x \geq 0$, the previous expression is uper bounded by 
%\begin{align*}
&\le \max_{1\le j < k\le d}\left|\frac{b_j^{\top}B^{-1/2}AB^{-1/2}b_k}{\sqrt{(b_j^{\top}B^{-1/2}AB^{-1/2}b_j)(b_k^{\top}B^{-1/2}AB^{-1/2}b_k)}}\right|\\
& \times\left|1 - \sqrt{\frac{(b_j^{\top}B^{-1/2}AB^{-1/2}b_j)(b_k^{\top}B^{-1/2}AB^{-1/2}b_k)}{b_j^{\top}b_jb_k^{\top}b_k}}\right|+ \|B^{-1/2}AB^{-1/2} - I_d\|_{\mathrm{op}}
\end{align*}
Using Cauchy-Schwarz inequality and the inequality $|1 - \sqrt{x}| \le |1 - x|$, valid for all $x \geq 0$, the previous expression is uper bounded by  the last expression is bounded by
\begin{align*}
&  \max_{1\le j \le k\le d}\left|1 - \frac{(b_j^{\top}B^{-1/2}AB^{-1/2}b_j)(b_k^{\top}B^{-1/2}AB^{-1/2}b_k)}{b_j^{\top}b_jb_k^{\top}b_k}\right| + \|B^{-1/2}AB^{-1/2} - I_d\|_{\mathrm{op}}\\
&\qquad~ \le \max_{1\le j\le d}\left|1 - \frac{b_j^{\top}B^{-1/2}AB^{-1/2}b_j}{b_j^{\top}b_j}\right| + \max_{1\le k\le d}\left|1 - \frac{b_k^{\top}B^{-1/2}AB^{-1/2}b_k}{b_k^{\top}b_k}\right|\|B^{-1/2}AB^{-1/2}\|_{\mathrm{op}}\\ &\qquad+ \|B^{-1/2}AB^{-1/2} - I_d\|_{\mathrm{op}}\\
&\qquad~ \le 2\|B^{-1/2}AB^{-1/2} - I_d\|_{\mathrm{op}} + \|B^{-1/2}AB^{-1/2} - I_d\|_{\mathrm{op}}\|B^{-1/2}AB^{-1/2}\|_{\mathrm{op}}\\
&\qquad~ = \|B^{-1/2}AB^{-1/2} - I_d\|_{\mathrm{op}}\left(2 + \|B^{-1/2}AB^{-1/2}\|_{\mathrm{op}}\right),
\end{align*}
where the first inequality follows form follows from the identity $(1 - ab) = (1 - b) - (a-1)b$ along with the triangle inequality.

 Lemma~\ref{lem:std-err-consistency} now yields the rate of convergence of $\Delta_0$. In detail,  with probability at least $1 - (d+2)/n - \sqrt{d/n}$,
\begin{equation}\label{eq:multiplier-bootstrap-bound-1}
\Delta_0 \le C\sqrt{\frac{d\log(ed) + \log n}{n}} + C\frac{d^{1-1/q}\log(ed)\log n}{n^{1-3/q}}.
\end{equation}
Substituting this in~\eqref{eq:Gaussian-comparison-bound} yields with probability at least $1 - (d + 2)/n - \sqrt{d/n}$,
\begin{equation}\label{eq:multiplier-bootstrap-bound}
\begin{split}
&\sup_{t\ge 0}\,\left|\mathbb{P}(T_b \le t\big|\mathcal{D}_n) - \mathbb{P}\left(\max_{1\le j\le d}|G_j| \le t\right)\right|\\ 
&\qquad\le C\log^3(dn)\left[\left(\frac{d\log d + \log n}{n}\right)^{1/6} + \left(\frac{d^{1-1/q}\log(d)\log n}{n^{1-3/q}}\right)^{1/3}\right].
\end{split}
\end{equation}
Combining inequalities~\eqref{eq:Average-to-conditional-probability} and~\eqref{eq:multiplier-bootstrap-bound} concludes the proof.
\end{proof}



%\section*{Acknowledgments} 


\end{appendices}


\bibliography{paper_CLT}
\bibliographystyle{apalike}
\newpage
\begin{appendices}
\begin{center}
{\Large {\bf Supplementary Material}}
\end{center}

\section{Proof of the Main Results from Section~\ref{section::partial} (Partial Correlations)}
\label{appendix:main.partial} 


Recall the functions $\psi_{jk}(\cdot)$ and their estimators $\widehat{\psi}_{jk}(\cdot)$ given in~\eqref{eq:def-psi-jk} and~\eqref{eq:def-widehat-psi-jk}, respectively, and, similarly, the definitions of $a_j(\cdot)$ and $\widehat{a}_j(\cdot)$  in~\eqref{eq:ajx-function} and~\eqref{eq:ajhatx-function}, respectively.

\begin{proof}[Proof of Theorem~\ref{thm:linear-expansion-partial-corr}]
For notational convenience, let
$\widehat{\omega}_{jk} := e_j^{\top}\widehat{\Sigma}^{-1}e_k$ and $\omega_{jk} := e_j^{\top}\Sigma^{-1}e_k.$
Before bounding $\widehat{\theta}_{jk} - \theta_{jk}$, we note a few inequalities related to $\widehat{\omega}_{jk}$ and $\omega_{jk}$ that follow from~\eqref{eq:inv-covariance-error-bound} of Lemma~\ref{lemma:linear-expansion-inv-covariance}:
\begin{equation}\label{eq:inequalities-omegas}
\begin{split}
\max\left\{\left|\sqrt{\frac{\widehat{\omega}_{jj}}{\omega_{jj}}} - 1\right|, \left|\frac{\widehat{\omega}_{jj}}{\omega_{jj}} - 1\right|, \left|\frac{\widehat{\omega}_{jk} - \omega_{jk}}{\sqrt{\omega_{jj}\omega_{kk}}}\right|\right\} ~&\le~ \frac{\mathcal{D}_n^{\Sigma}}{1 - \mathcal{D}_n^{\Sigma}};\\
\max\left\{\left|\sqrt{\frac{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}{\omega_{jj}\omega_{kk}}} - 1\right|, \left|\frac{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}{\omega_{jj}\omega_{kk}} - 1\right|\right\} ~&\le~ \frac{2\mathcal{D}_n^{\Sigma}}{1 - \mathcal{D}_n^{\Sigma}} + \left(\frac{\mathcal{D}_n^{\Sigma}}{1 - \mathcal{D}_n^{\Sigma}}\right)^2;\quad\mbox{and}\\
\frac{1}{1 + \mathcal{D}_n^{\Sigma}} ~\le~ \frac{\widehat{\omega}_{jj}}{\omega_{jj}} ~&\le~ \frac{1}{1 - \mathcal{D}_n^{\Sigma}},\quad\mbox{for all}\quad 1\le j\le d.
\end{split}
\end{equation}
All these inequalities follow from the fact that
\[
\|\Sigma^{1/2}(\widehat{\Sigma}^{-1} - \Sigma^{-1})\Sigma^{1/2}\|_{\mathrm{op}} = \sup_{x, y\in\mathbb{R}^d}\,\frac{x^{\top}\Sigma^{1/2}(\widehat{\Sigma}^{-1} - \Sigma^{-1})\Sigma^{1/2}y}{\|x\|\|y\|} \ge \max_{j,k}\frac{|e_j^{\top}(\widehat{\Sigma}^{-1} - \Sigma^{-1})e_k|}{\sqrt{(e_j^{\top}\Sigma^{-1}e_j)(e_k^{\top}\Sigma^{-1}e_k)}}.
\]
Observe that
\begin{align*}
\widehat{\theta}_{jk} - \theta_{jk} &= -\frac{\widehat{\omega}_{jk}}{\sqrt{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}} + \frac{\omega_{jk}}{\sqrt{\omega_{jj}\omega_{kk}}}\\
&= -\frac{\widehat{\omega}_{jk} - \omega_{jk}}{\sqrt{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}} - \frac{\omega_{jk}}{\sqrt{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}}\left[1 - \sqrt{\frac{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}{\omega_{jj}\omega_{kk}}}\right].
\end{align*}
The equation $\sqrt{a} - 1 = (a-1)/2 - (\sqrt{a} - 1)^2/2$ for $a > 0$ yields
\begin{align*}
\widehat{\theta}_{jk} - \theta_{jk} &= -\frac{\widehat{\omega}_{jk} - \omega_{jk}}{\sqrt{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}} - \frac{\omega_{jk}}{2\sqrt{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}}\left[1 - \frac{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}{\omega_{jj}\omega_{kk}}\right] - \frac{\omega_{jk}}{2\sqrt{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}}\left(1 - \sqrt{\frac{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}{\omega_{jj}\omega_{kk}}}\right)^2.
\end{align*}
Finally using $a'b' - 1 = (a' - 1) + (b' - 1) + (a' - 1)(b' - 1)$, we obtain
\begin{align*}
&\left|\widehat{\theta}_{jk} - \theta_{jk} + \frac{\widehat{\omega}_{jk} - \omega_{jk}}{\sqrt{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}} - \frac{\omega_{jk}}{2\sqrt{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}}\left[\frac{\widehat{\omega}_{jj}}{\omega_{jj}} + \frac{\widehat{\omega}_{kk}}{\omega_{kk}} - 2\right]\right|\\
&\qquad\le \frac{|\omega_{jk}|}{2\sqrt{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}}\times\left|1 - \frac{\widehat{\omega}_{jj}}{\omega_{jj}}\right|\times\left|1 - \frac{\widehat{\omega}_{kk}}{\omega_{kk}}\right| ~+~ \frac{|\omega_{jk}|}{2\sqrt{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}}\left|1 - \sqrt{\frac{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}{\omega_{jj}\omega_{kk}}}\right|^2
\end{align*}
We now replace $\widehat{\omega}_{jj}, \widehat{\omega}_{kk}$ in the denominator of the left side to get
\begin{align*}
&\left|\widehat{\theta}_{jk} - \theta_{jk} + \frac{\widehat{\omega}_{jk} - \omega_{jk}}{\sqrt{{\omega}_{jj}{\omega}_{kk}}} - \frac{\omega_{jk}}{2\sqrt{{\omega}_{jj}{\omega}_{kk}}}\left[\frac{\widehat{\omega}_{jj}}{\omega_{jj}} + \frac{\widehat{\omega}_{kk}}{\omega_{kk}} - 2\right]\right|\\
&\qquad\le \frac{|\omega_{jk}|}{2\sqrt{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}}\times\left|1 - \frac{\widehat{\omega}_{jj}}{\omega_{jj}}\right|\times\left|1 - \frac{\widehat{\omega}_{kk}}{\omega_{kk}}\right| ~+~ \frac{|\omega_{jk}|}{2\sqrt{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}}\left|1 - \sqrt{\frac{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}{\omega_{jj}\omega_{kk}}}\right|^2\\
&\qquad\qquad+ \frac{|\widehat{\omega}_{jk} - \omega_{jk}|}{\sqrt{\omega_{jj}\omega_{kk}}}\left|\sqrt{\frac{\omega_{jj}\omega_{kk}}{{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}}} - 1\right| + \frac{|\omega_{jk}|}{2\sqrt{\omega_{jj}\omega_{kk}}}\left|\frac{\widehat{\omega}_{jj}}{\omega_{jj}} + \frac{\widehat{\omega}_{kk}}{\omega_{kk}} - 2\right|\times\left|\sqrt{\frac{\omega_{jj}\omega_{kk}}{{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}}} -  1\right|. 
\end{align*}
To bound the right hand side we use inequalities~\eqref{eq:inequalities-omegas}. Note that
\[
\frac{|\omega_{jk}|}{\sqrt{\omega_{jj}\omega_{kk}}} \le 1,\quad\mbox{and}\quad \frac{|\omega_{jk}|}{\sqrt{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}}  \leq \frac{|\omega_{jk}|}{\sqrt{\omega_{jj}\omega_{kk}}} (1 + \mathcal{D}_n^{\Sigma}) 
\leq 1 + \mathcal{D}_n^{\Sigma}.
\]
Further, second and third inequalites of~\eqref{eq:inequalities-omegas} yield
\begin{align*}
\left|\sqrt{\frac{\omega_{jj}\omega_{kk}}{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}} - 1\right| &= \left|\sqrt{\frac{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}{\omega_{jj}\omega_{kk}}} - 1\right|\sqrt{\frac{\omega_{jj}\omega_{kk}}{\widehat{\omega}_{jj}\widehat{\omega}_{kk}}}\\
&\le \left[\frac{2\mathcal{D}_n^{\Sigma}}{1 - \mathcal{D}_n^{\Sigma}} + \left(\frac{\mathcal{D}_n^{\Sigma}}{1 - \mathcal{D}_n^{\Sigma}}\right)^2\right](1 + \mathcal{D}_n^{\Sigma}) \le 9\mathcal{D}_n^{\Sigma},
\end{align*}
under the assumption $\mathcal{D}_n^{\Sigma} \le 1/2$.

% Using inequalities~\eqref{eq:inequalities-omegas} along with the assumption $\mathcal{D}_n^{\Sigma} \le 1/2$ implies 
Combining these inequalities, we conclude
\begin{equation}\label{eq:prelim-partial-corr-expansion}
\left|\widehat{\theta}_{jk} - \theta_{jk} + \frac{\widehat{\omega}_{jk} - \omega_{jk}}{\sqrt{{\omega}_{jj}{\omega}_{kk}}} - \frac{\omega_{jk}}{2\sqrt{{\omega}_{jj}{\omega}_{kk}}}\left[\frac{\widehat{\omega}_{jj}}{\omega_{jj}} + \frac{\widehat{\omega}_{kk}}{\omega_{kk}} - 2\right]\right| \le C(\mathcal{D}_n^{\Sigma})^2,
\end{equation}
for a universal constant $C \in (0, \infty)$ and for all $1\le j, k\le d$.
Finally~\eqref{eq:final-linear-expansion-inv-covariance} of Lemma~\ref{lemma:linear-expansion-inv-covariance} implies
\[
\max_{1\le j\le k \le d}\left|\frac{\widehat{\omega}_{jk} - \omega_{jk} - e_j^{\top}\Sigma^{-1}(\widetilde{\Sigma} - \Sigma)\Sigma^{-1}e_k}{\sqrt{\omega_{jj}\omega_{kk}}}\right| \le \|\overline{X}_n - \mu_X\|_{\Sigma^{-1}}^2 + \frac{(\mathcal{D}_n^{\Sigma})^2}{1 - \mathcal{D}_n^{\Sigma}}.
\]
Combining this inequality with~\eqref{eq:prelim-partial-corr-expansion} and using $\mathcal{D}_n^{\Sigma} \le 1/2$ concludes
\begin{align*}
&\left|\widehat{\theta}_{jk} - \theta_{jk} + \frac{e_j^{\top}\Sigma^{-1}(\widetilde{\Sigma} - \Sigma)\Sigma^{-1}e_k}{\sqrt{\omega_{jj}\omega_{kk}}} - \frac{\theta_{jk}}{2}\left[\frac{e_j^{\top}\Sigma^{-1}(\widetilde{\Sigma} - \Sigma)\Sigma^{-1}e_j}{\omega_{jj}} + \frac{e_k^{\top}\Sigma^{-1}(\widetilde{\Sigma} - \Sigma)\Sigma^{-1}e_k}{\omega_{kk}}\right]\right|\\
&\qquad\le C(\mathcal{D}_n^{\Sigma})^2 + \|\overline{X}_n - \mu_X\|_{\Sigma^{-1}}^2,
\end{align*}
for a universal constant $C \in (0, \infty)$. This concludes the proof.
\end{proof}



\begin{proof}[Proof of Theorem \ref{thm:Berry-Esseen-bound-partial-corr}]
We will prove the theorem when $C\eta_n \le 1/2$; otherwise the result is trivially true by increasing the constant $C$. For notational convenience, let $\zeta_{jk} := \mathbb{E}[\psi_{jk}^2(X)]$. Theorem~\ref{thm:linear-expansion-partial-corr} implies that
\begin{equation}\label{eq:proper-normalized-partial-correlation}
\max_{1\le j < k\le d}\left|\frac{\widehat{\theta}_{jk} - \theta_{jk}}{\widehat{\zeta}_{jk}} + \frac{1}{n\widehat{\zeta}_{jk}}\sum_{i=1}^n \psi_{jk}(X_i)\right| ~\le~ \frac{C(\mathcal{D}_n^{\Sigma})^2 + \|\overline{X}_n - \mu_X\|_{\Sigma^{-1}}^2}{\min_{1\le j < k\le d}\widehat{\zeta}_{jk}},
\end{equation}
whenever $\mathcal{D}_n^{\Sigma} \le 1/2$. Furthermore,
\begin{equation}\label{eq:average-variance-estimator}
\left|\frac{1}{n\widehat{\zeta}_{jk}}\sum_{i=1}^n \psi_{jk}(X_i) - \frac{1}{n\zeta_{jk}}\sum_{i=1}^n \psi_{jk}(X_i)\right| = \frac{|n^{-1}\sum_{i=1}^n \psi_{jk}(X_i)|}{\widehat{\zeta}_{jk}\zeta_{jk}}\times|\widehat{\zeta}_{jk} - \zeta_{jk}|.
\end{equation}
Define the random variable
\[
\widetilde{\zeta}_{jk}^2 ~:=~ \frac{1}{n}\sum_{i=1}^n \psi_{jk}^2(X_i),
\]
which may be regarded as a pseudo-estimator of sort, since $\mathbb{E}[\widetilde{\zeta}_{jk}^2] = \zeta^2_{jk}$. Of course $\widetilde{\zeta}_{jk}^2$ is not a computable  estimator of $\zeta^2_{jk}$ because it depends on unknown quantities, namely $\Sigma$ and $\mu_X$.


Equations ~\eqref{eq:proper-normalized-partial-correlation} and \eqref{eq:average-variance-estimator} together imply that
\begin{equation}\label{eq:combination-influence-fn-exp-partial-correlation}
\begin{split}
\max_{1\le j < k\le d}\left|\frac{\widehat{\theta}_{jk} - \theta_{jk}}{\widehat{\zeta}_{jk}} + \frac{1}{n\zeta_{jk}}\sum_{i=1}^n \psi_{jk}(X_i)\right| ~&\le~ \frac{C(\mathcal{D}^{\Sigma})^2 + \|\widebar{X}_n - \mu_X\|_{\Sigma^{-1}}^2}{\min_{1\le j < k\le d}\widehat{\zeta}_{jk}}\\
~&\qquad+~ \max_{1\le j < k\le d}\frac{|\widehat{\zeta}_{jk} - \zeta_{jk}|}{\widehat{\zeta}_{jk}\zeta_{jk}}\left|\frac{1}{n}\sum_{i=1}^n \psi_{jk}(X_i)\right|. 
\end{split}
\end{equation}




Clearly,
\begin{equation}\label{eq:bound-hat.sigma-sigma}
\widehat{\zeta}_{jk} = \zeta_{jk}\left(1 + \frac{\widehat{\zeta}_{jk}}{\zeta_{jk}} - 1\right) \ge \zeta_{jk}\left(1 - \left|\frac{\widehat{\zeta}_{jk}}{\zeta_{jk}} - 1\right|\right).
\end{equation}
Using the pseudo-estimator $\widetilde{\zeta}_{jk}^2$, we have with probability $1 - C/n$,
\begin{equation}\label{eq:sigma-hat-to-sigma-tilde}
\max_{1\le j < k\le d}|\widehat{\zeta}_{jk} - \widetilde{\zeta}_{jk}| \le CK_x^6\sqrt{\frac{d + \log n}{n}} + CK_x^5\frac{d + \log n}{n},
\end{equation}
Next, since
\[
\left|\widetilde{\zeta}_{jk} - \zeta_{jk}\right| = \frac{|\widetilde{\zeta}_{jk}^2 - \zeta_{jk}^2|}{|\widetilde{\zeta}_{jk} + \zeta_{jk}|} \le \frac{1}{\zeta_{jk}}\left|\frac{1}{n}\sum_{i=1}^n \psi_{jk}^2(X_i) - \mathbb{E}[\psi_{jk}^2(X_i)]\right|.
\]
we have that
\[
\max_{1\le j < k\le d}\left|\widetilde{\zeta}_{jk} - \zeta_{jk}\right| ~\le~ \frac{1}{\min_{1\le j < k\le d}\zeta_{jk}}\max_{1\le j < k\le d}\left|\frac{1}{n}\sum_{i=1}^n \psi_{jk}^2(X_i) - \mathbb{E}[\psi_{jk}^2(X_i)]\right|.
\]
Then, applying  Lemma~\ref{lemma:Thm3.1.KuchAbhi} with $\alpha = 1/2, q = d^2$ and $t = \log n$,  we obtain that, with probability at least $1 - 3/n$,
\begin{equation}\label{eq:sigma-tilde-to-sigma}
\max_{1\le j < k\le d}\left|\widetilde{\zeta}_{jk} - \zeta_{jk}\right| ~\le~ \frac{CK_x^4}{\zeta_{\min}}\left[\sqrt{\frac{\log(dn)}{n}} + \frac{\log^2(dn)}{n}\right].
\end{equation}


Combining inequalities~\eqref{eq:sigma-hat-to-sigma-tilde} and~\eqref{eq:sigma-tilde-to-sigma} we now conclude that, with probability at least $1 - (C+3)/n$,
\begin{align*}
\max_{1\le j < k\le d}\left|\frac{\widehat{\zeta}_{jk}}{\zeta_{jk}} - 1\right| ~&\le~ \frac{CK_x^6}{\zeta_{\min}}\sqrt{\frac{d + \log n}{n}} + \frac{CK_x^5}{\zeta_{\min}}\frac{d + \log n}{n}\\
~&\qquad+~ \frac{CK_x^4}{\zeta_{\min}^2}\left[\sqrt{\frac{\log(dn)}{n}} + \frac{\log^2(dn)}{n}\right].
\end{align*}
Because $K_x \ge 1$, the previous bound reduces to
\begin{equation}\label{eq:sigma-hat-to-sigma}
\max_{1\le j < k\le d}\left|\frac{\widehat{\zeta}_{jk}}{\zeta_{jk}} - 1\right|  \leq C\left(\frac{K_x^6}{\zeta_{\min}} + \frac{K_x^4}{\zeta_{\min}^2}\right)\left[\sqrt{\frac{d + \log n}{n}} + \frac{d + \log^2n}{n}\right]. 
%\max_{1\le j < k\le d}\left|\frac{\widehat{\zeta}_{jk}}{\zeta_{jk}} - 1\right| ~&\le~ \frac{CK_x^6}{\zeta_{\min}}\left[\sqrt{\frac{d + \log n}{n}} + \frac{d + \log n}{n}\right]  \\
%~&\qquad+~ \frac{CK_x^4}{\zeta_{\min}^2}\left[\sqrt{\frac{\log(dn)}{n}} + \frac{\log^2(dn)}{n}\right] \\
%~&\le~ C\left(\frac{K_x^6}{\zeta_{\min}} + \frac{K_x^4}{\zeta_{\min}^2}\right)\left[\sqrt{\frac{d + \log n}{n}} + \frac{d + \log^2n}{n}\right]. 
\end{equation}
 Assuming $n$ large enough so that the quantity on the right hand side is bounded by $1/2$ and using the inequality \eqref{eq:bound-hat.sigma-sigma}, we get that, with probability at least $1 - (C + 3)/n$, $\widehat{\zeta}_{jk} \ge \zeta_{jk}/2$ for all $1\le j < k\le d$ and hence,
\begin{align*}
&\max_{1\le j < k\le d}\left|\frac{\widehat{\theta}_{jk} - \theta_{jk}}{\widehat{\zeta}_{jk}} + \frac{1}{n\zeta_{jk}}\sum_{i=1}^n \psi_{jk}(X_i)\right|\\ 
&\qquad\le~ \frac{2C(\mathcal{D}^{\Sigma})^2 + 2\|\widebar{X}_n - \mu_X\|_{\Sigma^{-1}}^2}{\zeta_{\min}}
% \\
% &\quad\qquad
+ 2\max_{1\le j < k\le d}\frac{|\widehat{\zeta}_{jk} - \zeta_{jk}|}{\zeta_{jk}^2}\left|\frac{1}{n}\sum_{i=1}^n \psi_{jk}(X_i)\right|. 
\end{align*}


We now proceed to derive a high probability bound for the last display. The term $\max_{1\le j < k\le d}{|\widehat{\zeta}_{jk} - \zeta_{jk}|}/{\zeta_{jk}^2}$ can be bounded as in equation~\eqref{eq:sigma-hat-to-sigma}, with probability at least  $1 - (C+3)/n$. Next, Lemma~\ref{lemma:Thm3.1.KuchAbhi} with $\alpha=1$ gives that, with probability at least $1 - 3/n$,
\begin{equation}\label{eq:bound-on-sum-psi}
\max_{1\le j < k\le d}\left|\frac{1}{n}\sum_{i=1}^n \psi_{jk}(X_i)\right| ~\le~ CK_x^2\sqrt{\frac{\log(dn)}{n}} + CK_x^2\frac{\log(dn)}{n}.
\end{equation}




%\begin{equation}%\label{eq:combination-influence-fn-exp-partial-correlation}
%\begin{split}
%\max_{1\le j < k\le d}\left|\frac{\widehat{\theta}_{jk} - \theta_{jk}}{\widehat{\sigma}_{jk}} + \frac{1}{n\sigma_{jk}}\sum_{i=1}^n% \psi_{jk}(X_i)\right| ~&\le~ \frac{C(\mathcal{D}^{\Sigma})^2 + \|\widebar{X}_n - \mu_X\|_{\Sigma^{-1}}^2}{\min_{1\le j < k\le d}\%widehat{\sigma}_{jk}}\\
%~&\qquad+~ \max_{1\le j < k\le d}\frac{|\widehat{\sigma}_{jk} - \sigma_{jk}|}{\widehat{\sigma}_{jk}\sigma_{jk}}\left|\frac{1}{n}\%sum_{i=1}^n \psi_{jk}(X_i)\right|. 
%\end{split}
%\end%{equation}
%Clearly,
%\[
%\widehat{\sigma}_{jk} = \sigma_{jk}\left(1 + \frac{\widehat{\sigma}_{jk}}{\sigma_{jk}} - 1\right) \ge \sigma_{jk}\left(1 - \left|\%frac{\widehat{\sigma}_{jk}}{\sigma_{jk}} - 1\right|\right).
%\]%
%With the intermediate variance estimator $\widetilde{\sigma}_{jk}^2$, we have with probability $1 - C/n$,
%\begin{equation}%\label{eq:sigma-hat-to-sigma-tilde}
%\max_{1\le j < k\le d}|\widehat{\sigma}_{jk} - \widetilde{\sigma}_{jk}| \le CK_x^6\sqrt{\frac{d + \log n}{n}} + CK_x^5\frac{d + \%log n}{n},
%\end{equation}
%Further
%\[
%\left|\widetilde{\sigma}_{jk} - \sigma_{jk}\right| = \frac{|\widetilde{\sigma}_{jk}^2 - \sigma_{jk}^2|}{|\widetilde{\sigma}_{jk} + %\sigma_{jk}|} \le \frac{1}{\sigma_{jk}}\left|\frac{1}{n}\sum_{i=1}^n \psi_{jk}^2(X_i) - \mathbb{E}[\psi_{jk}^2(X_i)]\right|.
%\]
%Therefore,
%\[
%\max_{1\le j < k\le d}\left|\widetilde{\sigma}_{jk} - \sigma_{jk}\right| ~\le~ \frac{1}{\min_{1\le j < k\le d}\sigma_{jk}}\max_{1%\le j\le k\le d}\left|\frac{1}{n}\sum_{i=1}^n \psi_{jk}^2(X_i) - \mathbb{E}[\psi_{jk}^2(X_i)]\right|.
%\]
%Clearly,
%\[
%|\widehat{\sigma}_{jk} - \widetilde{\sigma}_{jk}| \le \max_{1\le j < k\le d}\sqrt{\frac{1}{n}\sum_{i=1}^n \left(\widehat{\psi}_{jk%}(X_i) - \psi_{jk}(X_i)\right)^2},
%\]
%which by Lemma~\ref{lem:rate-of-convergence-psihat-minus-psi} can be bounded with probability at least $1 - C/n$ as
%\[
%\max_{1\le j < k\le d}|\widehat{\sigma}_{jk} - \widetilde{\sigma}_{jk}| \le CK_x^6\sqrt{\frac{d + \log n}{n}} + CK_x^5\frac{d + \%log n}{n},
%\]
%for some universal constant $C>0$, provided that $d \le \sqrt{n}$ and the right hand side is bounded by 1. Moreover,
%\[
%\max_{1\le j < k\le d}\left|\frac{\widetilde{\sigma}_{jk}}{\sigma_{jk}} - 1\right| \le \max_{1\le j < k\le d}\left|\frac{\%widetilde{\sigma}_{jk}^2}{\sigma_{jk}^2} - 1\right| \le \frac{1}{\min_{1\le j < k\le d}\sigma_{jk}^2}\max_{1\le j < k\le d}\left|%\frac{1}{n}\sum_{i=1}^n \psi_{jk}^2(X_i) - \mathbb{E}[\psi_{jk}^2(X_i)]\right|.
%\]
%Because $a_j(X) = \overline{e}_j^{\top}\Sigma^{-1/2}(X_i - \mu_X)$ for $\overline{e}_j = \Sigma^{-1/2}e_j/\|\Sigma^{-1/2}e_j\|_2$ %of unit norm, assumption~\eqref{eq:centered-subGaussian-x} implies that $a_j(X)$ is sub-Gaussian with constant $K_x$. This further %implies that $a_j(X)a_k(X), a_j^2(X), a_k^2(X)$ are each sub-exponential with sub-exponential norm bounded by $K_x^2$. Hence, $\psi%_{jk}(X)$) is in turn sub-exponential with norm bounded by $C K_x^2$ for some $C>0$. Thus, by Theorem 3.4 of~\cite{KuchAbhi17}, %for some universal constant $C>0$, with probability at least $1 - C/n$
%\[
%\max_{1\le j < k\le d}\left|\frac{1}{n}\sum_{i=1}^n \psi_{jk}^2(X_i) - \mathbb{E}[\psi_{jk}^2(X_i)]\right| \le CK_x^4\sqrt{\frac{\%log(dn)}{n}} + CK_x^4\frac{(\log(dn))^4}{n}.
%\]
%\textcolor{red}{Please provide details? $ \psi_{jk}^2(X_i)$ is sub-Weibul of order $1/2$ so where does the exponent $4$ come from %in the second term?}
%Further, using sub-exponentiality of $\psi_{jk}(X_i)$, Theorem 2.8.1 of~\cite{Vershynin18} implies that with probability at least $%1 - C/n$,
%\[
%\max_{1\le j < k\le d}\left|\frac{1}{n}\sum_{i=1}^n \psi_{jk}(X_i)\right| \le CK_x^2\sqrt{\frac{\log(dn)}{n}} + CK_x^2\frac{\log(%dn)}{n}.
%\]
%Therefore, from~\eqref{eq:proper-normalized-partial-correlation} and~\eqref{eq:average-variance-estimator}, we get with %probability at least $1 - C/n$,
%\begin{equation}\label{eq:main-expansion-with-estimated-variance}
%\begin{split}
%\max_{1\le j < k\le d}\left|\frac{\widehat{\theta}_{jk} - \theta_{jk}}{\widehat{\sigma}_{jk}} + \frac{1}{n\sigma_{jk}}\sum_{i=1}^n% \psi_{jk}(X_i)\right| &\le C{(\mathcal{D}_n^{\Sigma})^2 + \|\overline{X}_n - \mu_X\|_{\Sigma^{-1}}^2}\\
%&\qquad+ CK_x^8\frac{\sqrt{(\log(dn))(d + \log n)}}{n},
%\end{split}
%\end{equation}
%for a constant $C\in(0,\infty)$ depending only on $\min_{1\le j < k\le d}\sigma_{jk}$ \textcolor{red}{make dependence explicit?}. 
%



To bound $(\mathcal{D}_n^{\Sigma})^2 + \|\overline{X}_n - \mu_X\|_{\Sigma^{-1}}^2$, we notice that, by Proposition~\ref{prop:bounding-D-sigma} 
\[
\mathcal{D}_n^{\Sigma} ~\le~ \|\Sigma^{-1/2}\widetilde{\Sigma}\Sigma^{-1/2} - I_d\|_{\mathrm{op}} + \|\overline{X}_n - \mu_X\|_{\Sigma^{-1}}^2.
\]
Next, Lemma~\ref{lem:concentration-of-covariance} yields that
\[
\mathbb{P}\left(\|\Sigma^{-1/2}\widetilde{\Sigma}\Sigma^{-1/2} - I_d\|_{\mathrm{op}} \ge CK_x^2\sqrt{\frac{d + \log(1/\delta)}{n}} + CK_x^2\frac{d + \log(1/\delta)}{n}\right) \le \delta.
\]
and the sub-Gaussianity assumption further implies that
\[
\mathbb{P}\left(\|\overline{X}_n - \mu_X\|_{\Sigma^{-1}} \ge CK_x\sqrt{\frac{d + \log(1/\delta)}{n}}\right) \le \delta.
\]
Therefore, 
\begin{equation}\label{eq:linear-rep-error-partial-corr}
\mathbb{P}\left(n^{1/2}(\mathcal{D}_n^{\Sigma})^2 + n^{1/2}\|\overline{X}_n - \mu_X\|_{\Sigma^{-1}}^2 \ge C(K_x^2 + K_x^4)\frac{d + \log(n)}{\sqrt{n}}\right) \le \frac{1}{n}.
\end{equation}



Combining the bounds \eqref{eq:sigma-hat-to-sigma}, \eqref{eq:bound-on-sum-psi} and \eqref{eq:linear-rep-error-partial-corr}, we  conclude that, with probability at least $1 - C/n$,
\begin{align*}
&\max_{1\le j < k\le d}\left|\frac{n^{1/2}(\widehat{\theta}_{jk} - \theta_{jk})}{\widehat{\zeta}_{jk}} + \frac{1}{\sqrt{n}\zeta_{jk}}\sum_{i=1}^n \psi_{jk}(X_i)\right|\\ 
~&\le~ \frac{CK_x^4}{\zeta_{\min}}\frac{d + \log n}{\sqrt{n}}\\
~&+~ C\left(\frac{K_x^8\sqrt{\log(dn)}}{\zeta_{\min}^2} + \frac{K_x^6\sqrt{\log(dn)}}{\zeta_{\min}^3}\right)\left[\sqrt{\frac{d + \log n}{n}} + \frac{d + \log^2n}{n}\right]\left(1 + \sqrt{\frac{\log(dn)}{n}}\right),
\end{align*}
whenever the right hand side is smaller than $1/2$.
Because $d \le n$, ${d/n} \le \sqrt{d/n}$ and 
\[
\sqrt{\frac{\log n}{n}} + \frac{\log^2n}{n} = \sqrt{\frac{\log n}{n}}\left(1 + \frac{\log^{3/2}n}{n^{1/2}}\right) \le C\sqrt{\frac{\log n}{n}},
\]
we have that
\[
\sqrt{\frac{d + \log n}{n}} + \frac{d + \log^2n}{n} \le C\sqrt{\frac{d + \log n}{n}}.
\] 
Thus we have shown that, with probability at least $1 - C/n$,
\begin{align*}
&\max_{1\le j < k\le d}\left|\frac{n^{1/2}(\widehat{\theta}_{jk} - \theta_{jk})}{\widehat{\zeta}_{jk}} + \frac{1}{\sqrt{n}\zeta_{jk}}\sum_{i=1}^n \psi_{jk}(X_i)\right|\\ 
~&\le~ \frac{CK_x^4}{\zeta_{\min}}\frac{d + \log n}{\sqrt{n}} ~+~ C\left(\frac{K_x^8}{\zeta_{\min}^2} + \frac{K_x^6}{\zeta_{\min}^3}\right)\sqrt{\frac{(d + \log n)\log(dn)}{n}}\\
~& = \eta_n.
\end{align*}



By the same arguments used in the proof of Theorem ~\ref{thm:Berry-Esseen-OLS}, 
\begin{equation}\label{eq:penultimate-partial-correlation11}
\begin{split}
&\sup_{t\ge0}\left|\mathbb{P}\left(\max_{1\le j < k\le d}\left|\frac{n^{1/2}(\widehat{\theta}_{jk} - \theta_{jk})}{\widehat{\zeta}_{jk}}\right| \le t\right) - \mathbb{P}\left(\max_{1\le j < k\le d}|G_{jk}| \le t\right)\right|\\ 
&\qquad\le \mathbb{P}\left(t - \eta_n \le \max_{1\le j \le k\le d}|G_{jk}| \le t + \eta_n\right) + \frac{C}{n}\\
&\qquad\qquad+ \sup_{t\ge0}\left|\mathbb{P}\left(\max_{1\le j\le k \le d}|G_{jk}| \le t\right) - \mathbb{P}\left(\max_{1\le j < k\le d}\left|\frac{1}{\sqrt{n}}\sum_{i=1}^n \frac{\psi_{jk}(X_i)}{\zeta_{jk}}\right| \le t\right)\right|.
\end{split}
\end{equation}

By Nazarov's inequality  \citep[see Lemma A.1 in][]{chernozhukov2017detailed},
\[
\mathbb{P}\left(t - \eta_n \le \max_{1\le j \le k\le d}|G_{jk}| \le t + \eta_n\right) \le C  \eta_n\sqrt{\log d},
\]
for a universal constant $C>0$. 
To bound the last term of~\eqref{eq:penultimate-partial-correlation11}, we use Theorem 2.1(a) of~\cite{koike2019notes}. Firstly, note that $a_j(X)$ (in~\eqref{eq:ajx-function}) is sub-Gaussian by assumption and hence $\psi_{jk}(X)$ is sub-exponential satisfying $\|\psi_{jk}(X)\|_{\psi_1} \le CK_x^2$ for some universal constant $C$; this also implies that $B_n = CK_x^2$ in Theorem 2.1(a) of~\cite{koike2019notes}. Thus, Theorem 2.1(a) of~\cite{koike2019notes} yields
\begin{equation}
\begin{split}
&\sup_{t\ge0}\left|\mathbb{P}\left(\max_{1\le j\le k \le d}|G_{jk}| \le t\right) - \mathbb{P}\left(\max_{1\le j < k\le d}\left|\frac{1}{\sqrt{n}}\sum_{i=1}^n \frac{\psi_{jk}(X_i)}{\zeta_{jk}}\right| \le t\right)\right|\\ 
&\quad\le C(\log d)^{2/3}\left[\left(\frac{K_x^4\log^3d}{n}\right)^{1/6} + \left(\frac{K_x^3\log^2d\log^2n}{n}\right)^{1/3}\right]\\
&\quad\le CK_x^{4/3}\left[\frac{(\log d)^{5/6}}{n^{1/6}} + \frac{\log d(\log n)^{2/3}}{n^{1/3}}\right] \le CK_x^{4/3}\frac{(\log(d\vee n))^{5/6}}{n^{1/6}}.
\end{split}
\end{equation}
Substituting this bound in~\eqref{eq:penultimate-partial-correlation11} completes the proof.
% for a constant $C$ depending on $\min_{1\le j < k\le d}\mbox{Var}(\psi_{jk}(X_1))$ through the anti-concentration constant. \textcolor{red}{Same comments as at the end of the proof of Theorem~\ref{thm:main-rates-thm-OLS-independence}.}
\end{proof}

\begin{proof}[Proof of Theorem~\ref{eq:multplier-bootstrap-consistency-partial-corr}]
By Corollary 1 of~\cite{massart1990tight}, we obtain
\[
\sup_{t\ge0}\left|\frac{1}{B}\sum_{i=1}^n \mathbbm{1}\{T_b \le t\} - \mathbb{P}\left(T_b \le t\big|X_1,\ldots,X_n\right)\right| \le \sqrt{\frac{\log(2n)}{2B}},
\]
with probability at least $1 - 1/n$. 

% Define
% \[
% \widetilde{T}_b ~:=~ \max_{1\le j < k\le d}\frac{|n^{-1/2}\sum_{i=1}^n e_i^{b}{\psi}_{jk}(X_i)|}{\sqrt{n^{-1}\sum_{i=1}^n {\psi}_{jk}^2(X_i)}}.
% \]
% From the definition of $T_b$, it follows that
% \[
% |T_b - \widetilde{T}_b| \le \max_{1\le j < k\le d}\left|\frac{1}{\sqrt{n}}\sum_{i=1}^n e_i^{(b)}\left(\frac{{\psi}_{jk}(X_i)}{\sqrt{n^{-1}\sum_{i=1}^n {\psi}_{jk}^2(X_i)}} - \frac{\widehat{\psi}_{jk}(X_i)}{\sqrt{n^{-1}\sum_{i=1}^n \widehat{\psi}_{jk}^2(X_i)}}\right)\right|
% \]
% Gaussian concentration inequality~\citep[Equation (B.5)]{belloni2018high} now yields
% \[
% \mathbb{P}\left(|T_b - \widetilde{T}_b| \ge \Delta^*(\sqrt{2\log(d^2)} + \sqrt{2\log n})\right) \le \frac{1}{n},
% \]
% where
% \begin{align*}
% \Delta^* &:= \max_{1\le j < k\le d}\,\frac{1}{n}\sum_{i=1}^n \left(\frac{{\psi}_{jk}(X_i)}{\sqrt{n^{-1}\sum_{i=1}^n {\psi}_{jk}^2(X_i)}} - \frac{\widehat{\psi}_{jk}(X_i)}{\sqrt{n^{-1}\sum_{i=1}^n \widehat{\psi}_{jk}^2(X_i)}}\right)^2,\\
% &= 2\max_{1\le j < k\le d}\,1 - \frac{n^{-1}\sum_{i=1}^n \psi_{jk}(X_i)\widehat{\psi}_{jk}(X_i)}{\sqrt{n^{-1}\sum_{i=1}^n {\psi}_{jk}^2(X_i)}\sqrt{n^{-1}\sum_{i=1}^n \widehat{\psi}_{jk}^2(X_i)}}\\
% &\le 2\max_{1\le j < k\le d}\sqrt{\frac{n^{-1}\sum_{i=1}^n (\psi_{jk}(X_i) - \widehat{\psi}_{jk}(X_i))^2}{n^{-1}\sum_{i=1}^n \psi_{jk}^2(X_i)}} + \left|1 - \sqrt{\frac{n^{-1}\sum_{i=1}^n \widehat{\psi}_{jk}^2(X_i)}{n^{-1}\sum_{i=1}^n \psi_{jk}^2(X_i)}}\right|.
% \end{align*}
% The inequalities for $\Delta^*$ above yield
% \[
% \Delta^* \le 3\max_{1\le j < k\le d}\sqrt{\frac{n^{-1}\sum_{i=1}^n (\psi_{jk}(X_i) - \widehat{\psi}_{jk}(X_i))^2}{n^{-1}\sum_{i=1}^n \psi_{jk}^2(X_i)}}.
% \]
By Lemma 3.1 of~\cite{Cher13}, we obtain
\begin{equation}\label{eq:gaussian-comparison-partial-corr}
\sup_{t\ge0}\left|\mathbb{P}(T_b \le t|X_1,\ldots,X_n) - \mathbb{P}\left(\max_{1\le j < k\le d}|G_{jk}| \le t\right)\right| \le C\Delta_0^{1/3}(1\vee\log(d^2/\Delta_0))^{2/3},
\end{equation}
where
\[
\Delta_0 := \max_{\substack{1\le j < k\le d,\\1\le j' \le k'\le d}}\left|\widehat{\mbox{corr}}(\widehat{\psi}_{jk}, \widehat{\psi}_{j'k'}) - \mbox{corr}(\psi_{jk}, \psi_{j'k'})\right|,
\]
with $\widehat{\mbox{corr}}$  defined as the sample correlation between $(\widehat{\psi}_{jk}(X_i), 1\le i\le n)$ and $(\widehat{\psi}_{j'k'}(X_i), 1\le i\le n)$. The rest of the proof is devoted to bounding the term $\Delta_0$.
Towards that end, Lemma~\ref{lem:correlations-from-covariances}  yields that
\begin{equation}\label{eq:Delta_zero-Delta_tilde-bound}
\Delta_0 ~\le~ 4\widetilde{\Delta}_0 := 4\max_{\substack{1\le j,k\le d,\\1\le j',k' \le d}}\,\left|\frac{\widehat{\mbox{cov}}(\widehat{\psi}_{jk}, \widehat{\psi}_{j'k'}) - \mbox{cov}(\psi_{jk}\psi_{j'k'})}{\sqrt{\mbox{Var}(\psi_{jk})\mbox{Var}(\psi_{j'k'})}}\right|, 
\end{equation}
whenever $\widetilde{\Delta}_0 \le 1/2$. 
Below we will derive a high-probability bound for $\tilde{\Delta}_0$, which is shown to be vanishing provided that $d \le \sqrt{n}$.

Because $\sum_{i=1}^n \widehat{\psi}_{jk}(X_i) = 0$, the empirical covariance is given by
\[
\widehat{\mbox{cov}}(\widehat{\psi}_{jk}, \widehat{\psi}_{j'k'}) ~=~ \frac{1}{n}\sum_{i=1}^n \widehat{\psi}_{jk}(X_i)\widehat{\psi}_{j'k'}(X_i),
\]
and similarly, $\mbox{cov}(\psi_{jk}, \psi_{j'k'}) = \mathbb{E}[\psi_{jk}(X)\psi_{j'k'}(X)]$. These equalities lead to
\begin{equation}\label{eq:basic-decomposition-partial-correlation}
\begin{split}
\widehat{\mbox{cov}}(\widehat{\psi}_{jk}, \widehat{\psi}_{j'k'}) - \mbox{cov}(\psi_{jk}, \psi_{j'k'}) &= \frac{1}{n}\sum_{i=1}^n \left\{\widehat{\psi}_{jk}(X_i)\widehat{\psi}_{j'k'}(X_i) - \psi_{jk}(X_i)\psi_{j'k'}(X_i)\right\}\\
&\qquad+ \frac{1}{n}\sum_{i=1}^n \Big\{\psi_{jk}(X_i)\psi_{j'k'}(X_i) - \mathbb{E}\left[\psi_{jk}(X)\psi_{j'k'}(X)\right]\Big\}.
\end{split}
\end{equation}
By Lemma~\ref{lemma:Thm3.1.KuchAbhi}, with probability at least $1 - 3/n$,
\[
\left|\frac{1}{n}\sum_{i=1}^n \left\{\psi_{jk}(X_i)\psi_{j'k'}(X_i) - \mathbb{E}\left[\psi_{jk}(X)\psi_{j'k'}(X)\right]\right\}\right| \le CK_x^4\left[\sqrt{\frac{\log(dn)}{n}} + \frac{\log^{2}(dn)}{n}\right].
\]
%Theorem 3.4 of~\cite{KuchAbhi17} concludes that with probability at least $1 - d/n$,
%\begin{equation}\label{eq:easy-part-correlation}
%\left|\frac{1}{n}\sum_{i=1}^n \left\{\psi_{jk}(X_i)\psi_{j'k'}(X_i) - \mathbb{E}\left[\psi_{jk}(X)\psi_{j'k'}(X)\right]\right\}\right| \le CK_x^2\sqrt{\frac{\log(nd)}{n}} + CK_x^2\frac{(\log(nd))^4}{n},
%\end{equation}
%for an absolute constant $C\in(0,\infty)$. \textcolor{red}{Same comments as earlier: please provide details and where does the $4$ in the exponent come from in the second term?} 
We now bound the first term in~\eqref{eq:basic-decomposition-partial-correlation} as follows:
\begin{align*}
&\left|\frac{1}{n}\sum_{i=1}^n \left\{\widehat{\psi}_{jk}(X_i)\widehat{\psi}_{j'k'}(X_i) - \psi_{jk}(X_i)\psi_{j'k'}(X_i)\right\}\right|\\ 
&\qquad\le \left|\frac{1}{n}\sum_{i=1}^n \left(\widehat{\psi}_{jk}(X_i) - \psi_{jk}(X_i)\right)\psi_{j'k'}(X_i)\right|\\
&\qquad\qquad+ \left|\frac{1}{n}\sum_{i=1}^n \left(\widehat{\psi}_{j'k'}(X_i) - \psi_{j'k'}(X_i)\right)\psi_{jk}(X_i)\right|\\
&\qquad\qquad+ \left|\frac{1}{n}\sum_{i=1}^n \left(\widehat{\psi}_{j'k'}(X_i) - \psi_{j'k'}(X_i)\right)\left(\widehat{\psi}_{jk}(X_i) - \psi_{jk}(X_i)\right)\right|\\
&\qquad\le 2\max_{1\le j < k\le d}\,\sqrt{\frac{1}{n}\sum_{i=1}^n \left(\widehat{\psi}_{jk}(X_i) - \psi_{jk}(X_i)\right)^2}\max_{1\le j < k\le d}\,\sqrt{\frac{1}{n}\sum_{i=1}^n \psi_{jk}^2(X_i)}\\
&\qquad\qquad+ \max_{1\le j < k\le d}\,{\frac{1}{n}\sum_{i=1}^n \left(\widehat{\psi}_{jk}(X_i) - \psi_{jk}(X_i)\right)^2}.
\end{align*}

Applying Lemma~\ref{lem:application-theorem8} with $W_i = \psi_{jk}^2(X_i)$, which by assumption is sub-Weibull$(1/2)$, we have that for all $t > 0$,
\[
\mathbb{P}\left(\frac{1}{n}\sum_{i=1}^n \psi_{jk}^2(X_i) \ge 2e\mathbb{E}\left[\psi_{jk}^2(X)\right] + \frac{4etK_w\log^2n}{n} + \frac{4et^3K_w}{n}\right) \le 3e^{-t}.
\]
By union bound over $1\le j < k\le d$, i.e., taking $t = \log(d^2)$, this implies that, with probability at least $1 - 3/n$,
\begin{align*}
\max_{1\le j < k\le d}\frac{1}{n}\sum_{i=1}^n \psi_{jk}^2(X_i) &\le 2e\max_{1\le j < k\le d}\mathbb{E}\left[\psi_{jk}^2(X)\right] + \frac{8e\log(d)K_w\log^2n}{n} + \frac{32e\log^3dK_w}{n}\\
&\le 2eK_w + \frac{8eK_w\log^3(nd)}{n} + \frac{32eK_w\log^3(dn)}{n}\\
&= CK_w\left(1 + \frac{\log^3(nd)}{n}\right). 
\end{align*}
Hence with probability at least $1 - 3/n$,
\begin{equation}\label{eq:square-power-psi}
\max_{1\le j < k\le d}\sqrt{\frac{1}{n}\sum_{i=1}^n \psi_{jk}^2(X_i)} \le C\sqrt{K_w}\left(1 + \sqrt{\frac{\log^3(nd)}{n}}\right).
\end{equation}



%Theorem 8 of~\cite{Bouch05} along with Markov's inequality yields
%\[
%\mathbb{P}\left(\frac{1}{n}\sum_{i=1}^n \psi_{jk}^2(X_i) \ge 2\mathbb{E}[\psi_{jk}^2(X)] + CK_x^4\frac{t^5(\log n)^4}{n}\right) \le 3e^{-t}.
%\]
%\textcolor{red}{Please provide details}
%Now a union bound concludes with probability at least $1 - 3/n$,
%\begin{equation}\label{eq:square-power-psi}
%\max_{1\le j < k\le d}\,\sqrt{\frac{1}{n}\sum_{i=1}^n \psi_{jk}^2(X_i)} ~\le~ CK_x^2\left(1 + \sqrt{\frac{(\log(dn))^9}{n}}\right), 
%\end{equation}
%where we have used the fact that, for all $j$ and $k$, $\psi_{jk}(X_i)$ is sub-exponential with parameter bounded by $C K_x^2$.

Hence with probability at least $1 - 3/n,$
\begin{equation}\label{eq:first-term-decomposition-first}
\begin{split}
&\left|\frac{1}{n}\sum_{i=1}^n \left\{\widehat{\psi}_{jk}(X_i)\widehat{\psi}_{j'k'}(X_i) - \psi_{jk}(X_i)\psi_{j'k'}(X_i)\right\}\right|\\ &\qquad\le C\sqrt{K_x}\left(\sqrt{\frac{\log^3(dn)}{n}} + 1\right)\max_{1\le j < k\le d}\sqrt{\frac{1}{n}\sum_{i=1}^n \left(\widehat{\psi}_{jk}(X_i) - \psi_{jk}(X_i)\right)^2}\\
&\qquad\qquad+ \max_{1\le j < k\le d}\,{\frac{1}{n}\sum_{i=1}^n \left(\widehat{\psi}_{jk}(X_i) - \psi_{jk}(X_i)\right)^2}.
\end{split}
\end{equation}
Assuming $d \le \sqrt{n}$, Lemma~\ref{lem:rate-of-convergence-psihat-minus-psi} proves that with probability at least $1 - C/n$,
\[
\max_{1\le j < k\le d}\sqrt{\frac{1}{n}\sum_{i=1}^n \left(\widehat{\psi}_{jk}(X_i) - \psi_{jk}(X_i)\right)^2} \le CK_x^5\sqrt{\frac{d + \log n}{n}} + CK_x^6\frac{d + \log n}{n}.
\]
Substituting this in~\eqref{eq:first-term-decomposition-first} and then in~\eqref{eq:Delta_zero-Delta_tilde-bound} proves that with probability at least $1 - C/n$,
\[
\Delta_0 \le CK_x^5\sqrt{\frac{d + \log n}{n}} + CK_x^6\frac{d + \log n}{n},
\]
whenever  $d \le \sqrt{n}$ (required for Lemma~\ref{lem:rate-of-convergence-psihat-minus-psi}) and the right hand side is less than 1. Hence,
\[
\sup_{t\ge0}\left|\mathbb{P}(T_b \le t|X_1,\ldots,X_n) - \mathbb{P}\left(\max_{1\le j < k\le d}|G_{jk}| \le t\right)\right| \le CK_x^{2}(\log d)^{2/3}\left(\frac{d + \log n}{n}\right)^{1/6}.
\]
\end{proof}


\begin{lemma}\label{lem:correlations-from-covariances}
Suppose $X_1, \ldots, X_n\in\mathbb{R}^d$ are independent and identically distributed random vectors. Set
\[
\widehat{\sigma}_{jk} := \widehat{\mathrm{cov}}(X_{ij}, X_{ik}),
\]
to be the empirical covariance between $(X_{ij}, 1\le i\le n)$ and $(X_{ik}, 1\le i\le n)$. The empirical correlation $\widehat{\sigma}_{jk}/\sqrt{\widehat{\sigma}_{jj}\widehat{\sigma}_{kk}}$ is denoted by $\widehat{\rho}_{jk}$. Let $\sigma_{jk}$ and $\rho_{jk}$ represent the corresponding population covariance and correlations. Then
\[
\max_{1\le j \le k\le D}\left|\widehat{\rho}_{jk} - {\rho}_{jk}\right| ~\le~ 4\max_{1\le j < k\le d}\left|\frac{\widehat{\sigma}_{jk} - \sigma_{jk}}{\sqrt{\sigma_{jj}\sigma_{kk}}}\right|,
\]
whenever
\[
\max_{1\le j < k\le d}\left|\frac{\widehat{\sigma}_{jk} - \sigma_{jk}}{\sqrt{\sigma_{jj}\sigma_{kk}}}\right| ~\le~ \frac{1}{2}.
\]
\end{lemma}
\begin{proof}
Fix $1\le j < k\le d$ and set
\[
\Delta := \max_{1\le j < k\le d}\left|\frac{\widehat{\sigma}_{jk} - \sigma_{jk}}{\sqrt{\sigma_{jj}\sigma_{kk}}}\right|
\]
Then,
\begin{equation}\label{eq:Delta-definition}
 \frac{1}{1 + \Delta} ~\le~ \frac{\sigma_{jj}}{\widehat{\sigma}_{jj}} ~\le~ \frac{1}{1-\Delta}\quad\mbox{for all}\quad 1\le j\le d.
\end{equation}
Observe that
\begin{align*}
\left|\widehat{\rho}_{jk} - \rho_{jk}\right| &\le \frac{|\widehat{\sigma}_{jk} - \sigma_{jk}|}{\sqrt{\widehat{\sigma}_{jj}\widehat{\sigma}_{kk}}} + \frac{\sigma_{jk}}{\sqrt{\sigma_{jj}\sigma_{kk}}}\left|\sqrt{\frac{\sigma_{jj}\sigma_{kk}}{\widehat{\sigma}_{jj}\widehat{\sigma}_{kk}}} - 1\right|\\
&\le \Delta\sqrt{\frac{\sigma_{jj}\sigma_{kk}}{\widehat{\sigma}_{jj}\widehat{\sigma}_{kk}}} + \left|\sqrt{\frac{\sigma_{jj}\sigma_{kk}}{\widehat{\sigma}_{jj}\widehat{\sigma}_{kk}}} - 1\right|.% ~\le~ \Delta + (\Delta + 1)\left|\sqrt{\frac{\sigma_{jj}\sigma_{kk}}{\widehat{\sigma}_{jj}\widehat{\sigma}_{kk}}} - 1\right|.
\end{align*}
To bound the last term, we see from~\eqref{eq:Delta-definition} that, for all $j$ and $k$,
\[
\frac{1}{(1+\Delta)^2} ~\le~ \frac{\sigma_{jj}\sigma_{kk}}{\widehat{\sigma}_{jj}\widehat{\sigma}_{kk}} ~\le~ \frac{1}{(1 - \Delta)^2}, 
\]
which implies that
\[
\left|\sqrt{\frac{\sigma_{jj}\sigma_{kk}}{\widehat{\sigma}_{jj}\widehat{\sigma}_{kk}}} - 1\right| \le \frac{\Delta}{1 - \Delta}.
\]
Therefore, provided that $\Delta \leq 1/2$,
\[
\left|\widehat{\rho}_{jk} - \rho_{jk}\right| \le \frac{2\Delta}{1 - \Delta} \leq 4 \Delta.
\]
\end{proof}
\begin{lemma}\label{lem:psihat-minus-psi-part1}
For functions $\widehat{a}, \widehat{b}, a, b$ and scalars $\hat{\theta}, \theta\in[-1, 1]$, let
\begin{align*}
\widehat{\psi}(x) &:= \widehat{a}(x) - \mathbb{E}_n[\widehat{a}(X)] - \widehat{\theta}\left\{\widehat{b} - \mathbb{E}_n[b(X)]\right\},\\
\psi(x) &:= a(x) - \mathbb{E}[a(X)] - \theta\left\{b(x) - \mathbb{E}[b(X)]\right\}.
\end{align*}
Then
\begin{align*}
\|\widehat{\psi} - \psi\|_{n} &\le \|\widehat{a} - a\|_n + |\mathbb{E}_n[a(X)] - \mathbb{E}[a(X)]| + \|\widehat{b} - b\|_n + |\mathbb{E}_n[b(X)] - \mathbb{E}[b(X)]|\\
&\qquad+ |\theta - \widehat{\theta}|\;\|b - \mathbb{E}[b(X)]\|_n,
\end{align*}
where for any function $f$, $\|f\|_n := \sqrt{n^{-1}\sum_{i=1}^n f^2(X_i)}.$
\end{lemma}
\begin{proof}
The proof is mostly algebraic manipulation. For notational ease, we write $\widehat{\psi}$ for $\widehat{\psi}(x)$ and similarly for other functions. Firstly,
\begin{align*}
\widehat{\psi} - \psi &= \widehat{a} - a - \mathbb{E}_n[\widehat{a}] + \mathbb{E}[a] - \widehat{\theta}(\widehat{b} - \mathbb{E}_n[\widehat{b}]) + \theta(b - \mathbb{E}[b])\\
&= (\widehat{a} - a) - \mathbb{E}_n[\widehat{a} - a] - (\mathbb{E}_n[a] - \mathbb{E}[a]) - \widehat{\theta}(\widehat{b} - b - \mathbb{E}_n[\widehat{b}] + \mathbb{E}[b])\\
&\qquad+ (\theta - \widehat{\theta})(b - \mathbb{E}[b])\\
&= (\widehat{a} - a) - \mathbb{E}_n[\widehat{a} - a] - (\mathbb{E}_n[a] - \mathbb{E}[a]) - \widehat{\theta}(\widehat{b} - b - \mathbb{E}_n[\widehat{b} - b])\\
&\qquad + \widehat{\theta}(\mathbb{E}_n[b] - \mathbb{E}[b]) + (\theta - \widehat{\theta})(b - \mathbb{E}[b]).
\end{align*}
Observe now that
\[
\|(\widehat{a} - a) - \mathbb{E}_n[\widehat{a} - a]\|_n \le \|\widehat{a} - a\|_n.
\]
Using the fact $\widehat{\theta}, \theta\in[-1, 1]$ concludes the proof.
\end{proof}
\begin{lemma}\label{lem:psihat-minus-psi-part2}
In the notation of~\eqref{eq:ajx-function} and~\eqref{eq:ajhatx-function}, for any $1\le j < k\le d$, we have
\begin{align*}
\|\widehat{a}_j\widehat{a}_k - a_ja_k\|_n &\le 2\max_{1\le j\le d}\left(\frac{1}{n}\sum_{i=1}^n a_j^4(X_i)\right)^{1/4}\left(\frac{1}{n}\sum_{i=1}^n (\widehat{a}_j(X_i) - a_j(X_i))^4\right)^{1/4}\\
&\qquad+ \max_{1\le j\le d}\left(\frac{1}{n}\sum_{i=1}^n (\widehat{a}_j(X_i) - a_j(X_i))^4\right)^{1/2}.
\end{align*}
Consequently, there exists a universal constant $C\in(0, \infty)$ such that for all $1\le j < k\le d$,
\begin{align*}
\|\widehat{\psi}_{jk} - \psi_{jk}\|_n &\le C\max_{1\le j\le d}\left(\frac{1}{n}\sum_{i=1}^n a_j^4(X_i)\right)^{1/4}\left(\frac{1}{n}\sum_{i=1}^n (\widehat{a}_j(X_i) - a_j(X_i))^4\right)^{1/4}\\
&\qquad+ C\max_{1\le j\le d}\left(\frac{1}{n}\sum_{i=1}^n (\widehat{a}_j(X_i) - a_j(X_i))^4\right)^{1/2}\\
&\qquad+ C\max_{1\le j\le d}|\mathbb{E}_n[a_j^2(X)] - \mathbb{E}[a_j^2(X)]|\\
&\qquad+ C\max_{1\le j < k\le d}|\theta_{jk} - \widehat{\theta}_{jk}|\max_{1\le j\le d}\|a_j^2 - \mathbb{E}[a_j^2(X)]\|_n.
\end{align*}
\end{lemma}
\begin{proof}
Clearly,
\begin{align*}
|\widehat{a}_j\widehat{a}_k - a_j a_k| &\le |\widehat{a}_j||\widehat{a}_k - a_k| + |a_k||\widehat{a}_j - a_j|\\
&\le |a_j||\widehat{a}_k - a_k| + |a_k||\widehat{a}_j - a_j| + |\widehat{a}_j - a_j||\widehat{a}_k - a_k|.
\end{align*}
Applying $\|\cdot\|_n$ on both sides and using Cauchy-Schwarz inequality concludes the proof of the first inequality. The second part follows from an application of Lemma~\ref{lem:psihat-minus-psi-part1}. 
\end{proof}
\begin{lemma}\label{lem:ajhat-minus-aj}
For any $1\le j\le d$,
\begin{align*}
&\left(\frac{1}{n}\sum_{i=1}^n (\widehat{a}_j(X_i) - a_j(X_i))^4\right)^{1/4}\\ ~&\qquad\le~ \frac{2\mathcal{D}_n^{\Sigma}}{1 - \mathcal{D}_n^{\Sigma}}\sup_{\theta:\|\theta\| \le 1}\left(\frac{1}{n}\sum_{i=1}^n {\{\theta^{\top}\Sigma^{-1/2}(X_i - \mu_X)\}^4}\right)^{1/4} ~+~ \frac{\|\Sigma^{-1/2}(\overline{X}_n - \mu_X)\|}{1 - \mathcal{D}_n^{\Sigma}}.
\end{align*}
\end{lemma}
\begin{proof}
Recall that
\[
\widehat{a}_j(x) := \frac{(x - \overline{X}_n)^{\top}\widehat{\Sigma}^{-1}e_j}{\sqrt{e_j^{\top}\widehat{\Sigma}^{-1}e_j}}\quad\mbox{and}\quad a_j(x) := \frac{(x - \mu_X)^{\top}\Sigma^{-1}e_j}{\sqrt{e_j^{\top}\Sigma^{-1}e_j}}.
\]
We will now bound $\widehat{a}_j(x) - a_j(x)$. Note that
\begin{align*}
\sup_{x}\left|\widehat{a}_j(x) - \frac{(x - \mu_{X})^{\top}\widehat{\Sigma}^{-1}e_j}{\sqrt{e_j^{\top}\widehat{\Sigma}^{-1}e_j}}\right| &= \left|\frac{(\overline{X}_n - \mu_X)^{\top}\widehat{\Sigma}^{-1}e_j}{\sqrt{e_j^{\top}\widehat{\Sigma}^{-1}e_j}}\right|\\ ~&\le~ \|\widehat{\Sigma}^{-1/2}(\overline{X}_n - \mu_X)\| ~\le~ \frac{\|\Sigma^{-1/2}(\overline{X}_n - \mu_X)\|}{1 - \mathcal{D}_n^{\Sigma}}. 
\end{align*}
Furthermore, 
\begin{align*}
\frac{(x - \mu_X)^{\top}\widehat{\Sigma}^{-1}e_j}{\sqrt{e_j^{\top}\widehat{\Sigma}^{-1}e_j}} - a_j(x) &= \frac{(x - \mu_X)^{\top}(\widehat{\Sigma}^{-1} - \Sigma^{-1})e_j}{\sqrt{e_j^{\top}\widehat{\Sigma}^{-1}e_j}} + \frac{(x - \mu_X)^{\top}\Sigma^{-1}e_j}{\sqrt{e_j^{\top}\Sigma^{-1}e_j}}\left[\sqrt{\frac{e_j^{\top}\Sigma^{-1}e_j}{e_j^{\top}\widehat{\Sigma}^{-1}e_j}} - 1\right].
\end{align*}
Combining these two steps yields
\begin{align*}
&\left(\frac{1}{n}\sum_{i=1}^n (\widehat{a}_j(X_i) - a_j(X_i))^4\right)^{1/4}\\ ~&\qquad\le~ \frac{1}{\sqrt{e_j^{\top}\widehat{\Sigma}^{-1}e_j}}\left(\frac{1}{n}\sum_{i=1}^n \left\{(\Sigma^{-1/2}(X_i - \mu_X))^{\top}(\Sigma^{1/2}\widehat{\Sigma}^{-1}\Sigma^{1/2} - I_d)\Sigma^{1/2}e_j\right\}^4\right)^{1/4}\\
~&\qquad\qquad+~ \left|\sqrt{\frac{e_j^{\top}\Sigma^{-1}e_j}{e_j^{\top}\widehat{\Sigma}^{-1}e_j}} - 1\right|\left(\frac{1}{n}\sum_{i=1}^n \frac{\{(X_i - \mu_X)^{\top}\Sigma^{-1}e_j\}^4}{(e_j^{\top}\Sigma^{-1}e_j)^2}\right)^{1/4} + \frac{\|\Sigma^{-1/2}(\overline{X}_n - \mu_X)\|}{1 - \mathcal{D}_n^{\Sigma}}.
\end{align*}
The first term can be further bounded by
\begin{align*}
&\frac{\|(\Sigma^{1/2}\widehat{\Sigma}^{-1}\Sigma^{1/2} - I_d)\Sigma^{1/2}e_j\|}{\sqrt{e_j^{\top}\widehat{\Sigma}^{-1}e_j}}\sup_{\theta:\|\theta\| \le 1}\left(\frac{1}{n}\sum_{i=1}^n \left\{(\Sigma^{-1/2}(X_i - \mu_X))^{\top}\theta\right\}^4\right)^{1/4}\\
&\qquad\le \frac{\mathcal{D}_n^{\Sigma}}{1 - \mathcal{D}_n^{\Sigma}}\sqrt{\frac{e_j^{\top}\Sigma^{-1}e_j}{e_j^{\top}\widehat{\Sigma}^{-1}e_j}}\sup_{\theta:\|\theta\| \le 1}\left(\frac{1}{n}\sum_{i=1}^n \left\{(\Sigma^{-1/2}(X_i - \mu_X))^{\top}\theta\right\}^4\right)^{1/4}.
\end{align*}
Similarly, the second term is bounded by
\[
\left|\sqrt{\frac{e_j^{\top}\Sigma^{-1}e_j}{e_j^{\top}\widehat{\Sigma}^{-1}e_j}} - 1\right|\sup_{\theta:\|\theta\| \le 1}\left(\frac{1}{n}\sum_{i=1}^n {\{(\Sigma^{-1/2}(X_i - \mu_X))^{\top}\theta\}^4}\right)^{1/4}
\]
Also, we use the fact that
\[
\left|\sqrt{\frac{e_j^{\top}\Sigma^{-1}e_j}{e_j^{\top}\widehat{\Sigma}^{-1}e_j}} - 1\right| ~\le~ \left|{\frac{e_j^{\top}\Sigma^{-1}e_j}{e_j^{\top}\widehat{\Sigma}^{-1}e_j}} - 1\right| ~\le~ \mathcal{D}_n^{\Sigma}\quad\Rightarrow\quad \sqrt{\frac{e_j^{\top}\Sigma^{-1}e_j}{e_j^{\top}\widehat{\Sigma}^{-1}e_j}} \le 1 + \mathcal{D}_n^{\Sigma}.
\]
Therefore,
\begin{align*}
&\left(\frac{1}{n}\sum_{i=1}^n (\widehat{a}_j(X_i) - a_j(X_i))^4\right)^{1/4}\\ &\qquad\le \frac{2\mathcal{D}_n^{\Sigma}}{1 - \mathcal{D}_n^{\Sigma}}\sup_{\theta:\|\theta\| \le 1}\left(\frac{1}{n}\sum_{i=1}^n {\{(\Sigma^{-1/2}(X_i - \mu_X))^{\top}\theta\}^4}\right)^{1/4} + \frac{\|\Sigma^{-1/2}(\overline{X}_n - \mu_X)\|}{1 - \mathcal{D}_n^{\Sigma}}.
\end{align*}
\end{proof}
\begin{lemma}\label{lem:rate-of-convergence-psihat-minus-psi}
Under the assumptions of Theorem~\ref{thm:Berry-Esseen-bound-partial-corr}, there exists a universal constant $C\in(0,\infty)$ such that with probability at least $1 - C/n$,
\[
\max_{1\le j < k\le d}\,\left(\frac{1}{n}\sum_{i=1}^n \left\{\widehat{\psi}_{jk}(X_i) - \psi_{jk}(X_i)\right\}^2\right)^{1/2} \le CK_x^6\sqrt{\frac{d + \log n}{n}} + CK_x^5\frac{d + \log n}{n},
\]
whenever the right hand side is less than 1 and $d \le \sqrt{n}$. 
\end{lemma}
\begin{proof}
The calculations that lead to~\eqref{eq:quantity-M-4} imply that with probability $1 - 1/n$,
\[
\sup_{\theta:\|\theta\| \le 1}\left(\frac{1}{n}\sum_{i=1}^n {\{(\Sigma^{-1/2}(X_i - \mu_X))^{\top}\theta\}^4}\right)^{1/4} \le CK_x\left(1 + \frac{d^{1/2}}{n^{1/4}}\right) \le CK_x\left(1 + \frac{d}{\sqrt{n}}\right),
\]
for some universal constant $C\in(0, \infty)$. Further Lemma~\ref{lem:concentration-of-covariance} yields with probability $1 - 1/n$,
\[
\mathcal{D}_n^{\Sigma} \le CK_x^2\sqrt{\frac{d + \log n}{n}} + CK_x^2\frac{d + \log n}{n}.
\]
Finally,
\[
\|\Sigma^{-1/2}(\overline{X}_n - \mu_X)\| \le 2\sup_{\theta\in\mathcal{N}_{1/2}}\left|\frac{1}{n}\sum_{i=1}^n \theta^{\top}\Sigma^{-1/2}(X_i - \mu_X)\right|,
\]
where $\mathcal{N}_{1/2}$ is the $1/2$-net of $\{\theta\in\mathbb{R}^d:\,\|\theta\| \le 1\}$ with cardinality $|\mathcal{N}_{1/2}| \le 5^d$. Hence with probability at least $1 - 1/n$,
\[
\|\Sigma^{-1/2}(\overline{X}_n - \mu_X)\| \le CK_x\sqrt{\frac{d + \log n}{n}}.
\]
Combining these inequalities with Lemma~\ref{lem:ajhat-minus-aj} concludes that with probability at least $1 - 3/n$,
\begin{align*}
\max_{1\le j\le d}\,\left(\frac{1}{n}\sum_{i=1}^n (\widehat{a}_j(X_i) - a_j(X_i))^4\right)^{\frac{1}{4}} &\le CK_x^3\frac{d + \sqrt{n}}{\sqrt{n}}\left[\sqrt{\frac{d + \log n}{n}} + \frac{d + \log n}{n}\right] + CK_x\sqrt{\frac{d + \log n}{n}}\\
&\le CK_x^3\left(1 + \frac{d}{\sqrt{n}}\right)\sqrt{\frac{d + \log n}{n}} + CK_x\sqrt{\frac{d + \log n}{n}}\\
&\le CK_x^3\left(1 + \frac{d}{\sqrt{n}}\right)\sqrt{\frac{d + \log n}{n}},
\end{align*}
assuming  $d + \log n \le n$. Lemma~\ref{lem:psihat-minus-psi-part2} now yields with probability at least $1 - 3/n$,
\begin{align}
\max_{1\le j < k\le d}\,\|\widehat{\psi}_{jk} - \psi_{jk}\|_n &\le C\max_{1\le j\le d}\left(\frac{1}{n}\sum_{i=1}^n a_j^4(X_i)\right)^{1/4}K_x^3\sqrt{\frac{d + \log n}{n}} + CK_x^6\frac{d + \log n}{n}\nonumber\\
&\qquad+ C\max_{1\le j\le d}\left|\frac{1}{n}\sum_{i=1}^n a_j^2(X_i) - \mathbb{E}[a_j^2(X)]\right|\label{eq:first-part-psihat-minus-psi}\\
&\qquad+ C\max_{1\le j < k\le d}|\theta_{jk} - \widehat{\theta}_{jk}|\max_{1\le j\le d}\left(\frac{1}{n}\sum_{i=1}^n \{a_j^2(X_i) - \mathbb{E}[a_j^2(X)]\}^2\right)^{1/2}.\nonumber
\end{align}
The calculations leading to~\eqref{eq:square-power-psi} now yields with probability at least $1 - 6/n$,
\[
\max_{1\le j\le d}\left(\frac{1}{n}\sum_{i=1}^n \left\{a_j^2(X_i) - \mathbb{E}[a_j^2(X)]\right\}^2\right)^{1/4} \le CK_x\left(1 + \sqrt{\frac{(\log(dn))^9}{n}}\right),
\]
and
\[
\max_{1\le j\le d}\left(\frac{1}{n}\sum_{i=1}^n a_j^4(X_i)\right)^{1/4} \le CK_x\left(1 + \sqrt{\frac{(\log(dn))^9}{n}}\right).
\]
Because $a_j^2(X_i)$ is sub-exponential with parameter $K_x^2$, using Theorem 2.8.1 of~\cite{Vershynin18}, we get with probability at least $1 - 1/n$
\[
\max_{1\le j\le d}\left|\frac{1}{n}\sum_{i=1}^n a_j^2(X_i) - \mathbb{E}[a_j^2(X)]\right| \le CK_x^2\sqrt{\frac{\log(dn)}{n}} + CK_x^2\frac{\log(dn)}{n}.
\]
Substituting these in~\eqref{eq:first-part-psihat-minus-psi} concludes with probability at least $1 - C/n$, 
\begin{equation}
\begin{split}
\max_{1\le j < k\le d}\,\|\widehat{\psi}_{jk} - \psi_{jk}\|_n &\le CK_x^4\sqrt{\frac{d + \log n}{n}} + CK_x^6\frac{d + \log n}{n}\\
&\qquad+ CK_x^2\sqrt{\frac{\log(dn)}{n}} + CK_x^2\frac{\log(dn)}{n} + CK_x\max_{1\le j < k\le d}|\theta_{jk} - \widehat{\theta}_{jk}|.
\end{split}
\end{equation}
Using  $d \le \sqrt{n}$ as well as $K_x \ge 1$, we can simplify the terms above and write
\begin{equation}\label{eq:second-part-psihat-minus-psi}
\begin{split}
\max_{1\le j < k\le d}\,\|\widehat{\psi}_{jk} - \psi_{jk}\|_n &\le CK_x^6\sqrt{\frac{d + \log n}{n}} + CK_x^2\sqrt{\frac{\log(dn)}{n}} + CK_x\max_{1\le j < k\le d}|\theta_{jk} - \widehat{\theta}_{jk}|.
\end{split}
\end{equation}
The last term can be bounded based on Theorem~\ref{thm:linear-expansion-partial-corr} and~\eqref{eq:linear-rep-error-partial-corr} to get with probability at least $1 - 1/n$,
\[
\max_{1\le j < k\le d}|\theta_{jk} - \widehat{\theta}_{jk}| \le \max_{1\le j < k\le d}\left|\frac{1}{n}\sum_{i=1}^n \psi_{jk}(X_i)\right| + CK_x^4\frac{d + \log(n)}{n}.
\]
Because $\psi_{jk}(X_i), 1\le i\le n$ are sub-exponential with parameter $K_x^2$, Theorem 2.8.1 of~\cite{Vershynin18} implies that with probability $1 - C/n$,
\[
\max_{1\le j < k\le d}|\theta_{jk} - \widehat{\theta}_{jk}| ~\le~ CK_x\sqrt{\frac{\log(dn)}{n}} + CK_x^4\frac{d + \log n}{n}. 
\]
Substituting this in~\eqref{eq:second-part-psihat-minus-psi} concludes with probability at least $1 - C/n$,
\[
\max_{1\le j < k\le d}\|\widehat{\psi}_{jk} - \psi_{jk}\|_n \le CK_x^6\sqrt{\frac{d + \log n}{n}} + CK_x^2\sqrt{\frac{\log(dn)}{n}} + CK_x^2\sqrt{\frac{\log(dn)}{n}} + CK_x^5\frac{d + \log n}{n}.
\]
This concludes the proof.
\end{proof}







\section{Proof of the Auxiliary Results from Section ~\ref{section::explicit} (Projection Parameters)}
\label{appendix:auxiliary.ols}

In this section, we provide various lemmas used in the proofs of Theorems~\ref{thm:main-rates-thm-OLS-independence} and~\ref{thm:Berry-Esseen-OLS}. These lemmas prove concentration inequalities for the quantities that appear in the inequalities in previous sections. 
\begin{proposition}\label{prop:implication-moments-influence-function}
Under assumptions~\ref{eq:covariate-subGaussian} and~\ref{eq:moments-errors}, for every $\eta > 0$,
\[
\max_{a\in\mathbb{R}^d, \|a\|_{I_d} \le 1}\,\mathbb{E}\left[|a^{\top}\Sigma^{-1/2}X_i(Y_i - X_i^{\top}\beta)|^{q/(1+\eta)}\right] \le \left\{CK_qK_x\sqrt{q/\eta}\right\}^{q/(1+\eta)},
\] 
where $C\in(0, \infty)$ is a universal constant.
\end{proposition}
\begin{proof}
Fix $a\in\mathbb{R}^d$ with Euclidean norm bounded by 1. H{\"o}lder's inequality yields
\begin{align*}
\left(\mathbb{E}\left[|a^{\top}\Sigma^{-1/2}X_i(Y_i - X_i^{\top}\beta)|^{q/(1+\eta)}\right]\right)^{(1+\eta_n)/q} &\le (\mathbb{E}[|Y_i - X_i^{\top}\beta|^q])^{1/q}\left(\mathbb{E}[|a^{\top}\Sigma^{-1/2}X_i|^{q/\eta}]\right)^{\eta/q}\\
&\le K_q(CK_x\sqrt{q/\eta}),
\end{align*}
where the second inequality follows from the fact that condition~\ref{eq:covariate-subGaussian} implies $a^{\top} \Sigma^{-1/2}X_i$ is $K_x$-sub-Gaussian.
\end{proof}
\begin{lemma}\label{lem:concentration-of-covariance}
Under assumption~\ref{eq:covariate-subGaussian}, there exists a universal constant $C \in (0, \infty)$ such that
\[
\mathbb{P}\left(\mathcal{D}_n^{\Sigma} \le CK_x^2\sqrt{\frac{d + \log(1/\delta)}{n}} + CK_x^2\frac{d + \log(1/\delta)}{n}\right) \ge 1 - \delta\quad\mbox{for any}\quad\delta\in(0, 1).
\]
\end{lemma}
\begin{proof}
This results is standard: see, e.g., Theorem 4.7.1 of~\cite{Vershynin18} or Theorem 1 of~\cite{koltchinskii2017a}.
\end{proof}
\begin{lemma}\label{lem:concentration-influence-function}
Under assumptions~\ref{eq:DGP},~\ref{eq:covariate-subGaussian},~\ref{eq:moments-errors} and~\ref{eq:bounded-asymptotic-variance}, there exists a constant $C_q\in(0,\infty)$ depending only on $q$ such that for any $\eta > 0$ and $\delta\in(0, 1)$,
\[
\mathbb{P}\left(\left\|\frac{1}{n}\sum_{i=1}^n \psi_i\right\|_{\Sigma V^{-1}_n\Sigma} \le 2\sqrt{d} + \sqrt{3\log(1/\delta)} + K_qK_x\left(\frac{C_qn}{\delta}\right)^{{(1+\eta)}/{q}}\sqrt{\frac{q\overline{\lambda}d}{n\eta}}\right) ~\ge~ 1 - \delta.
\]
\end{lemma}
\begin{proof}
We apply Theorem 3.1 of~\cite{einmahl2008characterization}. Take 
\begin{align*}
Z_i &:= 
% n^{-1}(\Sigma V^{-1}_n\Sigma)^{1/2}\psi_i = n^{-1/2}(nV_n)^{-1/2}X_i(Y_i - X_i^{\top}\beta)\\ 
% &~= 
n^{-1/2}V^{-1/2}X_i(Y_i - X_i^{\top}\beta).
\end{align*}
% The last equality above follows from~\eqref{eq:Definiton-V-hat-V}. 
The definition of $\beta$ implies $\mathbb{E}[Z_i] = 0$ and the definition of $V$ implies $\mbox{Var}(Z_i) = n^{-1}I_d$. Then, $\left\|{n}^{-1}\sum_{i=1}^n \psi_i\right\|_{\Sigma V^{-1}_n\Sigma} = \| \sum_{i=1}^n Z_i\|_{I_d} $, so that
\[\textstyle
\mathbb{E}\left[\left\|{n}^{-1}\sum_{i=1}^n \psi_i\right\|_{\Sigma V^{-1}_n\Sigma}\right] \le \left(\mathbb{E}\left[\left\|\sum_{i=1}^n Z_i\right\|_{I_d}^2\right]\right)^{1/2} = \left(\sum_{i=1}^n \mbox{tr}(\mbox{Var}(Z_i))\right)^{1/2} = \sqrt{d}.
\]
Theorem 3.1 of~\cite{einmahl2008characterization} with $\eta = \delta = 1$ implies that
\begin{equation}\label{eq:tail-inequality-influence}
\mathbb{P}\left(\left\|\frac{1}{n}\sum_{i=1}^n \psi_i\right\|_{\Sigma V^{-1}_n\Sigma} \ge 2\sqrt{d} + t\right) \le \exp\left(-\frac{t^2}{3}\right) + \frac{C}{t^s}\sum_{i=1}^n \mathbb{E}[(Z_i^{\top}Z_i)^{s/2}], 
\end{equation}
for any $s > 2$ such that $\mathbb{E}[(Z_i^{\top}Z_i)^{s/2}]$ is finite. Here $C\in(0, \infty)$ is a constant depending only on $s$. It is clear that
\[
Z_i^{\top}Z_i = \frac{1}{n}(Y_i - X_i^{\top}\beta)^2X_i^{\top}V^{-1}X_i,
\]
and because of assumption~\ref{eq:DGP},
\begin{align*}
\sum_{i=1}^n \mathbb{E}[(Z_i^{\top}Z_i)^{s/2}] ~&=~ \frac{d^{s/2}}{n^{(s-2)/2}}\mathbb{E}\left[\left(\frac{1}{d}\sum_{j=1}^d (e_j^{\top}V^{-1/2}\Sigma^{1/2}\Sigma^{-1/2}X_i)^2(Y_i - X_i^{\top}\beta)^2\right)^{s/2}\right]\\
~&\le~ \frac{d^{s/2}}{n^{(s-2)/2}}\max_{1\le j\le d} \mathbb{E}\left[|a_j^{\top}\Sigma^{-1/2}X_i(Y_i - X_i^{\top}\beta)|^s\right],
\end{align*}
where $a_j := \Sigma^{1/2}V^{-1/2}e_j$. Assumption~\ref{eq:bounded-asymptotic-variance} implies that $\|a\|_{I_d} = \sqrt{e_j^{\top}V^{-1/2}\Sigma V^{-1/2}e_j} \le \sqrt{\lambda_{\max}(V^{-1/2}\Sigma V^{-1/2})} \le \overline{\lambda}^{1/2}$. Therefore, for any $s \ge 2$,
\[
\sum_{i=1}^n \mathbb{E}[(Z_i^{\top}Z_i)^{s/2}] ~\le~ \frac{\overline{\lambda}^{s/2}d^{s/2}}{n^{(s-2)/2}}\max_{1\le j\le d}\mathbb{E}\left[\|a\|_{I_d}^{-s}|a_j^{\top}\Sigma^{-1/2}X_i(Y_i - X_i^{\top}\beta)|^s\right].
\]
Fix $\eta > 0$. Taking $s = q/(1 + \eta)$ and applying Proposition~\ref{prop:implication-moments-influence-function} provides a bound on the right hand side. Further, taking (for a possibly different constant $C\in(0,\infty)$)
\[
t ~=~ \sqrt{3\log(1/\delta)} + (C\delta^{-1})^{(1 + \eta)/q}\frac{\overline{\lambda}^{1/2}d^{1/2}}{n^{1/2 - (1 + \eta)/q}}\left\{K_qK_x\sqrt{q/\eta}\right\},
\] 
in~\eqref{eq:tail-inequality-influence} yields for any $\eta > 0$,
\[
\mathbb{P}\left(\left\|\frac{1}{n}\sum_{i=1}^n \psi_i\right\|_{\Sigma V_n^{-1}\Sigma} \ge 2\sqrt{d} + \sqrt{3\log(1/\delta)} + \left(\frac{Cn}{\delta}\right)^{\frac{(1+\eta)}{q}}\left(\frac{q\overline{\lambda}d}{n\eta}\right)^{1/2}K_qK_x\right) \le \delta.
\]
\end{proof}
\begin{lemma}\label{lem:std-err-consistency}
Assume conditions~\ref{eq:DGP},~\ref{eq:covariate-subGaussian},~\ref{eq:bounded-asymptotic-variance}. If condition~\ref{eq:moments-errors} holds for some $q \ge 4\log n/(\log n - 2)$, then there exists a constant $C = C(q, K_x, \overline{\lambda} K_q^2, \overline{\lambda}/\underline{\lambda})$ depending only on its arguments such that with probability at least $1 - (d+2)/n - \sqrt{d/n}$,
\begin{align*}
\sup_{\theta\in\mathbb{R}^d}\left|\frac{\theta^{\top}\widehat{\Sigma}_n^{-1}\widehat{V}\widehat{\Sigma}_n^{-1}\theta}{\theta^{\top}\Sigma^{-1}V\Sigma^{-1}\theta} - 1\right| &\le \frac{C(1 + d/\sqrt{n})}{(1-4q^{-1})}\sqrt{\frac{d\log(ed) + \log n}{n}}\\ &\qquad+ C(1 + d/\sqrt{n})\frac{d^{1-1/q}\log(ed)\log n}{n^{1-3/q}},
\end{align*}
whenever the right hand side is less than $1$.
\end{lemma}
\begin{proof}
Because the sandwich estimator is a complicated non-linear function of the estimators $\widehat{\Sigma}_n, \widehat{V}$ and $\widehat{V}$ is not a sum of independent matrices, we first reduce the problem into basic components which are more easily controllable using results from sum of independent random variables/vectors/matrices.
Observe that
\begin{align*}
&\|(\Sigma^{-1}V\Sigma^{-1})^{-1/2}(\widehat{\Sigma}_n^{-1}\widehat{V}\widehat{\Sigma}_n^{-1})(\Sigma^{-1}V\Sigma^{-1})^{-1/2} - I_d\|_{\mathrm{op}}\\
&\qquad= \|V^{-1/2}\Sigma\widehat{\Sigma}_n^{-1}\widehat{V}\widehat{\Sigma}_n^{-1}\Sigma V^{-1/2} - I_d\|_{\mathrm{op}} ~=~ \|AA^{\top} - I_d\|_{\mathrm{op}},
\end{align*}
where $A = V^{-1/2}\Sigma\widehat{\Sigma}_n^{-1}\widehat{V}^{1/2}$ with $\widehat{V}^{1/2}$ representing the symmetric square root of $\widehat{V}$. Symmetry of $AA^{\top} - I_d$ and the definition of $\|\cdot\|_{\mathrm{op}}$ implies
\[
\|AA^{\top} - I_d\|_{\mathrm{op}} = \sup_{\theta\in\mathbb{R}^d}\left|\frac{\|A^{\top}\theta\|^2}{\|\theta\|^2} - 1\right|.
\]
Using the fact $$A^{\top}\theta ~=~ \widehat{V}^{1/2}V^{-1/2}V^{1/2}(\widehat{\Sigma}_n^{-1}\Sigma - I_d)V^{-1/2}\theta ~+~ \widehat{V}^{1/2}V^{-1/2}\theta,$$ we get
\begin{align*}
\left|\frac{\|A^{\top}\theta\|^2}{\|\theta\|^2} - 1\right| ~&\le~ \left|\frac{\|\widehat{V}^{1/2}V^{-1/2}\theta\|^2}{\|\theta\|^2} - 1\right| ~+~ \frac{\|\widehat{V}^{1/2}V^{-1/2}V^{1/2}(\widehat{\Sigma}_n^{-1}\Sigma - I_d)V^{-1/2}\theta\|^2}{\|\theta\|^2}\\
~&\qquad+~ 2\times\frac{\theta^{\top}V^{-1/2}\widehat{V}(\widehat{\Sigma}_n^{-1}\Sigma - I_d)V^{-1/2}\theta}{\|\theta\|^2}\\
~&\le~ \left|\frac{\theta^{\top}V^{-1/2}\widehat{V}V^{-1/2}\theta}{\theta^{\top}\theta} - 1\right| ~+~ \|\widehat{V}^{1/2}V^{-1/2}\|_{\mathrm{op}}^2\frac{\|V^{1/2}(\widehat{\Sigma}_n^{-1}\Sigma - I_d)V^{-1/2}\theta\|^2}{\|\theta\|^2}\\
~&\qquad+~ 2\times\frac{\|V^{-1/2}\widehat{V}V^{-1/2}\theta\|}{\|\theta\|}\times\frac{\|V^{1/2}(\widehat{\Sigma}_n^{-1}\Sigma - I_d)V^{-1/2}\theta\|}{\|\theta\|}.
\end{align*}
Taking the supremum over $\theta\in\mathbb{R}^d$ yields
\begin{equation}\label{eq:main-inequality-sandwich}
\begin{split}
\|AA^{\top} - I_d\|_{\mathrm{op}} &\le \sup_{\theta\in\mathbb{R}^d}\left|\frac{\|\widehat{V}^{1/2}V^{-1/2}\theta\|^2}{\|\theta\|^2} - 1\right|\\ &\qquad+ \|\widehat{V}^{1/2}V^{-1/2}\|_{\mathrm{op}}^2\left\{\|\widehat{\Sigma}_n^{-1}\Sigma - I_d\|_{\mathrm{op}}^2 + 2\|\widehat{\Sigma}_n^{-1}\Sigma - I_d\|_{\mathrm{op}}\right\}\\
&= \mathbf{I} ~+~ (\mathbf{I} + 1)\left\{\mathbf{II}^2 + 2\mathbf{II}\right\},
\end{split}
\end{equation}
where
\begin{equation}\label{eq:decomposition-sandwich-error}
\mathbf{I} := \sup_{\theta\in\mathbb{R}^d}\left|\frac{\|\widehat{V}^{1/2}V^{-1/2}\theta\|^2}{\|\theta\|^2} - 1\right|\quad\mbox{and}\quad \mathbf{II} := \|\widehat{\Sigma}_n^{-1}\Sigma - I_d\|_{\mathrm{op}}.
\end{equation}
% It is now clear that
% \begin{equation}\label{eq:First-implication}
% \{\mathbf{I} \le 1\}\cap\{\mathbf{II} \le 1\}\quad\Rightarrow\quad \|AA^{\top} - I_d\|_{\mathrm{op}} \le \mathbf{I} + 12\mathbf{II}
% \end{equation}
Based on this inequality, it suffices to bound $\mathbf{I}$ and $\mathbf{II}$.
Regarding $\mathbf{II}$, we note that $\mathbf{II} \le \mathcal{D}_n^{\Sigma}/(1 - \mathcal{D}_n^{\Sigma})$ and hence
\begin{equation}\label{eq:First-implication}
\{\mathcal{D}_n^{\Sigma} \le 1/2\}\quad\Rightarrow\quad \mathbf{II}\le1\quad\Rightarrow\quad \|AA^{\top} - I_d\|_{\mathrm{op}} \le \mathbf{I} + 6(\mathbf{I} + 1)\mathcal{D}_n^{\Sigma}.
\end{equation} To bound $\mathbf{I}$, define
\[
\overline{V} := \frac{1}{n}\sum_{i=1}^n X_iX_i^{\top}(Y_i - X_i^{\top}\beta)^2.
\]
The definition of $\overline{V}$ differs from $\widehat{V}$ in the use of $\beta$ in place of $\widehat{\beta}$ which yields an average of independent random matrices. Observe that
\[
\mathbf{I} ~\le~ \sup_{\theta\in\mathbb{R}^d}\left|\frac{\|\overline{V}^{1/2}V^{-1/2}\theta\|^2}{\|\theta\|^2} - 1\right| ~+~ \sup_{\theta\in\mathbb{R}^d}\left|\frac{\theta^{\top}V^{-1/2}(\widehat{V} - \overline{V})V^{-1/2}\theta}{\theta^{\top}\theta}\right| ~=:~ \mathbf{I}_1 + \mathbf{I}_{2}.
\]
Lemma~\ref{lem:operator-norm-Vhat-Vbar} proves
\[
\mathbf{I}_2 \le \frac{\mathcal{M}_4}{2}\|\widehat{\beta} - \beta\|^2_{\Sigma V^{-1}\Sigma} + \sqrt{2}\mathcal{M}_4^{1/2}\|\widehat{\beta} - \beta\|_{\Sigma V^{-1}\Sigma}\sqrt{1 + \mathbf{I}_1},
\]
for a quantity $\mathcal{M}_4$ defined in Lemma~\ref{lem:operator-norm-Vhat-Vbar}. Thus, 
\begin{equation}\label{eq:Second-implication}
\begin{split}
\left\{\mathbf{I}_1 \le \frac{1}{2}\right\}\cap\{\mathcal{M}_4^{1/2}\|\widehat{\beta} - \beta\|_{\Sigma V^{-1}\Sigma} \le 1\}\quad\Rightarrow\quad\mathbf{I} &\le \mathbf{I}_1 + 3\mathcal{M}_4^{1/2}\|\widehat{\beta} - \beta\|_{\Sigma V^{-1}\Sigma},\\
&\le 1/2 + 3 = 7/2.
\end{split}
\end{equation}
This combined with~\eqref{eq:First-implication} implies that if $\mathcal{A}$ holds then
\begin{equation}\label{eq:main-decomposition-sandwich}
\|AA^{\top} - I_d\|_{\mathrm{op}} \le \mathbf{I}_1 + 3\mathcal{M}_4^{1/2}\|\widehat{\beta} - \beta\|_{\Sigma V^{-1}\Sigma} + 21\mathcal{D}_n^{\Sigma},
\end{equation}
where
\begin{equation}\label{eq:crucial-event-sandwich}
\mathcal{A} := \left\{\mathcal{D}_n^{\Sigma} \le \frac{1}{2}\right\}\cap\left\{\mathbf{I}_1 \le \frac{1}{2}\right\}\cap\left\{\mathcal{M}_4^{1/2}\|\widehat{\beta} - \beta\|_{\Sigma V^{-1}\Sigma} \le 1\right\}.
\end{equation}
In the following we tackle each of the terms as well as the events.
\paragraph{Quantity $\mathcal{D}_n^{\Sigma}$} From Lemma~\ref{lem:concentration-of-covariance},
\[
\mathbb{P}\left(\mathcal{D}_n^{\Sigma} \le CK_x^2\sqrt{\frac{d + \log n}{n}} + CK_x^2\frac{d + \log n}{n}\right) \ge 1 - \frac{1}{n}.
\]
This implies that if $(d + \log n) \le n/(4CK_x^2)^2$ then with probability at least $1 - 1/n$, 
\begin{equation}\label{eq:quantity-D-Sigma}
\mathcal{D}_n^{\Sigma} \le 2CK_x^2\sqrt{\frac{d + \log n}{n}} \le \frac{1}{2}.
\end{equation}
\paragraph{Quantity $\mathcal{M}_4$} From Inequality (3.9) of~\cite{mendelson2010empirical}, there exists a universal constant $C\in(0,\infty)$ such that for any $t > 0$, with probability at least $1 - \exp(-t\log n)$,
\[
\sup_{\theta\in\mathbb{R}^d,\|\theta\|_{I_d} \le 1}\,\left(\sum_{i=1}^n (\theta^{\top}\Sigma^{-1/2}X_i)^4\right)^{1/4} ~\le~ CK_xt(\sqrt{d} + n^{1/4}),
\]
by taking $F = \{x\mapsto \theta^{\top}\Sigma^{-1/2}x:\,\theta\in\mathbb{R}^d, \|\theta\|_{I_d} \le 1\}$ and $|I| = n$. We refer the reader to~\cite{guedon2007lp} and~\cite{vershynin2011approximating} for similar results. Hence with probability at least $1 - 1/n$,
\begin{equation}\label{eq:quantity-M-4}
\mathcal{M}_4^{1/2} \le CK_x^2\sqrt{\frac{\overline{\lambda}}{\underline{\lambda}}}\left(\frac{d}{\sqrt{n}} + 1\right),
\end{equation}
for a possibly different constant $C\in(0,\infty)$. 
\paragraph{Quantity $\|\widehat{\beta} - \beta\|_{\Sigma V^{-1}\Sigma}$} The triangle inequality combined with  Theorem~\ref{thm:Basic-deter-ineq} yields that
\[
\|\widehat{\beta} - \beta\|_{\Sigma V_n^{-1}\Sigma} \le \frac{1}{(1 - \mathcal{D}_n^{\Sigma})}\textstyle\left\|n^{-1}\sum_{i=1}^n \psi_i\right\|_{\Sigma V_n^{-1}\Sigma}.
\]
Because $V_n = nV$, this implies that
\[
\|\widehat{\beta} - \beta\|_{\Sigma V^{-1}\Sigma} \le \frac{n^{-1/2}}{(1 - \mathcal{D}_n^{\Sigma})}\textstyle\left\|n^{-1}\sum_{i=1}^n \psi_i\right\|_{\Sigma V_n^{-1}\Sigma}.
\]
From inequality~\eqref{eq:quantity-D-Sigma} we get that, with probability at least $1 - 1/n$, $\mathcal{D}_n^{\Sigma} \le 1/2$ while Lemma~\ref{lem:concentration-influence-function} ensures that, with probability at least $1 - d/n$,
\[
\left\|\frac{1}{n}\sum_{i=1}^n \psi_i\right\|_{\Sigma V_n^{-1}\Sigma} \le 2\sqrt{d} + \sqrt{3\log(n/d)} + \left(\frac{C_qn^2}{d}\right)^{(1 + \eta_n)/q}\left(\frac{q\overline{\lambda}d}{n\eta_n}\right)^{1/2}K_qK_x,
\]
for some constant $C_q\in(0, \infty)$ depending only on $q$. Hence with probability at least $1 - (d+1)/n$, we get
\[
\|\widehat{\beta} - \beta\|_{\Sigma V^{-1}\Sigma} \le 4\sqrt{\frac{d}{n}} + 2\sqrt{\frac{3\log(n/d)}{n}} + 2\left(\frac{C_qn^2}{d}\right)^{(1+\eta_n)/q}\left(\frac{q\overline{\lambda}d}{\eta_n}\right)^{1/2}\frac{K_qK_x}{n}.
\]
This bound holds for all $\eta_n > 0$ and choosing $\eta_n$ to minimize the right hand side yields $\eta_n = q/(2\log(C_qn^2/d))$ and hence with probability at least $1 - (d + 1)/n$,
\[
\|\widehat{\beta} - \beta\|_{\Sigma V^{-1}\Sigma} \le 4\sqrt{\frac{d}{n}} + \sqrt{\frac{12\log(n/d)}{n}} + \sqrt{8e\overline{\lambda}}K_qK_xC_q^{1/q}\frac{d^{1/2 - 1/q}\sqrt{\log(C_qn^2/d)}}{n^{1-2/q}}.
\]
Combining this inequality with~\eqref{eq:quantity-M-4} shows that, with probability at least $1 - (d + 2)/n$,
\begin{equation}\label{eq:M-4-times-betahat-error}
\mathcal{M}_4^{{1}/{2}}\|\widehat{\beta} - \beta\|_{\Sigma V^{-1}\Sigma} \le C_qK_x^2\sqrt{\frac{\overline{\lambda}}{\underline{\lambda}}}\left[\sqrt{\frac{d + \log({n}/{d})}{n}} + \sqrt{\overline{\lambda}}K_qK_x\frac{\sqrt{d^{1 - 2/q}\log({n}/{d})}}{n^{1 - 2/q}}\right],
\end{equation}
 if $d \le \sqrt{n}$. %The right hand side is less than 1 if \textcolor{red}{ATTN: scaling in $d$} $d$ is sufficiently smaller than $n$.
\paragraph{Quantity $\mathbf{I}_1$} We provide a tail bound for $\mathbf{I}_1$ using Theorem 3.1 of~\cite{einmahl2008characterization}.
Observe that
\[
\mathbf{I}_1 = \left\|\frac{1}{n}\sum_{i=1}^n V^{-1/2}X_iX_i^{\top}V^{-1/2}(Y_i - X_i^{\top}\beta))^2 - \mathbb{E}[V^{-1/2}X_iX_i^{\top}V^{-1/2}(Y_i - X_i^{\top}\beta)^2]\right\|_{\mathrm{op}},
\] 
For an application of Theorem 3.1 of~\cite{einmahl2008characterization}, it is crucial to bound the expectation of $\mathbf{I}_1$ which we achieve by applying Theorem 5.1(2) of~\cite{tropp2016expected}. By this result,
\begin{align*}
\mathbb{E}[\mathbf{I}_1] &\le \sqrt{\frac{12\log(ed)}{n}}\left\|\mathbb{E}[V^{-1/2}X_iX_i^{\top}V^{-1/2}\|V^{-1/2}X_i\|^2_{I_d}(Y_i - X_i^{\top}\beta)^4]\right\|_{\mathrm{op}}^{1/2}\\ 
&\qquad+ \frac{24\log(ed)}{n}\left(\mathbb{E}\left[\max_{1\le i\le n}\|V^{-1/2}X_i\|_{I_d}^4(Y_i - X_i^{\top}\beta)^4\right]\right)^{1/2}\\
&\le \sqrt{\frac{12d\log(ed)}{n}}\sup_{\theta\in\mathbb{R}^d,\|\theta\|_{I_d} \le 1}\,\left(\mathbb{E}\left[(\theta^{\top}V^{-1/2}X_i)^4(Y_i - X_i^{\top}\beta)^4\right]\right)^{1/2}\\
&\qquad+ \frac{24d\log(ed)}{n}\sup_{\theta\in\mathbb{R}^d,\|\theta\|_{I_d} \le 1}\left(\sum_{i=1}^n \mathbb{E}\left[|\theta^{\top}V^{-1/2}X_i(Y_i - X_i^{\top}\beta)|^{q/(1 + \eta_n)}\right]\right)^{2(1 + \eta_n)/q}\\
&\overset{(a)}{\le} C\overline{\lambda}K_x^2K_q^2(1-4q^{-1})^{-1}\sqrt{d\log(ed)/n} + C\overline{\lambda}\frac{d\log(ed)}{n}n^{2(1+\eta_n)/q}K_x^2K_q^2\frac{q}{\eta_n},
\end{align*}
for some universal constant $C\in(0,\infty)$. The inequality (a) for the first term follows from Proposition~\ref{prop:implication-moments-influence-function} by taking $\eta_n = (q/4) - 1$ and for the second term follows from Proposition~\ref{prop:implication-moments-influence-function} for general $\eta_n > 0$. Minimizing the second term over all $\eta_n > 0$ yields the optimal choice of $\eta_n = q/(2\log n)$ and hence
\[
\mathbb{E}[\mathbf{I}_1] \le C\overline{\lambda}K_x^2K_q^2\left[\frac{\sqrt{d\log(ed)}}{(1 - 4q^{-1})\sqrt{n}} + \frac{d\log(ed)\log n}{n^{1 - 2/q}}\right].
\] 
Observe that for the preceding inequalities to apply, we need $4 \le q/(1 + \eta_n)$ and thus the feasibility of the optimal choice of $\eta_n$ then requires $q \ge 4(\log n)/(\log n - 2)$. An application of Theorem 3.1 of~\cite{einmahl2008characterization} now concludes
\begin{equation}
\begin{split}
&\mathbb{P}\left(\mathbf{I}_1 \ge 2\mathbb{E}[I_1] + t\right) \le \exp\left(-\frac{nt^2(1-4q^{-1})^2}{C\overline{\lambda}^2K_x^4K_q^4}\right) + C\frac{\mathbb{E}[\|V^{-1/2}X_i\|_{I_d}^{2s}|Y_i - X_i^{\top}\beta|^{2s}]}{n^{s-1}t^s},
\end{split}
\end{equation}
for some constant $C\in(0,\infty)$ depending only on $s$. Taking $s = q/(2(1+\eta_n))$ and inverting the tail bound implies for any $\eta_n > 0$,
\begin{equation}\label{eq:quantity-I-1}
\begin{split}
&\mathbb{P}\left(\mathbf{I}_1 \ge C\overline{\lambda}K_x^2K_q^2\left[\frac{\sqrt{d\log(ed) + \log(2/\delta)}}{(1-4q^{-1})\sqrt{n}} + \frac{d\log(ed)\log n}{n^{1-2/q}} + \frac{qd\delta^{-2/q}}{\eta_n n^{1-2/q}}{\left(\frac{n}{\delta}\right)^{2\eta_n/q}}\right]\right) \le \delta.
\end{split}
\end{equation}
Taking $\delta = \sqrt{d/n}$ and $\eta_n = q/(2\log(n^{3/2}/d^{1/2}))$ yields with probability $1 - \sqrt{d/n}$
\begin{equation}\label{eq:quantity-I-1-again}
\mathbf{I}_1 \le C_q\overline{\lambda}K_x^2K_q^2\left[\frac{\sqrt{d\log(ed) + \log(2n/d)}}{(1-4q^{-1})\sqrt{n}} + \frac{d\log(ed)\log n}{n^{1-2/q}} + \frac{d^{1-1/q}\log(n/d)}{n^{1-3/q}}\right].
\end{equation}
%{\color{red}(The last term converges to zero if and only if $d = o(n^{(q-3)/(q-1)})$. If $q \ge 5$ then this condition becomes $d = o(n^{1/2})$. If $q > 5$ then $d = O(n^{1/2})$ is permitted.)}

Substituting inequalities~\eqref{eq:quantity-D-Sigma},~\eqref{eq:quantity-M-4},~\eqref{eq:M-4-times-betahat-error}, and~\eqref{eq:quantity-I-1-again} in~\eqref{eq:main-decomposition-sandwich} yields with probability $1 - (d+2)/n - \sqrt{d/n}$,
\begin{equation}
\begin{split}
&\|AA^{\top} - I_d\|_{\mathrm{op}}\\ 
&\quad\le C_q\overline{\lambda}K_x^2K_q^2\left[\frac{\sqrt{d\log(ed) + \log(2n/d)}}{(1-4q^{-1})\sqrt{n}} + \frac{d\log(ed)\log n}{n^{1-2/q}} + \frac{d^{1-1/q}\log(n/d)}{n^{1-3/q}}\right]\\
&\quad\qquad+ C_qK_x^2\sqrt{\frac{\overline{\lambda}}{\underline{\lambda}}}\left(1 + \frac{d}{\sqrt{n}}\right)\left[\sqrt{\frac{d + \log({n}/{d})}{n}} + \sqrt{\overline{\lambda}}K_qK_x\frac{\sqrt{d^{1 - 2/q}\log({n}/{d})}}{n^{1 - 2/q}}\right]\\
&\quad\qquad+ 2CK_x^2\sqrt{\frac{d + \log n}{n}}\\
&\quad\le \frac{C_qK_x^2}{(1-4q^{-1})}\sqrt{\frac{d\log(ed) + \log(n/d)}{n}}\left[1 + \overline{\lambda} K_q^2 + \sqrt{\frac{\overline{\lambda}}{\underline{\lambda}}}\left(1 + \frac{d}{\sqrt{n}}\right)\right]\\
&\quad\qquad+ C_qK_x^2\frac{d\log(ed)\log n}{n^{1-2/q}}\left[\overline{\lambda}K_q^2 + \frac{\overline{\lambda}K_qK_x}{\sqrt{\underline{\lambda}}}\left(1 + \frac{d}{\sqrt{n}}\right)\right] + C_qK_x^2\overline{\lambda}K_q^2\frac{d^{1-1/q}\log(n/d)}{n^{1-3/q}}\\
&\quad\le \frac{C_qK_x^2}{(1-4q^{-1})}\sqrt{\frac{d\log(ed) + \log(n/d)}{n}}\left[1 + \overline{\lambda} K_q^2 + \sqrt{\frac{\overline{\lambda}}{\underline{\lambda}}}\left(1 + \frac{d}{\sqrt{n}}\right)\right]\\
&\quad\qquad+ C_qK_x^2\frac{d^{1-1/q}\log(ed)\log n}{n^{1-3/q}}\left[\overline{\lambda}K_q^2 + \frac{\overline{\lambda}K_qK_x}{\sqrt{\underline{\lambda}}}\left(1 + \frac{d}{\sqrt{n}}\right)\right].
\end{split}
\end{equation}
\end{proof}
% This bound holds for every $\eta_n > 0$ and minimizing over $\eta_n > 0$ yields $\eta_n = q/(2\log n)$. Hence
% \begin{equation}
% \begin{split}
% &\mathbb{P}\left(\mathbf{I}_1 \ge C\overline{\lambda}K_x^2K_q^2\left[\frac{\sqrt{d\log(ed) + \log(2/\delta)}}{(1-4q^{-1})\sqrt{n}} + \frac{d\log(ed)\log n}{n^{1-2/q}} + \frac{d\log n}{n^{1-2/q}\delta^{1/q}}\right]\right) \le \delta,
% \end{split}
% \end{equation}
% \[
% (2/q)\log n = 1/\eta_n\quad\Leftrightarrow\quad n^{(2\eta_n/q)} = e.
% \]
\begin{lemma}\label{lem:operator-norm-Vhat-Vbar}
Let
\[
\overline{V} := \frac{1}{n}\sum_{i=1}^n X_iX_i^{\top}(Y_i - X_i^{\top}\beta)^2\quad\mbox{and}\quad \widehat{V} := \frac{1}{n}\sum_{i=1}^n X_iX_i^{\top}(Y_i - X_i^{\top}\widehat{\beta})^2.
\]
Then
\begin{align*}
\|V^{-1/2}(\widehat{V} - \overline{V})V^{-1/2}\|_{\mathrm{op}} &\le \frac{1}{2}\mathcal{M}_4\|\widehat{\beta} - \beta\|_{\Sigma V^{-1}\Sigma}^2\\ &\qquad+ \sqrt{2}\mathcal{M}_4^{1/2}\|\widehat{\beta} - \beta\|_{\Sigma V^{-1}\Sigma}\|\overline{V}^{1/2}V^{-1/2}\|_{\mathrm{op}},
\end{align*}
where
\[
\mathcal{M}_4 := \frac{\overline{\lambda}}{\underline{\lambda}}\times\sup_{\theta\in\mathbb{R}^d,\|\theta\|_{I_d} = 1}\frac{1}{n}\sum_{i=1}^n (\theta^{\top}\Sigma^{-1/2}X_i)^4.
\]
\end{lemma}
\begin{proof}
The definition of the operator norm and the symmetry of $V^{-1/2}(\widehat{V} - \overline{V})V^{-1/2}$ implies
\[
\|V^{-1/2}(\widehat{V} - \overline{V})V^{-1/2}\|_{\mathrm{op}} = \sup_{\theta\in\mathbb{R}^d:\,\|\theta\|_{I_d} = 1}\,\left|\theta^{\top}V^{-1/2}(\widehat{V} - \overline{V})V^{-1/2}\theta\right|.
\]
Fix $\theta\in\mathbb{R}^d$ such that $\|\theta\|_{I_d} = 1$.
Expanding $(Y_i - X_i^{\top}\widehat{\beta})^2$ in $\widehat{V}$ by adding and subtracting $X_i^{\top}\beta$ to $X_i^{\top}\widehat{\beta}$ yields
\begin{align*}
&\left|{\theta^{\top}V^{-1/2}(\widehat{V} - \overline{V})V^{-1/2}\theta}\right|\\ 
&\quad\le \frac{1}{n}\sum_{i=1}^n {(\theta^{\top}V^{-1/2}X_i)^2}\left[(X_i^{\top}(\widehat{\beta} - \beta))^2 + 2|Y_i - X_i^{\top}\beta|\times|X_i^{\top}(\widehat{\beta} - \beta)|\right]\\
&\quad\le \frac{1}{n}\sum_{i=1}^n {(\theta^{\top}V^{-1/2}X_i)^2}(X_i^{\top}(\widehat{\beta} - \beta))^2\\ 
&\quad\qquad+ \frac{1}{n}\sum_{i=1}^n {(\theta^{\top}V^{-1/2}X_i)^2}\left[\frac{|Y_i - X_i^{\top}\beta|^2}{L} + L|X_i^{\top}(\widehat{\beta} - \beta)|^2\right]\\
&\quad\le \frac{(1 + L)}{n}\sum_{i=1}^n {(\theta^{\top}V^{-1/2}X_i)^2}(X_i^{\top}(\widehat{\beta} - \beta))^2 + \frac{1}{nL}\sum_{i=1}^n {(\theta^{\top}V^{-1/2}X_i)^2}(Y_i - X_i^{\top}\beta)^2\\
&\quad\le \frac{(1 + L)}{n}\sum_{i=1}^n {(\theta^{\top}V^{-1/2}X_i)^2}(X_i^{\top}(\widehat{\beta} - \beta))^2 + \frac{1}{L}\times{\theta^{\top}V^{-1/2}\bar{V}V^{-1/2}\theta}.
\end{align*}
We write $X_i^{\top}(\widehat{\beta} - \beta)$ as $(V^{1/2}\Sigma^{-1}X_i)^{\top}(V^{-1/2}\Sigma(\widehat{\beta} - \beta))$ and bound the first term on the right hand side as
\[
\frac{1}{n}\sum_{i=1}^n {(\theta^{\top}V^{-1/2}X_i)^2}(X_i^{\top}(\widehat{\beta} - \beta))^2 \le \frac{1}{n}\sum_{i=1}^n (\theta^{\top}V^{-1/2}X_i)^2(u^{\top}V^{1/2}\Sigma^{-1}X_i)^2\|\widehat{\beta} - \beta\|_{\Sigma V^{-1}\Sigma}^2,
\]
where $u = V^{-1/2}\Sigma(\widehat{\beta} - \beta)/\|\widehat{\beta} - \beta\|_{\Sigma V^{-1}\Sigma}$ is of unit norm. The right hand side (without the factor $\|\widehat{\beta} - \beta\|_{\Sigma V^{-1}\Sigma}^2$) can be further bounded by
\begin{align*}
&\sup_{\theta, u\in\mathbb{R}^d}\,\frac{1}{n}\sum_{i=1}^n \frac{(\theta^{\top}V^{-1/2}X_i)^2(u^{\top}V^{1/2}\Sigma^{-1}X_i)^2}{(\theta^{\top}\theta)(u^{\top}u)}\\
&\quad= \sup_{\gamma, v\in\mathbb{R}^d}\,\frac{1}{n}\sum_{i=1}^n \frac{(\gamma^{\top}\Sigma^{-1/2}X_i)^2(v^{\top}\Sigma^{-1/2}X_i)^2}{(\gamma^{\top}\Sigma^{-1/2}V\Sigma^{-1/2}\gamma)(v^{\top}\Sigma^{1/2}V^{-1}\Sigma^{1/2}v)}\\
&\quad\le \sup_{\gamma, v\in\mathbb{R}^d}\,\frac{1}{n}\sum_{i=1}^n \frac{(\gamma^{\top}\Sigma^{-1/2}X_i)^2(v^{\top}\Sigma^{-1/2}X_i)^2}{(\gamma^{\top}\gamma)(v^{\top}v)}\times\frac{(\gamma^{\top}\gamma)(v^{\top}v)}{(\gamma^{\top}\Sigma^{-1/2}V\Sigma^{-1/2}\gamma)(v^{\top}\Sigma^{1/2}V^{-1}\Sigma^{1/2}v)}\\
&\quad\le \sup_{\theta\in\mathbb{R}^d,\|\theta\|_{I_d} \le 1}\,\frac{1}{n}\sum_{i=1}^n (\theta^{\top}\Sigma^{-1/2}X_i)^4\times\frac{\overline{\lambda}}{\underline{\lambda}}.
\end{align*}
Combining these to bounds into~\eqref{eq:First-bound-Operator-norm-Vhat-Vbar} concludes
\begin{equation}\label{eq:First-bound-Operator-norm-Vhat-Vbar}
\|V^{-1/2}(\widehat{V} - \overline{V})V^{-1/2}\|_{\mathrm{op}} \le \frac{(1 + L)}{2}\mathcal{M}_4\|\widehat{\beta} - \beta\|_{\Sigma V^{-1}\Sigma}^2 + \frac{\|\overline{V}^{1/2}V^{-1/2}\|_{\mathrm{op}}^2}{L},
\end{equation}
% where $\mathcal{M}_4^*$ is defined as
% \[
% \mathcal{M}_4^* := \sup_{\theta\in\mathbb{R}^d,\|\theta\|_{I_d} = 1}\frac{1}{n}\sum_{i=1}^n (\theta^{\top}V^{-1/2}X_i)^4 ~+~ \sup_{\theta\in\mathbb{R}^d,\|\theta\|_{I_d} = 1} \frac{1}{n}\sum_{i=1}^n (\theta^{\top}V^{1/2}\Sigma^{-1}X_i)^4.
% \]
% The second term in the definition of $\mathcal{M}_4^*$ can be bounded as
% \begin{align*}
% \sup_{\theta\in\mathbb{R}^d}\,\frac{1}{n}\sum_{i=1}^n \frac{(\theta^{\top}V^{1/2}\Sigma^{-1}X_i)^4}{(\theta^{\top}\theta)^2} &= \sup_{u\in\mathbb{R}^d}\,\frac{1}{n}\sum_{i=1}^n \frac{(u^{\top}\Sigma^{-1/2}X_i)^4}{(u^{\top}\Sigma^{1/2}V^{-1}\Sigma^{1/2}u)^2}\\
% &= \lambda_{\max}^2(\Sigma^{-1/2}V\Sigma^{-1/2})\sup_{u\in\mathbb{R}^d}\,\frac{1}{n}\sum_{i=1}^n \frac{(u^{\top}\Sigma^{-1/2}X_i)^4}{(u^{\top}u)^2}\\
% &= \frac{1}{\underline{\lambda}^2}\sup_{\theta\in\mathbb{R}^d, \|\theta\|_{I_d} = 1}\,\frac{1}{n}\sum_{i=1}^n (\theta^{\top}\Sigma^{-1/2}X_i)^4. 
% \end{align*}
% Similar argument provides a bound on the first term of $\mathcal{M}_4^*$ in terms of $\overline{\lambda}^2$. 
Minimizing over $L > 0$ concludes the result.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\section{Proofs of Auxiliary Results for Section~\ref{section::partial} (Partial Correlations)}
\label{appendix:auxiliary.partial}
We begin by bounding $\mathcal{D}_n^{\Sigma}$ in terms of the intermediate Gram matrix $\widetilde{\Sigma}$. These bounds and associated derivations are be used repeatedly in the proofs of the results from Section~\ref{section::partial}.

\begin{proposition}\label{prop:bounding-D-sigma}
For every $n\ge1$,
\[
\mathcal{D}_n^{\Sigma} ~\le~ \|\Sigma^{-1/2}\widetilde{\Sigma}\Sigma^{-1/2} - I_d\|_{\mathrm{op}} + \|\overline{X}_n - \mu_X\|_{\Sigma^{-1}}^2.
\]
\end{proposition}

\begin{proof}
The triangle inequality implies that
\[
\mathcal{D}_n^{\Sigma} ~\le~ \|\Sigma^{-1/2}\widetilde{\Sigma}\Sigma^{-1/2} - I_d\|_{\mathrm{op}} + \|\Sigma^{-1/2}(\widehat{\Sigma}_n - \widetilde{\Sigma})\Sigma^{-1/2}\|_{\mathrm{op}}. 
\]
The definition of $\widetilde{\Sigma}$ yields.
\[
\|\Sigma^{-1/2}(\widehat{\Sigma}_n - \widetilde{\Sigma})\Sigma^{-1/2}\|_{\mathrm{op}} ~=~ \|\Sigma^{-1/2}(\overline{X}_n - \mu_X)\|_{I_d}^2.
\] 
This concludes the proof.
\end{proof}
\begin{lemma}\label{lemma:linear-expansion-inv-covariance}
Under the assumption that $\widehat{\Sigma}_n$ is invertible and $\mathcal{D}_n^{\Sigma} <1$,
\begin{equation}\label{eq:inv-covariance-error-bound}
\|\Sigma^{1/2}(\widehat{\Sigma}_n^{-1} - \Sigma^{-1})\Sigma^{1/2}\|_{\mathrm{op}} ~\le~ \frac{\mathcal{D}_n^{\Sigma}}{1 - \mathcal{D}_n^{\Sigma}}. 
\end{equation}
and
\begin{equation}\label{eq:final-linear-expansion-inv-covariance}
\left\|\Sigma^{1/2}\left\{\widehat{\Sigma}_n^{-1} - \Sigma^{-1} + \Sigma^{-1}(\widetilde{\Sigma} - \Sigma)\Sigma^{-1}\right\}\Sigma^{1/2}\right\|_{\mathrm{op}} \le \|\overline{X}_n - \mu_X\|^2_{\Sigma^{-1}} + \frac{(\mathcal{D}_n^{\Sigma})^2}{1 - \mathcal{D}_n^{\Sigma}}.
\end{equation}
\end{lemma}
\begin{proof}
We start with the following equality:
\begin{align*}
\widehat{\Sigma}_n^{-1} - \Sigma^{-1} ~&=~ \widehat{\Sigma}_n^{-1}(\Sigma - \widehat{\Sigma}_n)\Sigma^{-1}\\
~&=~ \Sigma^{-1}(\Sigma - \widehat{\Sigma}_n)\Sigma^{-1} + (\widehat{\Sigma}_n^{-1} - \Sigma^{-1})(\Sigma - \widehat{\Sigma}_n)\Sigma^{-1}\\
~&=~ \Sigma^{-1}(\Sigma - \widehat{\Sigma}_n)\Sigma^{-1} + \widehat{\Sigma}_n^{-1}\Sigma^{1/2}(I_d - \Sigma^{-1/2}\widehat{\Sigma}_n\Sigma^{-1/2})(I_d - \Sigma^{-1/2}\widehat{\Sigma}_n\Sigma^{-1/2})\Sigma^{-1/2}.
\end{align*}
The first equality implies
\[
\|\Sigma^{1/2}(\widehat{\Sigma}_n^{-1} - \Sigma^{-1})\Sigma^{1/2}\|_{\mathrm{op}} \le \|\Sigma^{1/2}\widehat{\Sigma}_n^{-1}\Sigma^{1/2}\|_{\mathrm{op}}\|\Sigma^{-1/2}(\Sigma - \widehat{\Sigma}_n)\Sigma^{-1/2}\|_{\mathrm{op}},
\]
which proves~\eqref{eq:inv-covariance-error-bound}. The last equality above implies
\begin{equation}\label{eq:linear-expansion-partial-corr}
\begin{split}
\left\|\Sigma^{1/2}\left\{\widehat{\Sigma}_n^{-1} - \Sigma^{-1} + \Sigma^{-1}(\widehat{\Sigma}_n - \Sigma)\Sigma^{-1}\right\}\Sigma^{1/2}\right\|_{\mathrm{op}} ~&\le~ \|\Sigma^{1/2}\widehat{\Sigma}_n^{-1}\Sigma^{1/2}\|_{\mathrm{op}}\|\Sigma^{-1/2}\widehat{\Sigma}_n\Sigma^{-1/2} - I_d\|_{\mathrm{op}}^2\\
% ~&\le~ \left( 1 + \|\Sigma^{1/2}\widehat{\Sigma}_n^{-1}\Sigma^{1/2} - I_d \|_{\mathrm{op}} \right) \|\Sigma^{-1/2}\widehat{\Sigma}_n\Sigma^{-1/2} - I_d\|_{\mathrm{op}}^2\\
~&\le~ \frac{\|\Sigma^{-1/2}\widehat{\Sigma}_n\Sigma^{-1/2} - I_d\|_{\mathrm{op}}^2}{1 - \|\Sigma^{-1/2}\widehat{\Sigma}_n\Sigma^{-1/2} - I_d\|_{\mathrm{op}}}.
\end{split}
\end{equation}
\textcolor{blue}{assuming $\|\Sigma^{-1/2}\widehat{\Sigma}_n\Sigma^{-1/2} - I_d\|_{\mathrm{op}} < 1$} and using the fact that $\|A^{-1}\|_{\mathrm{op}} \le (1 - \|I - A\|_{\mathrm{op}})^{-1}$ whenever $\|I - A\|_{\mathrm{op}} < 1$.
This inequality almost proves a linear representation of $\widehat{\Sigma}_n^{-1} - \Sigma^{-1}$ except that $\widehat{\Sigma}_n - \Sigma$ is \emph{not} an average of independent random matrices. Using $\widetilde{\Sigma}$, we get
\[
\|\Sigma^{-1/2}(\widehat{\Sigma}_n - \widetilde{\Sigma})\Sigma^{-1/2}\|_{\mathrm{op}} ~=~ \|\Sigma^{-1/2}(\overline{X}_n - \mu_X)\|_{I_d}^2.
\] 
Combining this equality with~\eqref{eq:linear-expansion-partial-corr} concludes the proof.
\end{proof}



\section{Auxiliary Results}
\label{appendix:auxiliary}

The following result is an application of Theorem 3.1 in \cite{KuchAbhi17}.

\begin{lemma}\label{lemma:Thm3.1.KuchAbhi}
Suppose $W_1, \ldots, W_n\in\mathbb{R}^q$ are independent mean zero random vectors such that each of their coordinate is sub-Weibull$(\alpha)$, that is, $\|W_i(j)\|_{\alpha} \le K_w$ for $1\le j\le q$ and for 
some $\alpha\in (0, 1]$, , then for all $t\ge0$,
\[
\mathbb{P}\left(\max_{1\le j\le q}\left|\frac{1}{n}\sum_{i=1}^n W_i(j)\right| \ge CK_w\left\{\sqrt{\frac{t + \log q}{n}} + \frac{(t + \log q)^{1/\alpha}}{n}\right\}\right) \le 3e^{-t}.
\]
\end{lemma}
\begin{proof}
Theorem 3.1 and Proposition A.3 of~\cite{KuchAbhi17} jointly give that
\[
\mathbb{P}\left(\left|\frac{1}{n}\sum_{i=1}^n W_i(j)\right| \ge \frac{CK_w}{\sqrt{n}}\left(\sqrt{t} + \frac{t^{1/\alpha}}{\sqrt{n}}\right)\right) \le 3e^{-t},
\]
for all $t > 0$ and for some universal constant $C\in(0, \infty)$. The result now follows from a union bound. 
% By a union bound over $1\le j\le q$ (i.e., taking $t \to t + \log q$) concludes
%\[
%\mathbb{P}\left(\max_{1\le j\le q}\left|\frac{1}{n}\sum_{i=1}^n W_i(j)\right| \ge \frac{CK_w}{\sqrt{n}}\left(\sqrt{t + \log(q)} + \frac{(t + \log q)^{1/\alpha}}{n}\right)\right) \le 3e^{-t}.
%\]
\end{proof}

The next bound is an application of Theorem 8 of~\cite{Bouch05}.

\begin{lemma}\label{lem:application-theorem8}
Suppose $W_1, \ldots, W_n$ are non-negative sub-Weibull$(1/2)$ random variables, that is, $\|W_i\|_{\psi_{1/2}} \le K_w < \infty$, then for all $t \ge 0$,
\begin{equation}\label{eq:to-be-proved-theorem-8}
\mathbb{P}\left(\frac{1}{n}\sum_{i=1}^n W_i \ge \frac{2}{n}\sum_{i=1}^n \mathbb{E}[W_i] + K_w\frac{t^3(\log n)^2}{n}\right) \le ee^{-t}.
\end{equation}
\end{lemma}
\begin{proof}
Theorem 8 of~\cite{Bouch05} implies
\begin{equation}\label{eq:main-implication-thereom8}
\left\|\frac{1}{n}\sum_{i=1}^n W_i\right\|_q \le \frac{2}{n}\sum_{i=1}^n \mathbb{E}[W_i] + \frac{2q}{n}\left\|\max_{1\le i\le n} W_i\right\|_q.
\end{equation}
(This follows by taking, following the notation of that paper, $\theta = 1$ and noting that $\kappa/2 < 1$). Because the $W_i$'s are sub-Weibull$(1/2)$, that is, $\mathbb{E}\left[\exp(\sqrt{|W_i|/K_w})\right] \le 2$,
\[
\mathbb{P}\left(W_i \ge K_wt^2\right) \le 2\exp\left(-t\right)\quad\mbox{for all}\quad t > 0.
\]
Hence by a union bound
\[
\mathbb{P}\left(\max_{1\le i\le n}W_i \ge K_w(t + \log n)^2\right) \le 2\exp(-t)\quad\mbox{for all}\quad t > 0.
\]
This yields
\[
\mathbb{P}\left((\max_{1\le i\le n} W_i - 2K_w\log^2n)_+ \ge 2K_wt^2\right) \le 2\exp(-t),
\]
or in other words, $(\max_{1\le i\le n}W_i - 2K_w\log^2n)_+$ is sub-Weibull$(1/2)$ with parameter $2K_w$. Therefore, for all $q \ge 1$,
\[
\left\|\max_{1\le i\le n}W_i\right\|_{q} \le 2K_w\log^2n + 2K_wq^2.
\]
Substituting this inequality in~\eqref{eq:main-implication-thereom8} concludes for all $q \ge 1$,
\[
\left\|\frac{1}{n}\sum_{i=1}^n W_i\right\|_q \le \frac{2}{n}\sum_{i=1}^n \mathbb{E}[W_i] + \frac{4qK_w\log^2n}{n} + \frac{4q^3K_w}{n}.
\]
For any random variable $R$, Markov's inequality implies
\[
\mathbb{P}\left(R \ge e\|R\|_t\right) \le \frac{\|R\|_t^t}{e^t\|R\|_t^t} = e^{-t},\quad\mbox{for}\quad t \ge 1. 
\]
Therefore, for all $t\ge 1$,
\[
\mathbb{P}\left(\frac{1}{n}\sum_{i=1}^n W_i \le \frac{2e}{n}\sum_{i=1}^n \mathbb{E}[W_i] + \frac{4etK_w\log^2n}{n} + \frac{4et^3K_w}{n}\right) \le e^{-t}.
\]
To make this valid over all $t > 0$, we use the fact that probabilities are bounded by 1 and multiply the right hand side by $e$ so that for $t < 1$, $ee^{-t} > 1$. This completes the proof of~\eqref{eq:to-be-proved-theorem-8}.
\end{proof}

% \begin{lemma}\label{lem:estimated-covariance-to-intermediate-covariance}
% Suppose $\widehat{\psi}_1$ and $\widehat{\psi}_2$ are two estimated functions of $X_1, \ldots, X_n$. Define
% \[
% \widehat{\sigma}_{12} ~:=~ \widehat{\mbox{cov}}\left(\widehat{\psi}_1(X_i),\widehat{\psi}_2(X_i)\right), 
% \]
% as the empirical covariance between $(\widehat{\psi}_1(X_i), 1\le i\le n)$ and $(\widehat{\psi}_2(X_i), 1\le i\le n)$. Also, define
% \[
% \widetilde{\sigma}_{12} ~:=~ \widehat{\mbox{cov}}\left(\psi_1(X_i),\psi_2(X_i)\right), 
% \]
% as the empirical covariance between $(\psi_1(X_i), 1\le i\le n)$ and $(\psi_2(X_i), 1\le i\le n)$. Then
% \[
% \left|\widehat{\sigma}_{12} - \widetilde{\sigma}_{12}\right| \le \max\left\{\frac{1}{n}\sum_{i=1}^n \left(\widehat{\psi}_1(X_i) - \psi_1(X_i)\right)^2, \frac{1}{n}\sum_{i=1}^n \left(\widehat{\psi}_2(X_i) - \psi_2(X_i)\right)^2\right\}.
% \]
% \end{lemma}
% \begin{proof}
% From the definit
% \end{proof}
\end{appendices}


%\bibliography{paper}
%\bibliographystyle{apalike}
\end{document}


